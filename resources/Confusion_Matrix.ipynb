{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "mkUYF0gHmDP_",
    "kPRRFg_zZT1L",
    "k2I70GMiBP1D"
   ]
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkUYF0gHmDP_"
   },
   "source": "# Model Performance and Generalization Errors with Confusion MX"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aNHXkwhojTf"
   },
   "source": [
    "# Today's Marketing Problem: From Lab to the Real-World!\n",
    "### **A Fintech wants to automate the loan approval process in real-time...**\n",
    "### **...based on information that applicants provide about themselves online.**\n",
    "\n",
    "1. How to find the best model?\n",
    "2. How can the Fintech be more certain that their model actually works in the real-world (i.e., on new data)?\n",
    "3. How can the Fintech investigate algorithmic bias?\n",
    "\n",
    "![Lab vs Real-World](https://upmic.files.wordpress.com/2013/05/real-vs-movie-scientist2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pphHqObEmCfR"
   },
   "source": [
    "# 1 Empirical Setting: ***Customer Eligibility for a Loan***\n",
    "\n",
    "[*Based on an Analytics Vidhya practice competition*](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/)\n",
    "\n",
    "A Fintech wants to automate the loan eligibility process (in real time) based on the details that applicants provide in their online application. These details are:\n",
    "\n",
    "- Gender\n",
    "- Marital Status\n",
    "- Education\n",
    "- Number of Dependents\n",
    "- Income\n",
    "- Loan Amount\n",
    "- Credit History and others.\n",
    "\n",
    "To automate this process, the Fintech needs to assign applicants to either being eligible for the requested loan (amount), or not. You are provided with a training dataset in which the loan eligibility is marked.\n",
    "The dataset contains the following variables:\n",
    "\n",
    "| Variable          \t | Description                     \t |\n",
    "|---------------------|-----------------------------------|\n",
    "| Loan_ID           \t | Unique Loan ID                  \t |\n",
    "| Gender            \t | Male/ Female                    \t |\n",
    "| Married           \t | Applicant married (Y/N)         \t |\n",
    "| Dependents        \t | Number of dependents            \t |\n",
    "| Education         \t | Graduate/ Under Graduate        \t |\n",
    "| Self_Employed     \t | Self employed (Y/N)             \t |\n",
    "| ApplicantIncome   \t | Applicant income                \t |\n",
    "| CoapplicantIncome \t | Coapplicant income              \t |\n",
    "| LoanAmount        \t | Loan amount in thousands        \t |\n",
    "| Loan_Amount_Term  \t | Term of loan in months          \t |\n",
    "| Credit_History    \t | credit history meets guidelines \t |\n",
    "| Property_Area     \t | Urban/ Semi Urban/ Rural        \t |\n",
    "| Loan_Status       \t | Loan approved (Y/N)             \t |\n",
    "\n",
    "You can download the dataset ***loan_train.csv*** from CANVAS along with the notebook of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPRRFg_zZT1L"
   },
   "source": [
    "## 1.1 Mount our Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m1ObTrR3Tymw"
   },
   "source": [
    "#0a Mount Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "#0b Change into directory where data files are\n",
    "%cd /content/gdrive/MyDrive/488/Class18\n",
    "\n",
    "#0c show files in current directory\n",
    "!ls # view the files in the current directory of the notebook environment"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2I70GMiBP1D"
   },
   "source": [
    "## 1.2 Preprocess Data in one Step!\n",
    "\n",
    "We create a function that takes care of all the preprocessing steps for us.  \n",
    "\n",
    "***What is the advantage of doing so?***"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RlhLktTZBZxS"
   },
   "source": [
    "# 1a Import some libraries that you will need for this step\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 2 Let's create two functions that help us with the preprocessing to make it faster and easier to execute\n",
    "\n",
    "# 2a Outlier Detection (we will call this function within our preprocessing function that we define next in 2b)\n",
    "'This function can be used on any dataset to return a list of index values for the outliers (based on standard deviation)'\n",
    "'Appropriate only for numerical features'\n",
    "def get_outliers(data, columns, nsd=3):\n",
    "    # we create an empty list\n",
    "    outlier_idxs = []\n",
    "    for col in columns:\n",
    "        elements = data[col]\n",
    "        # we get the mean value for each column\n",
    "        mean = elements.mean()\n",
    "        # and the standard deviation of the column\n",
    "        sd = elements.std()\n",
    "        # we then get the index values of all values higher or lower than the mean +/- nsd standard deviations\n",
    "        outliers_mask = data[(data[col] > mean + nsd*sd) | (data[col]  < mean  - nsd*sd)].index\n",
    "        # and add those index values to our list\n",
    "        outlier_idxs  += [x for x in outliers_mask]\n",
    "    return list(set(outlier_idxs))\n",
    "\n",
    "# 2b Put all the preprocessing from class 04 into a function\n",
    "'Function that receives the loan eligibility raw data'\n",
    "'as well as the imputation strategies for categorical and numerical features'\n",
    "'and runs all preprocessing steps'\n",
    "'Outputs a data object that contains features and response variable'\n",
    "def prepro(df,numimp,catimp):\n",
    "      #2b1 The first column (Loan_ID) is not informative to our task. So let's drop it!\n",
    "      df=df.drop(['Loan_ID'],axis=1)\n",
    "\n",
    "      #2b2 Make your work easy: Automatically identify which features are numeric and which are categorical.\n",
    "      # To do so, create two indices that hold the colum names of our data set that are numerical and categorical, respectively\n",
    "      # Make sure that you do not include our response variable here, since this is just about features!\n",
    "      numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "      categorical_features = df.select_dtypes(include=['object', 'category']).drop(['Loan_Status'], axis=1).columns\n",
    "\n",
    "      #2b3 What about Credit_History? Should that not be a categorical variable?\n",
    "      numeric_features=numeric_features.drop('Credit_History')\n",
    "      categorical_features=categorical_features.insert(6, 'Credit_History')\n",
    "\n",
    "      #2b4 Now typecast all variables that are categorical as type \"category\"\n",
    "      for col in categorical_features.tolist():\n",
    "              df[col] = df[col].astype('category')\n",
    "\n",
    "      #2b5 Don't forget to also typecast our response variable as category\n",
    "      df['Loan_Status'] = df['Loan_Status'].astype('category')\n",
    "\n",
    "      #2b6 Let's identify and remove outliers for the numerical variables using the function we created in 2a above\n",
    "      outs = get_outliers(df, numeric_features)\n",
    "      df = df.drop(outs, axis = 0)\n",
    "\n",
    "      #2b7a Now you need to impute missing numerical values. What imputation strategy will you choose?\n",
    "      imputer = SimpleImputer(missing_values = np.nan, strategy = numimp)\n",
    "      imputer = imputer.fit(df[numeric_features.tolist()])\n",
    "      df[numeric_features.tolist()] = imputer.transform(df[numeric_features.tolist()])\n",
    "\n",
    "      #2b7b Now you need to impute missing categorical values.\n",
    "      imputercat = SimpleImputer(missing_values = np.nan, strategy = catimp)\n",
    "      imputercat = imputercat.fit(df[categorical_features.tolist()])\n",
    "      df[categorical_features.tolist()] = imputercat.transform(df[categorical_features.tolist()])\n",
    "      df['Credit_History'] = df['Credit_History'].astype('category')   # imputer set type back to numeric, so we correct it again\n",
    "\n",
    "      #2b8 Create a new dataframe X that includes only our feature variables\n",
    "      feat = df.loc[:, df.columns != 'Loan_Status']\n",
    "\n",
    "      #2b9 Create a new dataframe y that includes only our target variable\n",
    "      lab = df.Loan_Status\n",
    "\n",
    "      #2b10 Now you need to one hot encode the categorical features to make them machine readable.\n",
    "      feat = pd.get_dummies(feat)\n",
    "\n",
    "      #2b11 And re-code the response variable using a dictionary and replace\n",
    "      repmap={\"Y\": 1, \"N\": 0}\n",
    "      lab.replace(repmap, inplace=True)\n",
    "\n",
    "      #2b12 Feature engineer Household_Income_log (why log?) and add it to our features (X)\n",
    "      feat['Household_Income_log']=np.log(feat.ApplicantIncome + feat.CoapplicantIncome)\n",
    "\n",
    "      #2b13 Feature engineer EMI and add it to our features (X)\n",
    "      # incorrect --> feat['EMI']=(feat['LoanAmount']*0.06*(1.06**feat['Loan_Amount_Term']))/(12*(1.06**(feat['Loan_Amount_Term']-1)))\n",
    "      r = 0.06/12\n",
    "      feat['EMI']=np.ceil(feat['LoanAmount']*1000*r*((1+r)**feat['Loan_Amount_Term'])/((1+r)**feat['Loan_Amount_Term']-1)*100)/100\n",
    "\n",
    "      #2b14 Append our two new features to our numerical features\n",
    "      numeric_features=numeric_features.insert(len(numeric_features),'EMI')\n",
    "      numeric_features=numeric_features.insert(len(numeric_features),'Household_Income_log')\n",
    "\n",
    "      #2b15 Return the features (feat) and labels (lab)\n",
    "      return feat, lab, numeric_features"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XiqiXdpVHaq6"
   },
   "source": [
    "# 3 Load data file into a pandas dataframe\n",
    "data = pd.read_csv(\"loan_train.csv\") # parse to a Pandas DataFrame using pd.read_csv()\n",
    "display(f\"Observations in Dataset: {len(data)}\")\n",
    "\n",
    "# 4 Call preprocessing function and pass it the dataframe as well as the imputation strategies for (1) numerical and (2) categorical features\n",
    "data = prepro(data,'median','most_frequent')\n",
    "\n",
    "# 5 Separate features (X) and labels (y) from data object\n",
    "X=data[0]\n",
    "y=data[1]\n",
    "\n",
    "# 6 Get numerical features\n",
    "numeric_features = data[2]\n",
    "\n",
    "# 7 Verify that it worked\n",
    "display(X.head())\n",
    "display(y.head())\n",
    "display(numeric_features)\n",
    "display(f\"Observations after Preprocessing: {len(X)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O71YOPtmYSqt"
   },
   "source": [
    "# 2. Predicting Loan Eligibility using a Support Vector Machine\n",
    "\n",
    "**SVMs in a  Nutshell**: In machine learning, support-vector machines (SVMs, also support-vector networks) are ***supervised learning models*** with associated learning algorithms that analyze data used for ***classification*** and ***regression*** analysis.\n",
    "\n",
    "- Representation of the examples (observations) as points in space ...\n",
    "- ... mapped so that the examples of the separate categories are divided by a clear gap ...\n",
    "- ... that is as wide as possible.\n",
    "\n",
    "New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.\n",
    "\n",
    "![SVM](https://mapxp.app/BUSI488/svm3.png)\n",
    "\n",
    "\n",
    "If you would like to read-up on more details about SVMs and how the work, take a look at the following tutorial:\n",
    "[SVM tutorial](https://towardsdatascience.com/an-introduction-to-support-vector-machine-3f353241303b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "show_videos(video_urls=[\"https://www.youtube.com/embed/_YPScrckx28?si=4PGr0_ijAXwHINrw\",\"https://www.youtube.com/embed/Y6RRHw9uN9o?si=FsWJ_gWaAvd6MiPP\"], width=520)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "laioLfbK7Kfd",
    "outputId": "2ea3c350-1bd4-43a0-b9b3-4434832dc644"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style=\"display: flex;\"><div style=\"flex: 1;\">\n",
       "      <iframe width=\"520\" height = 280 src=\"https://www.youtube.com/embed/_YPScrckx28?si=4PGr0_ijAXwHINrw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
       "    </div><div style=\"flex: 1;\">\n",
       "      <iframe width=\"520\" height = 280 src=\"https://www.youtube.com/embed/Y6RRHw9uN9o?si=FsWJ_gWaAvd6MiPP\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O-EWUO0alkl"
   },
   "source": [
    "## 2.1 NOT SO FAST! Feature Scaling First\n",
    "\n",
    "We want to impose equal weights on our features and many ML models assume standardized data.\n",
    "\n",
    "To do so:\n",
    "\n",
    "1. We can create a function to rescale our data so that the mean is zero and standard deviation is unity (1) on all features, or we can use a minmaxscaler (with range 0 to 1).\n",
    "2. Apply that function to our numerical data\n",
    "3. Describe our numerical data afterwards\n",
    "\n",
    "**!** A good exercise would be to research what MinMaxScaler and StandardScaler do (both are from the scikit-learn library)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2C7xWv3ta1ke"
   },
   "source": [
    "#1 This function loops through columns in a feature set and defines a predefined scaler to each\n",
    "def scale_numeric(features, numeric_features, scaler):\n",
    "    for col in numeric_features.tolist():\n",
    "        features[col] = scaler.fit_transform(features[col].values.reshape(-1, 1))\n",
    "    return features\n",
    "\n",
    "#2 we can now define the scaler we want to use and apply it to our features\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scale_numeric(X.copy(), numeric_features, scaler)\n",
    "\n",
    "#3 Let's see if it worked\n",
    "X_scaled.describe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N4AqeaqZkyr"
   },
   "source": [
    "## 2.2 Split Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sTHGBctDZuhM"
   },
   "source": [
    "# 1 Split sample into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size=0.25, stratify=y)\n",
    "\n",
    "# 2 Check if data have right shape\n",
    "print(\"Train: Response Variable: \",y_train.shape)\n",
    "print(\"Train: Feature Variables: \",X_train.shape)\n",
    "print(\"Test: Response Variable: \",y_test.shape)\n",
    "print(\"Test: Feature Variables: \",X_test.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-ngLE04Z7L1"
   },
   "source": [
    "## 2.3 Instantiate Model, Fit it, and Predict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j-8A1jGiZ-q0"
   },
   "source": [
    "# 3 import Support Vector Classification (SVC) from sklearn\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 4 Instantiate Model\n",
    "svmachine = SVC(C=100, gamma=0.25, kernel='poly')\n",
    "     # c=regularization (penalty), gamma=fitting (over vs under), kernel=transformation function\n",
    "\n",
    "# 5 Train the model using the training sets\n",
    "svmachine.fit(X_train, y_train)\n",
    "\n",
    "# 6 Predict the response for test dataset\n",
    "y_pred = svmachine.predict(X_test)\n",
    "\n",
    "#7 Output the accuracy of our prediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy of SVM Classifier is {accuracy_score(y_test, y_pred):.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO7mPQY1hiMF"
   },
   "source": [
    "**QUESTION**: Did everyone's model have the same accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAT1LkmuhyyU"
   },
   "source": [
    "# 3. Cross-Validation\n",
    "\n",
    "***Model performance is dependent on way the data is split!***  \n",
    "\n",
    "That is problematic because:\n",
    "- A specific test-train-split is not representative of the model’s ability to generalize\n",
    "\n",
    "**Solution:** Cross-validation!\n",
    "\n",
    "#### Cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance\n",
    "\n",
    "Cross-validation, sometimes called *rotation estimation* or *out-of-sample testing*, is any of various similar model validation techniques for **assessing** how the results of a statistical analysis will **generalize** to an independent data set.\n",
    "\n",
    "Cross-validation is mainly used in settings where the **goal is prediction**, and one wants to estimate how accurately a predictive model will **perform in practice**.\n",
    "\n",
    "The **goal** of cross-validation is to:\n",
    "- Test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias\n",
    "- Give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n",
    "\n",
    "**Multiple Rounds**\n",
    "- One round of cross-validation involves:\n",
    "  * partitioning a sample of data into complementary subsets  \n",
    "  * performing the analysis on one subset (called the training set)  \n",
    "  * validating the analysis on the other subset (called the testing set)\n",
    "- Perform multiple rounds (do not seed!) and average performance\n",
    "- **Note:** We are splitting the training set in each round into train and test. We use the actual testing set later to evaluate model performance.\n",
    "\n",
    "\n",
    "#### Cross-validation and model performance:\n",
    "- 5 folds = 5-fold CV\n",
    "- 10 folds = 10-fold CV\n",
    "- k folds = k-fold CV\n",
    "- More folds = More computationally expensive\n",
    "\n",
    "***Conveniently, scikit-learn has built-in functionality to run cross-validation!***\n",
    "\n",
    "![K-Fold Cross-Validation sklearn](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png \"K-Fold Cross-Validation\")\n",
    "\n",
    "*sources: Scikit-learn, DataCamp and Wikipedia*\n",
    "\n",
    "---------\n",
    "\n",
    "> **You could also use the entire data for CV.** ***How? Why?***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J28_PigWh_z0"
   },
   "source": [
    "# Let's cross-validate our basic linear model\n",
    "\n",
    "# 1 Import from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 2 We start by splitting our data into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size=0.25, stratify=y)\n",
    "\n",
    "# 3 Instantiate the model (i.e., define it)\n",
    "cvsvm = SVC(C=100, gamma=0.25, kernel='poly')\n",
    "\n",
    "# 4 Run 5-fold cross-validation\n",
    "cv_results = cross_val_score(cvsvm, X_train, y_train, cv=5)\n",
    "print(cv_results)\n",
    "\n",
    "# 5 Find average performance\n",
    "print(f\"Average CV Accuracy of SVM Classifier on Training Data is {cv_results.mean():.2%} with standard deviation of {cv_results.std():.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**QUESTION**: Is everyone's CV Accuracy the same?"
   ],
   "metadata": {
    "id": "lUUFojjxtN5r"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# 6. Let's explore the impact of random train-test-splits on mean CV accuracy\n",
    "cvf = 5 #number of folds per iteration\n",
    "itr = 250 # number of iterations #### WARNING: I'm fitting the model 250 x 5 folds = 1250 times here! That can take a VERY LONG TIME with bigger data and more complex models!\n",
    "\n",
    "# 7. Set-up dataframe to hold results and instantiate our model\n",
    "acc = pd.DataFrame(columns=['5CV_Accuracies', 'MeanAccuracy', 'StD', 'Test_Accuracy']) # set-up dataframe with columns for variables of interest\n",
    "cvsvm = SVC(C=100, gamma=0.25, kernel='poly') # instantiate model\n",
    "\n",
    "# 8. Loop over splitting the data, fitting the model with CV, saving the model CV performance, predicting test data, evaluating test performance and saving it\n",
    "for i in range(1,itr+1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size=0.25, stratify=y) # no random seed = different random splits\n",
    "    cv_results = cross_val_score(cvsvm, X_train, y_train, cv=cvf) # CV of model\n",
    "    cvsvm.fit(X_train, y_train) # fit model to entire train data\n",
    "    y_pred = cvsvm.predict(X_test) # predict test data\n",
    "    acc.loc[len(acc.index)] = [cv_results, cv_results.mean(), cv_results.std(),accuracy_score(y_test, y_pred)] #put performance results in dataframe\n",
    "    if i % 10 == 0: print(f\"{i*cvf} models fitted and evaluated ...\") # print status: every iteration that is evenly divisible by 10\n",
    "\n",
    "# 9. Create Historgram of Model performance with 5-Fold CV at different train-test-splits\n",
    "allACC = pd.DataFrame(np.concatenate(acc['5CV_Accuracies'].to_numpy()).ravel(), columns=['Train_Accuracy']) #I'm putting the arrays that contain the accuracies of each fold of each run in the loop into a new dataframe\n",
    "hist = allACC.hist(column='Train_Accuracy',bins=25,figsize=(8,8))\n",
    "\n",
    "# 10. Print Mean and Standard Deviation\n",
    "print(f\"\\nMean Train CV Accuracy is {allACC.Train_Accuracy.mean():.2%} with Standard Deviation of {allACC.Train_Accuracy.std():.2%}\")"
   ],
   "metadata": {
    "id": "ndtLiNXfZoga"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 11. Let's look at the Accuracies across different test sets\n",
    "print(f\"Mean Test Accuracy is {acc.Test_Accuracy.mean():.2%} with Standard Deviation of {acc.Test_Accuracy.std():.2%}\\n\")\n",
    "hist = acc.hist(column='Test_Accuracy',bins=25,figsize=(8,8))"
   ],
   "metadata": {
    "id": "FGge6XQfK8rE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------\n",
    "\n",
    "> Again, you could also use the entire data for CV. ***When and Why?***"
   ],
   "metadata": {
    "id": "fwXdNQ9jyU1I"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEb2nwfOkIcm"
   },
   "source": [
    "# 4. Hyperparameter tuning\n",
    "\n",
    "Most models have a number of parameters that need to be set.\n",
    "These are commonly referred to as **Hyperparameters**.  \n",
    "\n",
    "*How should you set Hyperparameters?*\n",
    "\n",
    "- k-Nearest Neighbors: n_neighbors (i.e., k), etc.\n",
    "- Support Vector Machines: C and Gamma, etc.\n",
    "- Random Forest: n_estimates, max_depth, max_features, etc.\n",
    "- ***Hyperparameters cannot be learned by fitting the model***\n",
    "\n",
    "***Unfortunately, there is no \"one-size-fits-all\" approach!***   \n",
    "\n",
    "Why?\n",
    "\n",
    "#### Choosing the best Hyperparameter\n",
    "- Try a bunch of different hyperparameter values\n",
    "- Fit all of them separately\n",
    "- See how well each performs\n",
    "- Choose the best performing one\n",
    "- It is essential to use cross-validation!!!\n",
    "\n",
    "*Let's fit a different model to our data:* **A Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuSmcZtjlXgu"
   },
   "source": [
    "## 4.1 Manual Hyperparameter tuning\n",
    "\n",
    "- Let's tune a Random Forest Classifier\n",
    "   - Number of Trees: n_estimators\n",
    "   - Tree depth: max_depth\n",
    "   - Features to select from at each node: max_features\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mz1kyYSYlWz3"
   },
   "source": [
    "# 1 Let's tune the Hyperparameters of a Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 2 Split our data again and seed the random number generator to ensure consistent results\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,y, test_size=0.25, stratify=y, random_state=23)\n",
    "\n",
    "# 3 Instantiate Model (i.e., define it)\n",
    "rfbase = RandomForestClassifier(n_estimators = 25, max_depth = 25, max_features = 3, random_state=23)\n",
    "\n",
    "# 4 Fit Model\n",
    "rfbase.fit(X_train, y_train)\n",
    "\n",
    "# 5 Make Prediction on test data\n",
    "y_pred = rfbase.predict(X_test)\n",
    "\n",
    "# 6 Output the accuracy of our prediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy of Random Forest Classifier on Training Data is {accuracy_score(y_train, rfbase.predict(X_train)):.2%}\")\n",
    "print(f\"Accuracy of Random Forest Classifier on Testing  Data is {accuracy_score(y_test, y_pred):.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDMS_nXemTdu"
   },
   "source": [
    "## 4.2 Grid-Search for Hyperparameter tuning\n",
    "\n",
    "* We can see that we have improved our model as we have added features and trained new models.\n",
    "* At the point that we feel comfortable with a good model, we can start to tune the parameters of the model.\n",
    "* There are a number of ways to do this (e.g., by manually trying different model parameters)\n",
    "* An automated technique is called **Grid-Search** (essentially a *brute-force* search for optimal hyperparameters)\n",
    "\n",
    "#### Important Note\n",
    "\n",
    "- How well can the model perform on never before seen data?\n",
    "- Using ALL data for cross-validation is not ideal\n",
    "- Split data into training and hold-out set at the beginning\n",
    "- Perform grid search cross-validation on training set\n",
    "- Choose best hyperparameters and evaluate on hold-out set (i.e., test set)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BzTU5BOrkIjF"
   },
   "source": [
    "%%time\n",
    "# GridSearchCV for a Random Forest\n",
    "# Grid search is a 'brute force' search, one that will explore every possible combination of parameters that you provide it\n",
    "\n",
    "# 1 Import Grid Search with Cross Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 2 First decide on the model to use and instantiate it\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=23)\n",
    "\n",
    "# 3 Then define the parameters we want to search as a dictionary. Explore the documentation to what other options are available\n",
    "params = {'n_estimators': [5, 10, 25, 50, 100], 'max_depth' : [5, 10, 25, 50], 'max_features' : [3, 5, 7]}\n",
    "\n",
    "# 4 Next, create a grid search object with our chosen model and parameters.\n",
    "grid = GridSearchCV(rf, params, cv=5, n_jobs=-1, verbose=3)\n",
    "\n",
    "# 5 Fit our model to the data as before\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 6 Output of the grid search function: get the best_estimator - the model and parameters that scored best on the training data -\n",
    "#   and save it as a new a model\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"Best Paramaters are: {grid.best_params_}\")\n",
    "\n",
    "# 7. Examine model performance\n",
    "print(f\"Mean CV Accuracy of best Random Forest Classifier is {grid.best_score_:.2%}\")\n",
    "\n",
    "# 8 Now we use the best estimator to predict\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 9 Output the accuracy of our prediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Best Estimator Accuracy on Test Data is {accuracy_score(y_test, y_pred):.2%} \\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**An exhaustive search of the parameter grid can take a long time.** Particularly:\n",
    "- when there are many parameters to tune\n",
    "- when these parameters can assume many different values\n",
    "- on a slow computer (or limited virtual environment, like free CoLab)"
   ],
   "metadata": {
    "id": "A0F4dQHuM6-6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***To speed things up***: Use RandomizedSearchCV.\n",
    "\n",
    "In contrast to `GridSearchCV`, not all parameter values are tried, but rather a fixed number of parameter settings is sampled from specified distributions. The number of parameter settings that are tried is given by `n_iter`."
   ],
   "metadata": {
    "id": "dQ6xeDK5NJce"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df-wu6FEydAL"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# RandomizedSearchCV for a Random Forest\n",
    "# The number of parameter settings that are tried is given by n_iter.\n",
    "\n",
    "# 1. First, import RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 2. Define search space\n",
    "param_grid = {'n_estimators': [5, 10, 25, 50, 100], 'max_depth' : [5, 10, 25, 50], 'max_features' : [3, 5, 7]}\n",
    "\n",
    "# 3. Instatiante base estimator\n",
    "rf = RandomForestClassifier(random_state=23)\n",
    "\n",
    "# 4. Instantiate Search Model\n",
    "search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=15, n_jobs=-1, cv=5, verbose=3)\n",
    "\n",
    "# 5. Fit the model (i.e., train it on training data)\n",
    "search.fit(X_train, y_train);\n",
    "\n",
    "# 6. Output optimal Hyperparameter combination\n",
    "print(f\"Best Parameters are: {search.best_params_}\")\n",
    "\n",
    "# 7. Examine model performance\n",
    "print(f\"Mean CV Accuracy of best Random Forest Classifier is {search.best_score_:.2%}\")\n",
    "\n",
    "# 8 Now we use the best estimator to predict\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 9 Output the accuracy of our prediction\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Best Estimator Accuracy on Test Data is {accuracy_score(y_test, y_pred):.2%} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWIBLqNuogW4"
   },
   "source": [
    "# 5. The Problem with Accuracy: Confusion Matrices are Better!\n",
    "\n",
    "Measuring model performance with Accuracy:  \n",
    "- Fraction of correctly classified samples  \n",
    "- Not always a useful metric\n",
    "\n",
    "### Class imbalance example: Tarheel Party crashed by Blue Devils  \n",
    "- 300 Tarheels had a great party until 3 Blue Devils managed to sneak in and cause trouble\n",
    "  - 99% of guests are Tarheels, 1% are Blue Devils\n",
    "- Can you build a classifier that predicts almost perfectly whether a party guest is a Blue Devil seeking to disrupt?\n",
    "\n",
    "<details><summary>Here is one:</summary>\n",
    "\n",
    "- Just predict \"TarHeel\" for everyone.\n",
    "  - 99% Accuracy\n",
    "  - Horrible at actually classifying guest as Blue Devils!  \n",
    "  - Fails at its original purpose  \n",
    "\n",
    "  </details>\n",
    "<ul>\n",
    "<li> Basic problem: Misclassifying a Blue Devil has more consequences than misclassifying a Tarheel.</li>\n",
    "<li> We need more nuanced metrics</li>\n",
    "</ul>\n",
    "\n",
    "### Diagnosing classification predictions: The Confusion Matrix  \n",
    "\n",
    "Act vs Pred  | Predicted: Devil | Predicted: Tarheel\n",
    "--- | --- | ---\n",
    "Actual: Devil | True Positives $t_p$ | False Negatives $f_n$\n",
    "Actual: Tarheel | False Positives $f_p$| True Negatives $t_n$\n",
    "\n",
    "<p style=\"text-align: left; font-size:120%; font-weight: normal; font-style: normal;\">\n",
    "$\\textit{Accuracy} = \\frac{t_p + t_n}{t_p + t_n + f_p + f_n}$ <br><br>\n",
    "$\\textit{Precision} = \\frac{t_p}{t_p + f_p}$    <br><br>    \n",
    "$\\textit{Recall} = \\frac{t_p}{t_p + f_n}$   <br><br>    \n",
    "$\\textit{F1 score} = 2 \\times \\frac{{\\textit{Precision}} \\times {\\textit{Recall}}}{{\\textit{Precision}} + {\\textit{Recall}}}$\n",
    "</p>\n",
    "\n",
    "**Precision** measures the ability of the classifier to not label as positive a sample that is negative\n",
    "- High Precision: Not many Tarheels predicted as Devils  \n",
    "\n",
    "**Recall** measures the ability of the classifier to find all the positive samples.\n",
    "- High Recall: Predicted most Devils correctly\n",
    "\n",
    "**The F1 score** is the *harmonic mean* of the precision and recall:\n",
    "- an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "- Why harmonic mean? Note that Precision and Recall are fractions with the same numerator: $t_p$.  Harmonic mean essentially averages the denominators under the common numerator.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sLqBSMcWpGgR"
   },
   "source": [
    "# Generate the confusion matrix and classification report\n",
    "\n",
    "# 1 First, import confusion matrix and classification report from sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 2 Show the accuracy of our prediction\n",
    "print(f\"Accuracy of best Random Forest Classifier is {accuracy_score(y_test, y_pred):.2%}\")\n",
    "\n",
    "# 3 Show Classification Report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 4 Visualize the confusion matrix to make it easier to read\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix_df = pd.DataFrame(con_matrix, ('Decline', 'Approve'), ('Decline', 'Approve'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "plt.ylabel('Actual', fontsize = 14)\n",
    "plt.xlabel('Predicted', fontsize = 14)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXSQp6T0s8yF"
   },
   "source": [
    "\n",
    "**Precision** is the ability of the classifier to not label as positive a sample that is negative\n",
    "\n",
    "**Recall** is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**The F1 score** is the *harmonic mean* of the precision and recall, which is the appropriate mean since both are ratios with the same numerator and different denominators.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **What matters more to the Fintech:** Precision or Recall?"
   ],
   "metadata": {
    "id": "mt0890XJPCEM"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYU9CkDEuAPv"
   },
   "source": [
    "# 6. Binary Outputs vs. Probabilities\n",
    "\n",
    "In a classification problem, we ***may decide to predict the class values directly***. For example, the customer is eligible for a loan or not (Y vs. N or 1 vs. 0)\n",
    "\n",
    "Alternately, it can be ***more flexible to predict the probabilities for each class instead***.    \n",
    "\n",
    "The reason for this is to ***provide the capability to choose and even calibrate the threshold*** for how to interpret the predicted probabilities.\n",
    "\n",
    "For example, by default we might use a threshold of 0.5, meaning that:\n",
    "- a probability 0.0 >= *P* < 0.5 is a negative outcome (0)\n",
    "- a probability from 0.5 >= p <= 1.0 is a positive outcome (1)  \n",
    "\n",
    "***This threshold can be adjusted*** to tune the behavior of the model for a specific problem. An example would be to reduce more of one or another type of error.\n",
    "\n",
    "**>>Take-Away<<**: We can change this threshold if, for example, we require a higher confidence to take a certain action such as giving out a loan."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "58kp_B8auHQ-"
   },
   "source": [
    "# 1a Show binary classes\n",
    "y_pred = best_model.predict(X_test)\n",
    "#print(y_pred)\n",
    "\n",
    "# 1b Show probabilities\n",
    "probs = best_model.predict_proba(X_test)[:, 1]\n",
    "#print(probs)\n",
    "\n",
    "# 1c Show them together for those where probabilities are closer to 0.5\n",
    "results= pd.DataFrame({'classes':y_pred, 'probabilities':probs})\n",
    "results[(results.probabilities >.3) & (results.probabilities <.7)]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O3MIVHaoyar3"
   },
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "# 2a Set threshold level\n",
    "threshold = 0.7\n",
    "\n",
    "# 2b Get probabilities from model\n",
    "probs = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2c Assign prediction classes using new threshold\n",
    "predicted = (probs >= threshold).astype('int')\n",
    "\n",
    "# 2d Evaluate prediction\n",
    "print(f\"Threshold @{threshold:.2%}: Accuracy {accuracy_score(y_test, predicted):.2%}  Precision {precision_score(y_test, predicted):.2%}  Recall {recall_score(y_test, predicted):.2%}\")\n",
    "print(f\"Threshold @50.0%: Accuracy {accuracy_score(y_test, y_pred):.2%}  Precision {precision_score(y_test, y_pred):.2%}  Recall {recall_score(y_test, y_pred):.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y-UR88MRzYgM"
   },
   "source": [
    "# 3 Output Classification Report\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "# 4 Visualize the confusion matrix to make it easier to read\n",
    "con_matrix = confusion_matrix(y_test, predicted)\n",
    "confusion_matrix_df = pd.DataFrame(con_matrix, ('Decline', 'Approve'), ('Decline', 'Approve'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "plt.ylabel('Actual', fontsize = 14)\n",
    "plt.xlabel('Predicted', fontsize = 14)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### ***We can check the trade-off between precision and recall visually by plotting how they change across different thresholds***"
   ],
   "metadata": {
    "id": "WFmQ91d5I-R1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "display = PrecisionRecallDisplay.from_estimator(best_model, X_test, y_test, name=\"Random Forest\")\n",
    "_ = display.ax_.set_title(\"Trade-Off: Precision-Recall Curve\")"
   ],
   "metadata": {
    "id": "q4vkOr3mI-gk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P09SKPyauFmG"
   },
   "source": [
    "# 7. Receiver Operating Characteristic (ROC) Curves\n",
    "Classification reports and confusion matrices are great methods to quantitatively evaluate model performance.\n",
    "\n",
    "**ROC curves provide a way to visually evaluate models**  \n",
    "- Commonly used graph that summarizes the performance of a classifier over all possible thresholds\n",
    "- Generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis)...\n",
    "- ... as we vary the threshold for assigning customers to a given segment\n",
    "  \n",
    "   \n",
    "\n",
    "- ROC visually compares ***signal*** (True Positive Rate) against ***noise*** (False Positive Rate)\n",
    "- Model performance is determined by inspecting the area under the ROC curve (or AUC)   \n",
    "- Best possible AUC is 1\n",
    "- Worst possible AUC is 0.5 (the 45 degree random line)\n",
    "\n",
    "**>>Question<<:** What would a AUC less than 0.5 tell us?  \n",
    "\n",
    "*Many classifiers in scikit-learn have a .predict_proba() method which returns the probability of a given sample being in a particular class.*\n",
    "\n",
    "#### Having built a Random Forest model, we can now evaluate its performance by plotting an ROC curve.\n",
    "- We will use of the .predict_proba() method and become familiar with its functionality.\n",
    "- We start by defining a function that caculates the AUC for training and test set as well as visualizes the ROC curve in a plot\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ypxCyd5AFyPQ"
   },
   "source": [
    "# 1 Import ROC_curve and roc_auc_score from sklearn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# 2 Let's create a function that allows us to quickly and conveniently evaluate ROC and AUC for Classifier Predictions\n",
    "#   (this function might come in handy one day)\n",
    "\n",
    "'Function that evaluates ROC and AUC'\n",
    "def evaluate_model(train_props,y_train,test_probs,y_test):\n",
    "  # 2a Calcualte AUC\n",
    "  train_AUC = roc_auc_score(y_train, train_probs)\n",
    "  test_AUC = roc_auc_score(y_test, test_probs)\n",
    "  base_AUC = roc_auc_score(y_test, [1 for _ in range(len(y_test))])\n",
    "\n",
    "  # 2b Print to screen\n",
    "  print(f'Baseline ROC AUC Sc: {base_AUC:.2f}')\n",
    "  print(f'Test  ROC AUC Score: {test_AUC:.2f}')\n",
    "  print(f'Train ROC AUC Score: {train_AUC:.2f}')\n",
    "\n",
    "  # 2c Calculate false positive rates and true positive rates\n",
    "  base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n",
    "  test_model_fpr, test_model_tpr, _ = roc_curve(y_test, test_probs)\n",
    "  train_model_fpr, train_model_tpr, _ = roc_curve(y_train, train_probs)\n",
    "\n",
    "  # 2d Plot both curves\n",
    "  plt.figure(figsize = (8, 6))\n",
    "  plt.rcParams['font.size'] = 16\n",
    "  plt.plot(base_fpr, base_tpr, 'b', label = 'Baseline')\n",
    "  plt.plot(test_model_fpr, test_model_tpr, 'r', label = 'Test')\n",
    "  plt.plot(train_model_fpr, train_model_tpr, 'g', label = 'Train')\n",
    "  plt.legend();\n",
    "  plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "  return"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-XI2DwRmGKfd"
   },
   "source": [
    "## Let's evaluate our Original Random Forest Tree's performance using AUC\n",
    "\n",
    "# 3a Get probabilities for training and testing\n",
    "train_probs = rfbase.predict_proba(X_train)[:, 1]\n",
    "test_probs = rfbase.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 3b Evaluate\n",
    "evaluate_model(train_probs,y_train,test_probs,y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-YOb04vUG7pY"
   },
   "source": [
    "## Let's evaluate our Best Model Random Forest Tree's performance using AUC\n",
    "\n",
    "# 4a Get probabilities for training and testing\n",
    "train_probs = best_model.predict_proba(X_train)[:, 1]\n",
    "test_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4b Evaluate\n",
    "evaluate_model(train_probs,y_train,test_probs,y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF2Yzb_trUl2"
   },
   "source": [
    "# 8. Algorithmic Bias\n",
    "\n",
    "- Unjust, unfair, or prejudicial treatment of people\n",
    "- Related to race, income, sexual orientation, religion, gender\n",
    "- And other characteristics historically associated with discrimination and marginalization\n",
    "- When and where they manifest in algorithmic systems or algorithmically aided decision-making\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*“Although neural networks might be said to write their own programs, they do so towards ***goals set by humans, using data collected for human purposes***.\n",
    "If the data is skewed, even by accident, the computers will ***amplify injustice***.”*  \n",
    "\n",
    "\n",
    "— The Guardian  \n",
    "https://www.theguardian.com/commentisfree/2016/oct/23/the-guardian-view-on-machine-learning-people-must-decide\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Can we find evidence that our model is biased?**  \n",
    "\n",
    "What about gender bias and loan eligibility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gem_EXoWu5TM"
   },
   "source": [
    "## 8.1 Testing for Fairness & Inclusion in Binary Classification\n",
    "\n",
    "**Disaggregated Evaluation**\n",
    "- Create for each (subgroup, prediction) pair\n",
    "- Compare across subgroups  \n",
    "\n",
    "*Example*\n",
    "- Women, approve loan\n",
    "- Men, approve loan\n",
    "\n",
    "\n",
    "**Intersectional Evaluation**\n",
    "- Create for each (subgroup1, subgroup2, prediction) pair\n",
    "- Compare across subgroups\n",
    "\n",
    "*Example*\n",
    "- Women with children, approve loan\n",
    "- Men without children, approve loan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAHY6unHv1gq"
   },
   "source": [
    "## 8.2 Inspect Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wJ-1u-oJsR34"
   },
   "source": [
    "# 1a Identify Female and Male applicants in Test Set\n",
    "idxM=X_test.index[X_test['Gender_Male'] == 1].tolist()\n",
    "idxF=X_test.index[X_test['Gender_Female'] == 1].tolist()\n",
    "print(f\"Test cases for Males: {len(idxM)}\")\n",
    "print(f\"Test cases for Females: {len(idxF)}\")\n",
    "\n",
    "# 1b Now predict for Females and Males separately\n",
    "y_predF = best_model.predict(X_test.loc[idxF])\n",
    "y_predM = best_model.predict(X_test.loc[idxM])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r4SXwqWssR62"
   },
   "source": [
    "# 2a Confusion Matrix for Females\n",
    "con_matrix = confusion_matrix(y_test.loc[idxF], y_predF)\n",
    "confusion_matrix_df = pd.DataFrame(con_matrix, ('Decline', 'Approve'), ('Decline', 'Approve'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\",square=True)\n",
    "bottom, top = heatmap.get_ylim()\n",
    "heatmap.set_ylim(bottom + 0.5, top - 0.5)\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "heatmap.set_title('Females')\n",
    "plt.ylabel('Actual', fontsize = 14)\n",
    "plt.xlabel('Predicted', fontsize = 14)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BWHXVWgHsf5p"
   },
   "source": [
    "# 2b Confusion Matrix for Males\n",
    "con_matrix = confusion_matrix(y_test.loc[idxM], y_predM)\n",
    "confusion_matrix_df = pd.DataFrame(con_matrix, ('Decline', 'Approve'), ('Decline', 'Approve'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\", cmap=\"Blues\",square=True)\n",
    "bottom, top = heatmap.get_ylim()\n",
    "heatmap.set_ylim(bottom + 0.5, top - 0.5)\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "heatmap.set_title('Males')\n",
    "plt.ylabel('Actual', fontsize = 14)\n",
    "plt.xlabel('Predicted', fontsize = 14)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0n1UbursuRY"
   },
   "source": [
    "**Remember the definitions of Precision, Recall and F1 Score from Section 5?**\n",
    "\n",
    "**Precision** is intuitively the ability of the classifier not to label as positive a sample that is negative\n",
    "\n",
    "**Recall** is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**The F1 score** is the harmonic mean of the precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgsbnR-7uEph"
   },
   "source": [
    "## 8.3 Equality of Opportunity\n",
    "\n",
    "**Fairness criterion:**   \n",
    "- *Recall* is equal across subgroups\n",
    "- We should grant the loan to an equal proportion of applicants from the qualified fraction of each subgroup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L2qV9Li2sqmd"
   },
   "source": [
    "# 0 Import precision and recall scores from sklearn\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# 1 Let's compare recall\n",
    "print(f\"Recall for Females: {recall_score(y_test.loc[idxF], y_predF):.2f}\")\n",
    "print(f\"Recall for Males:   {recall_score(y_test.loc[idxM], y_predM):.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg-jQZNkt4Yw"
   },
   "source": [
    "## 8.4 Predictive Parity\n",
    "\n",
    "**Fairness criterion:**\n",
    "- *Precision* is equal across subgroups\n",
    "-\n",
    "A prediction for an applicant should reflect the candidate’s real eligibility for their loan"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iKjzMQcrufui"
   },
   "source": [
    "# 2 Let's compare precision\n",
    "print(f\"Precision for Females: {precision_score(y_test.loc[idxF], y_predF):.2f}\")\n",
    "print(f\"Precision for Males:   {precision_score(y_test.loc[idxM], y_predM):.2f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z52szOHXwgrD"
   },
   "source": [
    "---\n",
    "\n",
    "*Even if we don't find evidence for a bias against men or women,*  \n",
    "*this does not mean that there isn't any bias.*  \n",
    "    \n",
    "> **Why?**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ]
}
