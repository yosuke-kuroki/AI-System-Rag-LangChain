{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSf9iLFAToaQ"
   },
   "source": "# Predictive Modelling II: Regression"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6HsoHtG3LcX"
   },
   "source": [
    "# 1. **Supervised Machine Learning**: Predictions from data with *labels* and *features*  \n",
    "- Recommendation systems\n",
    "- Email subject optimization\n",
    "- Churn prediction\n",
    "\n",
    "![Overview Machine Learning](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3BdjHruDGtgHBZUgHbpJA.png)\n",
    "*Source: COGNUB 2016*\n",
    "\n",
    "***X's and Y's***:\n",
    "* Unsupervised Learning operates on X's only (i.e., features)\n",
    "* Supervised Learning requires X's (features) and Y's (labels)\n",
    "\n",
    "\n",
    "##### **Hint:** Many Names, same thing:\n",
    "\n",
    "- Features = predictor variables = independent variables = X's\n",
    "- Target variable = dependent variable = response variable = labels = Y\n",
    "\n",
    "##### **Important Concept:** We can think of all features (i.e., columns) of an observation (i.e., row) as a ***feature vector*** (of that observation).\n",
    "- At this point in the course, these features are structured data that are directly interpretable.   \n",
    "- Later in this course, we will work with ***latent feature vectors*** where features are encoded (or embedded) in vectors. These latent features are not directly intepretable, but are very useful in dimensionality reduction, natural language processing, and AI applications in general.\n",
    "- You can think of a ***latent feature vector*** as a row (i.e., an observation) with many columns that contain numbers. The meaning of those numbers is, however, not immediately apparent. Yet, there is information embedded in these latent feature vectors that can be of interest/value to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zus_wATjV0Y5"
   },
   "outputs": [],
   "source": [
    "# We first import a number of libraries that we will be using in today's class\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting packages we'll use\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Rather than importing the whole sklearn library, we will import only certain modules\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drUapZC-ToaY"
   },
   "source": [
    "# 2. The Data: Housing Prices in California\n",
    "\n",
    "In today's class we explore a public dataset on median house prices in blocks in California that can be found already split into training and test sets on your Google Drive in sample_data.  (Information on the original source: https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)\n",
    "We'll use a version from the book Hands on Machine Learning by Aurelien Geron, since it adds a category feature, ocean_proximity.\n",
    "\n",
    "**Today's objective** is to predict housing prices accurately and explore which factors contribute to them most.  \n",
    "\n",
    "We load the data set below and display a description of the variables.   \n",
    "\n",
    "**?** Which factors do you think will influence house prices upward and downward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Jd9XmWKkM68"
   },
   "outputs": [],
   "source": [
    "# 1. Connect your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "# 2. Change path to where the class files are: Let's assume you've created a folder 488 on your Google drive where you download Class folders from Canvas\n",
    "%cd /content/gdrive/MyDrive/488/Class22\n",
    "# 3. Look what's in the folder\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnRNa6reToac"
   },
   "source": [
    "## 2.1 Read data into a single data frame\n",
    "Later we will split out the features X from target y = `median_house_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppIf0aBwV0ZT"
   },
   "outputs": [],
   "source": [
    "# 1. Read file into a dataframe\n",
    "df = pd.read_csv(\"housing.csv\")\n",
    "# 2. Check rows and columns\n",
    "print(df.shape)\n",
    "# 3. output first 5 observations\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raMx00iGVo42"
   },
   "source": [
    "## 2.2 Explore Numerical and Categorical Data\n",
    "With the data collected, it will make sense to do some **feature engineering** -- making new columns by calculations on input columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCClpUxNrFvB"
   },
   "source": [
    "### 2.2.1 Numerical data\n",
    "\n",
    "* `describe()` shows the numeric features, and lists `count`, or number of data entries, the `mean` value, and the `standard deviation`, `min`, `max` and `quartile` values.\n",
    "* The ranges of feature values differ quite a lot, so we can start to think about whether to normalize, or to eliminate outliers.\n",
    "* We'll want to make some new per capita or per household features that are more comparable across blocks with different populations.\n",
    "* We'll use histograms and boxplots to summarize the feature distributions, using the low-level matplotlib to start with. By wrapping the plot code in a function, we can reapply it after changing variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bUVzRTKV0Ze"
   },
   "outputs": [],
   "source": [
    "# 1. Get some summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIOKQnrOToaf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2. Define a function that creates histograms and boxplots for all numerical variables\n",
    "def plot_numerics(df): # we loop through all the numeric columns\n",
    "  for col in df.columns:\n",
    "    if df[col].dtype == 'float64':\n",
    "      # and for each column we create space for one row with 2 charts\n",
    "      f, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "      # our first chart is a histogram and we set the title\n",
    "      df[col].hist(bins = 50, ax = axes[0])\n",
    "      axes[0].set_title('Distribution of '+ col)\n",
    "      # our second column is the boxplot\n",
    "      df.boxplot(column = col, ax = axes[1])\n",
    "      # we then use this command to display the charts\n",
    "      plt.show()\n",
    "\n",
    "# 3. Call function and pass our dataframe to it\n",
    "plot_numerics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXoDYJzxVubY"
   },
   "source": [
    "#### Recap:\n",
    "* A `histogram` tells is the number of times, or frequency, a value occurs within a `bin`, or bucket, that splits the data (and which we defined). A histogram shows the frequency with which values occur within each of these bins, and can tell us about the distribution of data.\n",
    "* A `boxplot` captures within the box the `interquartile range`, the range of values from Q1/25th percentile to Q3/75th percentile, and the median value. It also captures the `min` and `max` values of each feature.\n",
    "* Together, these charts show us the distribution of values for each feature. We can start to make judgements about how to treat the data, for example whether we want to deal with outliers; or whether we want to normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoeqUJSuqQ0D"
   },
   "source": [
    "### 2.2.2 Dealing with the category data\n",
    "* `ocean_proximity` contains category data, so we convert it to category type.  \n",
    "* We can then make a boxplot of the median_house_value by category:\n",
    "Inland houses clearly appear lower, and island higher, so this may be an important categorical variable.\n",
    "* On the other hand, we may want to drop the Island category data, since it is only 5 houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSYLcPFiA6Ic"
   },
   "outputs": [],
   "source": [
    "# 1. See how many observations we have per category\n",
    "df.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bOU6Iznk5dP"
   },
   "outputs": [],
   "source": [
    "# 2. optional: drop ISLAND\n",
    "df = df[df['ocean_proximity'] != 'ISLAND']\n",
    "df.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1BVNskibOGM"
   },
   "outputs": [],
   "source": [
    "# 3. Typecast Ocean Proximity to categorical\n",
    "df['ocean_proximity'] = df['ocean_proximity'].astype('category')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALETmPeUuM4Q"
   },
   "outputs": [],
   "source": [
    "# 4. Seaborn can show us the shape of the data with a simple scatter plot\n",
    "# and also apply categories to hue or style.\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(data=df, x='longitude', y='latitude', hue='ocean_proximity', style='ocean_proximity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oMlQMTiV0aS"
   },
   "outputs": [],
   "source": [
    "# 5. Let's plot values by category here\n",
    "f, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "df.boxplot(column='median_house_value', by = 'ocean_proximity', ax = axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e45bovZToai"
   },
   "source": [
    "### 2.2.3 Let's examine our response (or target) variable\n",
    "* seaborn is a package on top of matplotlib that has a nicer color scheme and works with pandas to make some plots easier. Experiment with plots showing two columns together.\n",
    "* It seems that blocks whose median house is over $500K were all lumped together.  If we want accurate predictions, we'll have to discard these as outliers. Similarly for houses over 50 years.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64AP3rwTcOrK"
   },
   "outputs": [],
   "source": [
    "# 1. Let's visualize the distirubtions of some variables and variable combinations\n",
    "sns.histplot(data=df, x='median_house_value')\n",
    "#sns.histplot(data=df, x='median_house_value', y='median_income') # try me\n",
    "#sns.scatterplot(data=df, x='median_house_value', y='median_income') # try me\n",
    "#sns.histplot(data=df[df.population>4000], x='median_house_value') #, y='population') # try me\n",
    "#sns.histplot(data=df, x='median_house_value', y='housing_median_age') # try me\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFZOKKlddJwY"
   },
   "outputs": [],
   "source": [
    "# 2. We can also look at the counts for each house value (is that really helpful?)\n",
    "df['median_house_value'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL5zpORqV0ad"
   },
   "source": [
    "### 2.2.4 Correlation and Multicollinearity\n",
    "\n",
    "We can generate a heatmap based on the correlation between features (and the response variable) to visually explore which features:  \n",
    "- might be most relevant in explaining our response variable\n",
    "- are strongly correlated so that we might not want to include them jointly in our predicition model\n",
    "\n",
    "**?** What are the features that are most positively correlated?  Why should you not be surprised?\n",
    "\n",
    "**?** What are the features that are most negatively correlated, and why?  \n",
    "\n",
    "**?** What correlations with median_house_value are farthest from zero?  Are they what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjb52AX_Toaj"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12)) # if you need this correlation heatmap to be bigger\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr,  vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm', annot = True,\n",
    "            mask = np.triu(corr));  # masks upper part of triangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kADGrF2a3-I3"
   },
   "source": [
    "## 2.3 Feature selection, feature transformation, and feature engineering\n",
    "This is where you have chances to customize what features you want to consider in your model.\n",
    "* For a small number of records, `total_bedrooms` is not reported.  Since above we saw that 20% of `total_rooms` being bedrooms is typical for the other data, we can impute at this value. (What other ways might you consider imputing?)\n",
    "\n",
    "\n",
    "* We can replace the category by \"One-Hot Encoding\". We make dummy or indicator variables for each possibility (1=in that category).  This takes memory, but allows these to be used as numbers in regression.\n",
    "\n",
    "\n",
    "* The numbers of rooms is closely correlated with population, but rooms per household or rooms per person might be more informative for house price.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn8exykb8R2s"
   },
   "outputs": [],
   "source": [
    "# 1. Missing data: replace nans in total_bedrooms by imputing them to be 20% of all rooms\n",
    "df.loc[df['total_bedrooms'].isna(), 'total_bedrooms'] = 0.20*df['total_rooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V08E4dL7s8JW"
   },
   "outputs": [],
   "source": [
    "# 2. Add indicator variables for the ocean_proximity values, except the first.\n",
    "# (When no other is selected, then we know the first must have been selected.)\n",
    "df = pd.get_dummies(df, columns=['ocean_proximity'], prefix=\"prox\", drop_first=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAPmDDmj104w"
   },
   "outputs": [],
   "source": [
    "# 3. Save before further feature engineering\n",
    "df_preFE = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZM-7rPcZmM4"
   },
   "outputs": [],
   "source": [
    "# 4. Engineer some new featurs that might be meaningful\n",
    "# (Comment out or drop features you don't want to add)\n",
    "\n",
    "df['Br/Rooms'] = df['total_bedrooms']/df['total_rooms']\n",
    "#df['Rooms/person'] = df['total_rooms']/df['population']\n",
    "df['Rooms/hhold'] = df['total_rooms']/df['households']\n",
    "#df['Br/person'] = df['total_bedrooms']/df['population']\n",
    "df['Br/hhold'] = df['total_bedrooms']/df['households']\n",
    "df['pop/hhold'] = df['population']/df['households']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_026osIQhCnG"
   },
   "source": [
    "### 2.3.2 Visualize all features again\n",
    "First, move target column to the end of the data frame to make correlation easier to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf1YAbx35noN"
   },
   "outputs": [],
   "source": [
    "# 1. Moves target column\n",
    "target = 'median_house_value'\n",
    "cols = list(df.columns.values)\n",
    "cols.pop(cols.index(target))\n",
    "cols.append(target) # Ensure target column is at the end\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsGvIehdhQbL"
   },
   "outputs": [],
   "source": [
    "# 2. Call our previously defined function in 2.2.1 and pass it our dataframe to plot histograms and scatterplots of all numerical values\n",
    "plot_numerics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dskgce2yhVm5"
   },
   "outputs": [],
   "source": [
    "# 3. Let's check our variables for correlation\n",
    "plt.figure(figsize=(15,12)) # if you need this correlation heatmap to be bigger\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr,  vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm', annot = True,\n",
    "            mask = np.triu(corr));  # masks upper part of triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCwoBAWqs73U"
   },
   "outputs": [],
   "source": [
    "# 4. Drop strongly correlated columns\n",
    "df.drop(columns = ['total_rooms','total_bedrooms','population','households'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgWUK_xwToak"
   },
   "source": [
    "## 2.4 Preprocess the data\n",
    "\n",
    "We are in the middle of proprocessing our data to ensure it is a suitable state for modelling.  \n",
    "Let's review the common preprocessing steps:\n",
    "\n",
    "* ***Dealing with missing values***  \n",
    "   where we identify which, if, any missing data we have and how to deal with them.  \n",
    "   \n",
    "   For example, we may replace missing values with the mean value for that feature, or by the average of the neighboring values:  \n",
    "   \n",
    "    * `pandas` has a number of options for filling in missing data that is worth exploring\n",
    "    * We can also use `k-nearest neighbor` to help us predict what the missing values should be, or `sklearn Imputer` function (amongst other ways)\n",
    "    \n",
    "    \n",
    "    \n",
    "* ***Dealing with categorical values***\n",
    "   by converting them into a numerical representation that can be modelled.  \n",
    "   \n",
    "    * There are a number of different ways to do this in `sklearn` and `pandas` such as `pandas.get_dummies`  \n",
    "\n",
    "* ***Removing outliers***\n",
    "   to avoid undue influence on the analysis.  Outliers in this data come from many sources:\n",
    "   * Miscoded data (home values over 500K, age over 50 years)\n",
    "   * True anomalies from how or what data was gathered (Los Angeles blocks with huge population, vacation areas with huge numbers of rooms per person)\n",
    "   * Rare circumstances (homes on islands)\n",
    "\n",
    "* ***Scaling the data***\n",
    "   to enable different features to be combined or compared, feature data may be transformed to be:\n",
    "   * all on the same scale (such as within two defined values)\n",
    "   * (approximately) normally distributed\n",
    "   * zero-mean\n",
    "   * etc.  \n",
    "   \n",
    " Not all ML models require this step, but most benefit from it.  Again, `sklearn` and `pandas` have in-built functions to help you do this.\n",
    "\n",
    " Finally, we split into Train and Test data, and hide the Test data from any model building efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOj3RL6YToal"
   },
   "source": [
    "### 2.4.1 Outlier Removal\n",
    "\n",
    "Houses over 50 years old or over $500K will not have those parameters accurately recorded, so we should not use them for training or testing.\n",
    "We can also make a function to identify outliers that are at least three standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYUVc2Fr06jj"
   },
   "outputs": [],
   "source": [
    "# 1. Let's keep only records with housees under 50 years of age and a value under $500K\n",
    "print(df.shape)\n",
    "df = df[(df.housing_median_age <= 50) & (df.median_house_value <=500000)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJmWevxDV0ag"
   },
   "outputs": [],
   "source": [
    "# 2. This function can be used on a list of numerical columns to return a list of index values of the outliers\n",
    "def get_outliers(data, columns, nsd = 3):\n",
    "    \"\"\" Takes a data frame and list of numeric columns names, return the list of indices for which\n",
    "    a value in any of the columns is more than nsd standard deviations from the column mean.\"\"\"\n",
    "    outlier_idxs = []\n",
    "    for col in set(data.columns).intersection(set(columns)):\n",
    "        elements = data[col]\n",
    "        # we get the mean value for each column\n",
    "        mean = elements.mean()\n",
    "        # and the standard deviation of the column\n",
    "        sd = elements.std()\n",
    "        # we then get the index values of all values higher or lower than the mean +/- nsd standard deviations\n",
    "        outliers_mask = data[(data[col] > mean + nsd*sd) | (data[col]  < mean  - nsd*sd)].index\n",
    "        # and add those index values to our list\n",
    "        outlier_idxs  += [x for x in outliers_mask]\n",
    "    return list(set(outlier_idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqpfMLh-x1Ny"
   },
   "source": [
    "Define the columns where we have identified there could be outliers, in the order you'd like them removed.\n",
    "**? Any columns you would add?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "498Sxk1FToal"
   },
   "outputs": [],
   "source": [
    "# 3. Defne column list\n",
    "numeric_columns = ['median_income','median_house_value', 'Br/Rooms', 'Br/person', 'Br/hhold', 'Rooms/person', 'Rooms/hhold', 'pop/hhold']\n",
    "\n",
    "# 4. Let's check how many observations we have:\n",
    "print (f\"Number of Observations and Features before Cleaning: {df.shape}\")\n",
    "\n",
    "# 5. We call the function we just created on the housing dataset ...\n",
    "outlier_list = get_outliers(df, numeric_columns)\n",
    "\n",
    "# 6. ... and drop those records from both our feature and response data\n",
    "df.drop(outlier_list, axis = 0, inplace=True)\n",
    "\n",
    "# 7. We can check that this code has worked by looking at the shape of our data\n",
    "print (f\"Number of Observations and Features after Cleaning: {df.shape}\")\n",
    "\n",
    "# WARNING - Be careful: if you repeatedly re-run this cell, you are removing more and more records!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJOxaideToal"
   },
   "source": [
    "### 2.4.2 Re-Scale the Data\n",
    "\n",
    "We can create a function to rescale our data so that the mean is zero and standard deviation is unity (1) on all features.  \n",
    "\n",
    "Lets look at the data before and after re-scaling.  \n",
    "\n",
    "**!** A good exercise would be to research what StandardScaler does - it is from the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyByf0YeV0am"
   },
   "outputs": [],
   "source": [
    "# 1. Check Variable distributions\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgS-jd4IFrPJ"
   },
   "outputs": [],
   "source": [
    "# 2. This function loops through columns in a data set and applies the scaler to each that we passed to the function\n",
    "def scale_numeric(data, numeric_columns, scaler):\n",
    "    for col in numeric_columns:\n",
    "        data.loc[:,col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAmdRz8yV0a0"
   },
   "outputs": [],
   "source": [
    "# 3. We can now define the scaler we want to use and apply it to our dataset\n",
    "scaler = StandardScaler()\n",
    "numeric_columns = [col for col in df.columns if df[col].dtype == 'float64']\n",
    "df = scale_numeric(df, numeric_columns, scaler)\n",
    "\n",
    "# 4. Let's check out varible's distribution after the rescaling\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8MgcED4WBG4"
   },
   "source": [
    "### 2.4.3 Split the Data into X and y, and Train and Test sets\n",
    "* In order to train our model and see how well it performs, we need to split our data into training and testing sets.  \n",
    "\n",
    "* We can then train our model on the training set, and test how well it has generalised to the data on the test set.  \n",
    "\n",
    "* There are a number of options for how we can split the data, and for what proportion of our original data we set aside for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bU5Acjl-Toao"
   },
   "outputs": [],
   "source": [
    "# 1. Split out Xs and Ys\n",
    "housing_y = df['median_house_value'].values\n",
    "housing_X = df.drop(columns=['median_house_value'])\n",
    "\n",
    "# 2. A common way for splitting our dataset is using train_test_split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(housing_X, housing_y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# 3. Get shape of test and training sets\n",
    "print('Training Set:')\n",
    "print('Number of records: ', X_train.shape[0])\n",
    "print('Number of features: ', X_train.shape[1])\n",
    "print('\\n')\n",
    "print('Test Set:')\n",
    "print('Number of records: ', X_test.shape[0])\n",
    "print('Number of features: ', X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY9VqsGYToao"
   },
   "source": [
    "# 3. Regression Analysis for Prediction\n",
    "Derived from a notebook accompanying \"Classification and Regression in a Weekend\" by Ajit Jaokar & Dan Howarth (With contributions from Ayse Mutlu), published by Data Science Central.\n",
    "Their original notebook is here: https://colab.research.google.com/drive/14m95e5A3AtzM_3e7IZLs2dd0M4Gr1y1W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ukAmAfNUB_q"
   },
   "source": [
    "##  3.1 Linear regression\n",
    "\n",
    "Linear regression is simple model compared to more complicated regression options, so provides a good baseline.\n",
    "\n",
    "**Regression Mechanics:**  \n",
    "\n",
    "y = ax + b\n",
    "  - y = target (response)  \n",
    "  - x = single feature  \n",
    "  - a, b = parameters of model  \n",
    "  \n",
    "How do we choose a and b?\n",
    "  - Define an error function for any given line\n",
    "  - Choose the line that minimizes the error function\n",
    "  \n",
    "**The Loss Function**\n",
    "\n",
    "Ordinary least squares (OLS): Minimize sum of squares of\n",
    "residuals\n",
    "\n",
    "![OLS Loss Function Residuals](https://www.mapxp.app/BUSI488/OLS-residual.jpg \"OLS Loss Function\")\n",
    "\n",
    "\n",
    "### 3.1.1 Let's run an OLS on our housing Housing Data!\n",
    "We start with a single feature, median_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAWKQdMoEy7K"
   },
   "outputs": [],
   "source": [
    "# 1. We pull out only one varibale, median_income, from our X_train and X_test sets (which include all variables)\n",
    "X_income_train = X_train['median_income'].values\n",
    "X_income_test = X_test['median_income'].values\n",
    "\n",
    "# 2. Instantiate model: In scikit-learn you create a model separate from your data, which allows you to swap in other models (or data) with minimal code changes\n",
    "lm = LinearRegression()\n",
    "\n",
    "# 3. Fit model: we train the model on our training data. Fitting requires both X and y variables for training.\n",
    "# Because we have only one colum, we reshape it into the format that the linear regressor expects\n",
    "lm.fit(X_income_train.reshape(-1,1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTWIynVxToaq"
   },
   "source": [
    "### 3.1.2 Prediction and Performance\n",
    "\n",
    "Now that we have fit our model, we can use it to predict our response variable in the test set...  \n",
    "... and check how well our model performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpGw2ydMHnW1"
   },
   "outputs": [],
   "source": [
    "# 1. Predict: We  generate a set of predictions on our unseen features X_test\n",
    "y_pred = lm.predict(X_income_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "294YmsPYV0be"
   },
   "source": [
    "#### Choose an evaluation metric\n",
    "* We then need to compare these predictions with the actual result and measure them in some way.  \n",
    "\n",
    "\n",
    "* This is where the selection of evaluation metric is important. For regression, we measure the distance between the predicted and actual answers in some way. The shorter the distance, the more correct the model is.   \n",
    "\n",
    "\n",
    "* We cover three common metrics below:  \n",
    "  \n",
    "  * `Mean Absolute Error`: which provides a mean score for all the predicted versus actual values as an absolute value   \n",
    "  \n",
    "  * `Means Squared Error`: which provides a mean score for all the predicted versus actual values as a square of the absolute value  \n",
    "  \n",
    "  * `R2`: which we recommend you research as an exercise to grow your knowledge. WIkipedia and `sklearn` document are a great place to start!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyhAGab1V0bg"
   },
   "outputs": [],
   "source": [
    "# 2. Import a library that we will need for our evaluation metrics\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 3. Define a function that evaluates our predictions (y_pred) relative to the ground truth (i.e., y_test)\n",
    "def evaluate(y_test, y_pred, viz=True):\n",
    "    # this block of code returns all the metrics we are interested in\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    msa = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Mean squared error: \", mse)\n",
    "    print(\"Mean absolute error: \", msa)\n",
    "    print(\"R^2 : \", r2)\n",
    "    if viz==True:\n",
    "      # this creates a chart plotting predicted and actual with distribution of residual\n",
    "      f, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "      axes[0].scatter(y_test, y_pred)\n",
    "      axes[0].set(xlabel=\"median house values: $y_i$\", ylabel=\"Predicted values: $\\hat{y}_i$\")\n",
    "      #plt.title(\"Prices vs Predicted values: $y_i$ vs $\\hat{y}_i$\")\n",
    "      axes[1].scatter(x=y_pred, y=y_pred-y_test, )\n",
    "      axes[1].set(xlabel=\"residual $y_i-\\hat{y}_i$\", ylabel=\"Count\", label='resid')\n",
    "\n",
    " # 4. Pass the ground truth (y_test) and our predictions (y_pred) to our evaluation function\n",
    "evaluate(y_test, y_pred, viz=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOhat9VmH_n8"
   },
   "source": [
    "## 3.1.3 Multilinear regression\n",
    "OLS can use more input variables to get a better prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jWVAOdXToap"
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# 2. Fit model to full set of variables\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# 3. let's save the coefficients and plot them large to small\n",
    "linear1_coeff = lm.coef_.copy()\n",
    "coeff_perm = np.flipud(np.argsort(linear1_coeff))\n",
    "\n",
    "# 4. Plot coefficients\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(linear1_coeff[coeff_perm])\n",
    "plt.ylabel('Coefficients')\n",
    "plt.xticks(range(housing_X.columns.size), housing_X.columns.values[coeff_perm], rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmCDgyAyToaq"
   },
   "outputs": [],
   "source": [
    "# 5. Use our trained model to predict the median house values in the test set\n",
    "y_pred = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4-SIrYBGYNW"
   },
   "outputs": [],
   "source": [
    "# 6. Evaluate model performance\n",
    "evaluate(y_test, y_pred, viz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9QbxqNK7s57"
   },
   "outputs": [],
   "source": [
    "#### BONUS ####\n",
    "# We can explore how metrics are derivied in a little more detail by looking at MAE\n",
    "# here we will implement MAE using numpy, building it up step by step\n",
    "\n",
    "# 7a. With MAE, we get the absolute values of the error - as you can see this is of the difference between the actual and predicted values\n",
    "np.abs(y_test - y_pred)\n",
    "\n",
    "# 7b. We sum them up\n",
    "np.sum(np.abs(y_test - y_pred))\n",
    "\n",
    "# 7c. then divide by the total number of predictions/actual values\n",
    "# as you will see, we get to the same score implemented above\n",
    "print (np.sum(np.abs(y_test - y_pred))/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8SUuvB60oES"
   },
   "source": [
    "#### 3.1.3.1 YellowBrick\n",
    "- YellowBrock is a visualization tool for sklearn models, built on top of matplotlib/seaborn.  \n",
    "- It enables you to easily and quickly explore the performance of different models\n",
    "- https://www.scikit-yb.org/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ro1usGFa88yy"
   },
   "outputs": [],
   "source": [
    "# 0. Let's install yellowbrick\n",
    "# !pip install yellowbrick\n",
    "\n",
    "# 1. Import some libraries\n",
    "from yellowbrick.regressor import residuals_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgVp1YXwWE3C"
   },
   "outputs": [],
   "source": [
    "# 2. Use YellowBrick to plot residuals for our original LM model\n",
    "plt.figure(figsize=(12,6))\n",
    "# 2b. Fit and plot with yellowbrick\n",
    "viz = residuals_plot(lm, X_train, y_train, X_test, y_test,  hist=False, qqplot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDSME90PUVId"
   },
   "source": [
    "### 3.1.4 Refine the model\n",
    "* This step allows us to add or modify features of the datatset. We might do this if, for example, some combination of features better represents the problems space and so is an indicator of the target variable.\n",
    "* Here, we create one additional feature as an example, but you should reflect on our EDA earlier and see whether there are other features that can be added to our dataset.\n",
    "* The feature we add is **the log of an existing feature,** so we fit a non-linear model.\n",
    "* There are other transformations or other algorithms that can fit other non-linear models into this linear framework.  Think about your data to decide what model would work well.  \n",
    "   \n",
    "![Types of regression](https://storage.ning.com/topology/rest/1.0/file/get/952937562?profile=original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIh18ftH-Wge"
   },
   "outputs": [],
   "source": [
    "# 0. Go back to dataset before we did out previous preprocessing\n",
    "log_df = df_preFE.copy() # Restore all fields\n",
    "\n",
    "# 0a. Missing data: replace nans in total_bedrooms by imputing them to be 20% of all rooms\n",
    "log_df.loc[log_df['total_bedrooms'].isna(), 'total_bedrooms'] = 0.20*log_df['total_rooms']\n",
    "\n",
    "# 0b. Remove older houses and those bucketed to $500K\n",
    "log_df = log_df[(log_df.housing_median_age <= 50) & (log_df.median_house_value <=500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DBz4Enp_-4Z"
   },
   "outputs": [],
   "source": [
    "# 1. Engineer some additional features.\n",
    "# (Comment out or drop features you don't want to add)\n",
    "\n",
    "log_df['Br/Rooms'] = log_df['total_bedrooms']/log_df['total_rooms']\n",
    "#log_df['Rooms/person'] = log_df['total_rooms']/log_df['population']\n",
    "log_df['Rooms/hhold'] = log_df['total_rooms']/log_df['households']\n",
    "#log_df['Br/person'] = log_df['total_bedrooms']/log_df['population']\n",
    "log_df['Br/hhold'] = log_df['total_bedrooms']/log_df['households']\n",
    "log_df['pop/hhold'] = log_df['population']/log_df['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWlk1161-77e"
   },
   "outputs": [],
   "source": [
    "#### Someone on Kaggle suggested using log to transform skew distributions: https://www.kaggle.com/ilialar/california-housing-analysis-and-preciction\n",
    "\n",
    "# 2. Define skewed features\n",
    "skewed_features=['households','median_income','population', 'total_bedrooms', 'total_rooms']\n",
    "\n",
    "# 3. Keep the names of the log features we will create\n",
    "log_numerical_features=[]\n",
    "\n",
    "# 4. Create log features\n",
    "for f in skewed_features:\n",
    "    log_df[f + '_log']=np.log1p(log_df[f])\n",
    "    log_numerical_features.append(f + '_log')\n",
    "\n",
    "# 5. Drop skewed features\n",
    "log_df.drop(columns=skewed_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRCqQ6rXHJq-"
   },
   "outputs": [],
   "source": [
    "# 6. Remove outliers with previously defined function\n",
    "numeric_columns = [col for col in log_df.columns if log_df[col].dtype == 'float64']\n",
    "outlier_list = get_outliers(log_df, numeric_columns)\n",
    "\n",
    "# 7. and drop those records from both our feature and response data\n",
    "log_df.drop(outlier_list, axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NK3TbCT9F07t"
   },
   "outputs": [],
   "source": [
    "# 7. Let's scale all numeric columns\n",
    "scaler = StandardScaler()\n",
    "numeric_columns = [col for col in log_df.columns if log_df[col].dtype == 'float64']\n",
    "log_df = scale_numeric(log_df, numeric_columns, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOEX_REcFZ1E"
   },
   "outputs": [],
   "source": [
    "# 8. Split out dependent variable (y) and independent variables (X)\n",
    "log_y = log_df['median_house_value'].values\n",
    "log_X = log_df.drop(columns=['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJeXAlUE-S8Z"
   },
   "outputs": [],
   "source": [
    "# 9. Train-Test-Split our Sample\n",
    "logX_train, logX_test, logy_train, logy_test = model_selection.train_test_split(log_X, log_y, test_size = 0.2, random_state=0)\n",
    "\n",
    "# 10. Instantiate Model\n",
    "loglm = LinearRegression()\n",
    "\n",
    "# 11. Use YellowBrick to fit model and output Performance\n",
    "plt.figure(figsize=(12,6))\n",
    "viz = residuals_plot(loglm, logX_train, logy_train, logX_test, logy_test,  hist=False, qqplot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEASBlx2IG8Q"
   },
   "outputs": [],
   "source": [
    "# 12. let's save the coefficients and plot them large to small\n",
    "linear2_coeff = loglm.coef_.copy()\n",
    "coeff_perm = np.flipud(np.argsort(linear2_coeff))\n",
    "\n",
    "# 13. Plot coefficients\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(linear2_coeff[coeff_perm])\n",
    "plt.ylabel('Coefficients')\n",
    "plt.xticks(range(log_X.columns.size), log_X.columns.values[coeff_perm], rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1zED3ePToat"
   },
   "source": [
    "# 4. Regularization\n",
    "\n",
    "**Recall:** Linear regression minimizes a loss function\n",
    "- It chooses a coefficient for each feature variable\n",
    "- Large coefficients can lead to overfitting\n",
    "- Penalizing large coefficients is called Regularization\n",
    "\n",
    "\n",
    "In statistics, there are two critical characteristics of estimators to be considered:\n",
    "- ***Bias*** is the difference between the true population parameter and the expected estimator\n",
    "  - It measures the accuracy of the estimates\n",
    "- ***Variance*** measures the spread, or uncertainty, in these estimates\n",
    "\n",
    "**Remember:**\n",
    "- Bias and the variance are desired to be low, as large values result in poor predictions from the model.\n",
    "- The OLS estimator has the desired property of being unbiased.\n",
    "- However, it can have a huge variance.\n",
    "\n",
    "![from KDnuggets](https://www.mapxp.app/BUSI488/bias-variance-tradeoff.jpg \"Bias-Variance Trade-Off\")\n",
    "\n",
    "\n",
    "**Solution**: Reduce variance at the cost of introducing some bias.\n",
    "- This approach is called **regularization** and is almost always beneficial for the predictive performance of the model.\n",
    "\n",
    "**Principle:**\n",
    "- Regularization adds a penalty when coefficients get too large in an attempt to reduce the impact of outliers and make models that better predict new values for samples that have not been seen before.\n",
    "- Important techniques are ridge, lasso, and their combination as elastic net.\n",
    "- Each has a hyperparameter ($\\alpha$) for the size of the penalty\n",
    "- We will tune these hyperparameters later in this course.\n",
    "\n",
    "**Rather than go into further details on these techniques, we will apply them to improve model performance!**\n",
    "\n",
    "If you would like further details you might want to look at DataCamp's excellent tutorial:\n",
    "[Regularization tutorial](https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJeHDXv6Toat"
   },
   "source": [
    "## 4.1 Ridge Regression\n",
    "\n",
    "**Basic Idea:**\n",
    "\n",
    "- Some predictors may not be as important for out model and we could reduce model complexity by removing them all together.   \n",
    "\n",
    "\n",
    "- Removing predictors from the model can be seen as settings their coefficients to zero.   \n",
    "\n",
    "\n",
    "- Instead of forcing them to be exactly zero, let's penalize them if they are too far from zero, thus enforcing them to be small in a continuous way.   \n",
    "\n",
    "\n",
    "- This way, we decrease model complexity while keeping all variables in the model.  \n",
    "\n",
    "\n",
    "\n",
    "**Mechanics:**\n",
    "\n",
    "- Loss function = OLS loss function $+  \\alpha \\times \\sum_{i=1}^n a_i^2$\n",
    "\n",
    "\n",
    "- Alpha ($\\alpha$ ): Parameter we need to choose  \n",
    "\n",
    "\n",
    "- Picking $\\alpha$ here is similar to picking k in k-NN  \n",
    "\n",
    "\n",
    "- Hyperparameter tuning next section of this notebook  \n",
    "\n",
    "\n",
    "- $\\alpha$ controls model complexity  \n",
    "\n",
    "\n",
    "- $\\alpha$ = 0: We get back OLS (Can lead to overfitting)  \n",
    "\n",
    "\n",
    "- Very high $\\alpha$: Can lead to underfitting\n",
    "\n",
    "- Assumes the features are standardized and response is centered\n",
    "\n",
    "**Let's run a ridge regression on our housing Housing Data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdlz8UscToat"
   },
   "outputs": [],
   "source": [
    "# 1. Import Model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 2. Instantiate Model\n",
    "ridge = Ridge(alpha=3)\n",
    "\n",
    "# 2. Fit Model\n",
    "ridge.fit(logX_train, logy_train)\n",
    "\n",
    "# 3. Predict Text\n",
    "ridge_pred = ridge.predict(logX_test)\n",
    "\n",
    "# 4. Evaluate Model Performance\n",
    "ridge.score(logX_test, logy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg2_2NaOToau"
   },
   "source": [
    "### 4.1.2 The Effect of Changing Alpha in a Ridge Regression\n",
    "\n",
    "So how will changes of Alpha impact the effect sizes (of the features) as expresseed by their coefficients?\n",
    "\n",
    "**Let's try various values for alpha and see!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsDGtvEaToau"
   },
   "outputs": [],
   "source": [
    "# 1. Define Base model\n",
    "lm = Ridge(alpha=0)\n",
    "lm.fit(logX_train, logy_train)\n",
    "\n",
    "# 2. Create dataframe for our test\n",
    "ridge_df = pd.DataFrame({'variable': log_X.columns, 'estimate': lm.coef_})\n",
    "ridge_train_pred = []\n",
    "ridge_test_pred = []\n",
    "\n",
    "# 3. Set range of alphas to test\n",
    "alphas = np.arange(0, 200, 5)\n",
    "\n",
    "# 4. Create a loop that runs ridge regression with every value of alpha we specified above\n",
    "for alpha in alphas:\n",
    "    lm = Ridge(alpha=alpha)\n",
    "    lm.fit(logX_train, logy_train)\n",
    "    var_name = 'estimate' + str(alpha)\n",
    "    ridge_df[var_name] = lm.coef_\n",
    "    # prediction\n",
    "    ridge_train_pred.append(lm.predict(logX_train))\n",
    "    ridge_test_pred.append(lm.predict(logX_test))\n",
    "\n",
    "# 5. Consolidate results to prepare to plot\n",
    "ridge_df = ridge_df.set_index('variable').T.rename_axis('estimate').rename_axis(None, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urSouYIDz6Kx"
   },
   "outputs": [],
   "source": [
    "# 6. Plot coefficients for all alphas\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ridge_df.plot(ax=ax)\n",
    "ax.axhline(y=0, color='black', linestyle='--')\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_ylabel(\"Beta Estimate\")\n",
    "ax.set_title(\"Ridge Regression Trace\", fontsize=16)\n",
    "ax.legend(labels=ridge_df.columns)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1HVv0jeUv8N"
   },
   "source": [
    "## 4.2 LASSO Regression\n",
    "\n",
    "- Can be used to select important features of a dataset\n",
    "- Shrinks the coefficients of less important features to exactly 0\n",
    "\n",
    "Lasso adds a penalty $\\alpha$ times the sum of the absolute value of the $a$ parameters.  \n",
    "\n",
    "Doing so allows many **parameters that are not needed** in the fitting to be driven to zero.  \n",
    "\n",
    "Thus, we find out which parameters are necessary for the model.  \n",
    "\n",
    "\n",
    "**Mechanics:**\n",
    "\n",
    "- Loss function = OLS loss function $+  \\alpha \\times \\sum_{i=1}^n \\left\\lvert a_i \\right\\rvert$\n",
    "\n",
    "\n",
    "- Alpha ($\\alpha$ ): Parameter we need to choose  \n",
    "\n",
    "\n",
    "- Picking $\\alpha$ here is similar to picking k in k-NN  \n",
    "\n",
    "\n",
    "- Hyperparameter tuning next section of this notebook  \n",
    "\n",
    "\n",
    "- $\\alpha$ controls model complexity  \n",
    "\n",
    "\n",
    "**Let's run it on our housing Housing Dataset!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V94brdUMToa1"
   },
   "outputs": [],
   "source": [
    "# 1. Import Model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 2. Create placeholder for fittet coefficients\n",
    "lasso_list = {}\n",
    "\n",
    "# 3. Loop through values of alpha\n",
    "for a in [0.01, 0.025, 0.05, 0.075, 0.1, 0.5, 1]:\n",
    "  # instatiate\n",
    "  lm = Lasso(alpha = a)\n",
    "  # fit\n",
    "  lm.fit(logX_train, logy_train)\n",
    "  # predict\n",
    "  y_pred = lm.predict(logX_test)\n",
    "  print(f'alpha = {a}:')\n",
    "  # evaluate\n",
    "  print(lm.score(logX_test, logy_test))\n",
    "  #evaluate(logy_test, y_pred)\n",
    "  # Store coefficients\n",
    "  lasso_list[a] = lm.coef_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiNz3fnsToa2"
   },
   "outputs": [],
   "source": [
    "# 4 Plot effect on coefficient for different alphas in Lasso Regression\n",
    "plt.figure(figsize=(12,6))\n",
    "#plt.plot(linear1_coeff[coeff_perm])\n",
    "#plt.plot(linear2_coeff[coeff_perm])\n",
    "for item in lasso_list.values():\n",
    "  plt.plot(item[coeff_perm])\n",
    "plt.legend(['linear', *lasso_list.keys()])\n",
    "plt.ylabel('Coefficients')\n",
    "plt.xticks(range(log_X.columns.size), log_X.columns.values[coeff_perm], rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmAmoq5StMkl"
   },
   "source": [
    "# 5. Different Model: K-Nearest Neighborhood (KNN) Algorithm\n",
    "\n",
    "> *Show me who your friends are, and I’ll tell you who you are*\n",
    "\n",
    "The concept of K-NN can hardly be described more simply. This is an old saying, which can be found in many languages and many cultures.\n",
    "\n",
    "\n",
    "**Basic idea:** Predict the value of a data point by  \n",
    "- Looking at the values of the ‘k’ closest data points (local values)\n",
    "- Interpolate these local values to make a prediction  \n",
    "\n",
    "**Underlying Principle**:\n",
    "- Find a predefined number (k) training samples closest in distance to a new sample that has to be classified\n",
    "- The value of the new sample will be defined from these neighbors\n",
    "- KNN has a fixed user defined constant for the number of neighbors which have to be determined\n",
    "\n",
    "\n",
    "![A Comparative Study of Linear and KNN Regression](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Fqej14RxTPp9zzQ5gKm_g.png \"KNN Intuition\")\n",
    "*Image source: https://pub.towardsai.net/a-comparative-study-of-linear-and-knn-regression-a31955e6263d*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG3dJIm1vXKh"
   },
   "outputs": [],
   "source": [
    "# 1a. Import model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 1b. Train-Test-Split our Sample\n",
    "logX_train, logX_test, logy_train, logy_test = model_selection.train_test_split(log_X, log_y, test_size = 0.2, random_state=42)\n",
    "\n",
    "# 1c. Instantiate model\n",
    "kn = KNeighborsRegressor(n_neighbors=7)\n",
    "\n",
    "# 1d. Use YelloBrick to fit model, evaluate and visualize\n",
    "plt.figure(figsize=(12,6))\n",
    "viz = residuals_plot(kn, logX_train, logy_train, logX_test, logy_test,  hist=False, qqplot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja-2Y2i4MciR"
   },
   "source": [
    "### 3.1.6 Location, Location, Location!\n",
    "\n",
    "![Location!](https://www.socaladvocates.com/wp-content/uploads/2016/03/VenueforBankruptcy-300-300.jpg.webp)\n",
    "\n",
    "Can location suffice for great predictions?\n",
    "\n",
    "- Linear Regression\n",
    "\n",
    "  vs.\n",
    "- K-NN Regression\n",
    "\n",
    "> ***Which one do you think will work better?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaOPVfD9tLwi"
   },
   "outputs": [],
   "source": [
    "# 1. Location only: Linear Regression\n",
    "\n",
    "# 1a. Train-Test-Split our Sample and define which variables to use\n",
    "logX_train, logX_test, logy_train, logy_test = model_selection.train_test_split(log_X, log_y, test_size = 0.2, random_state=21)\n",
    "latlon = ['latitude','longitude']\n",
    "\n",
    "# 1b. Instantiate model\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# 1c. Use YelloBrick to fit model, evaluate and visualize (we only actually use latlon = ['latitude','longitude'] as the variables)\n",
    "plt.figure(figsize=(12,6))\n",
    "viz = residuals_plot(linreg, logX_train[latlon], logy_train, logX_test[latlon], logy_test,  hist=False, qqplot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtiZK8KlOiNY"
   },
   "outputs": [],
   "source": [
    "# 2. Location only: K-NN\n",
    "\n",
    "# 2a. Train-Test-Split our Sample and define which variables to use\n",
    "logX_train, logX_test, logy_train, logy_test = model_selection.train_test_split(log_X, log_y, test_size = 0.2, random_state=21)\n",
    "latlon = ['latitude','longitude']\n",
    "\n",
    "# 2b. Instantiate model\n",
    "kn = KNeighborsRegressor(n_neighbors=7)\n",
    "\n",
    "# 2c. Use YelloBrick to fit model, evaluate and visualize (we only actually use latlon = ['latitude','longitude'] as the variables)\n",
    "plt.figure(figsize=(12,6))\n",
    "viz = residuals_plot(kn, logX_train[latlon], logy_train, logX_test[latlon], logy_test,  hist=False, qqplot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recap: Matrices and Solving systems of equations"
   ],
   "metadata": {
    "id": "ZCI0vdEE2Fne"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdnyEwzBStFT"
   },
   "source": [
    "\n",
    "## 2.2 A Matrix is: A System of Equations\n",
    "\n",
    "<img src=\"https://i.imgur.com/K6ySni0.png\" alt=\"prices for apples, bananas, and carrots at 3 different stores\" height = 120 align=\"right\">\n",
    "\n",
    "Matrices provide a compact and convenient way to represent systems of linear equations. Consider the problem of buying apples, bananas, and carrots at different stores with varying prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFcqWe2_StFT"
   },
   "source": [
    "Let 'a' be the quantity of apples, 'b' the quantity of bananas, and 'c' the quantity of carrots. The prices at three stores (Food Lion, Harris Teeter, and a local grocery) can be represented in a matrix. The total cost at each store can be expressed as a linear equation.\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "* Food Lion cost = 1.29a + 0.84b + 0.42c\n",
    "* Harris Teeter cost = 1.24a + 0.86b + 0.41c\n",
    "* Grocery cost = 1.09a + 0.98b + 0.45c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP5_aaz6StFT"
   },
   "source": [
    "\n",
    "#### Matrix Representation\n",
    "\n",
    "We can represent this system of equations in matrix form as: _Prices @ Quantity = Total Cost_.  \n",
    "\n",
    "$$\\left[\\matrix{1.29&0.84& 0.42\\\\\n",
    "1.24& 0.86& 0.41\\\\\n",
    "1.09& 0.98& 0.45}\\right]\\cdot \\left[\\matrix{a\\\\b\\\\c}\\right] = \\textit{cost}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBObvCdMStFT"
   },
   "source": [
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "<img src=\"https://i.imgur.com/t2mHJtN.png\" alt=\"Matrix multiplication: For A*B the number of columns of A must equal the number of rows of B.\" align=\"right\" height=120>\n",
    "\n",
    "Matrix multiplication is a fundamental operation. If $A$ is a $j \\times k$ matrix and $B$ is a $k \\times m$ matrix, then their product $C = A @ B$ is a $j \\times m$ matrix. The inner dimensions ($k$) must agree for multiplication to be defined.\n",
    "\n",
    "**Formula** for all $r\\in[0,j)$ and $c\\in[0,m)$, calculate $C(r, c) = \\sum_{0\\le  \\ell\\lt k} A(r, \\ell) \\cdot B(\\ell, c)$.\n",
    "\n",
    "**Notes:**\n",
    "- Numpy uses the @ sign for matrix multiplication; mathematics will use $\\cdot$ or *.  \n",
    "- $A@ B$ is not the same as $B@ A$, and may not even be defined.  Matrix multiplication is not commutative.\n",
    "- **Why define matrix multiplication this way?**  Laziness.  \n",
    " We can write our grocery cost calculations as $A\\cdot B = C$, where $A$ is the matrix of prices, $B$ is our vector quantities, and $C$ is the cost. If we know $A$ and $B$, we get costs at all three stores.   \n",
    "- The quantity should be a column vector. In python, if you use a 1-d vector for the quantity with @, it will figure it out, and give a 1-d vector in return. Experiment below with different quantities of apples, bananas, and carrots.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "prices = np.array([[1.29, 0.84, 0.42],\n",
    "                   [1.24, 0.86, 0.41],\n",
    "                   [1.09, 0.98, 0.45]])"
   ],
   "metadata": {
    "id": "pWbP5myZDeSJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(prices @ np.array([[1],[2],[3]]))\n",
    "print(prices @ np.array([1,2,3]))"
   ],
   "metadata": {
    "id": "mfiLMjTzDqUn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIKFETkUStFU"
   },
   "source": [
    "\n",
    "## 2.2.1 Solving a System of Equations\n",
    "\n",
    "A system of linear equations can be written in the form $A\\cdot x = b$, where:\n",
    "\n",
    "* A is the matrix of coefficients.\n",
    "* x is the column vector of unknowns (variables).\n",
    "* b is the column vector of constants.\n",
    "\n",
    "If A is a square matrix (same number of rows and columns) and has full rank, there is generally a unique solution for x. In Python, we can solve for x using `np.linalg.solve(A, b)`.\n",
    "\n",
    "#### How to solve $Ax=b$?\n",
    "If $A$ is a single number, $A\\ne0$ we can divide both sides by $A$, AKA multiply both sides by the reciprocal $A^{-1}Ax = A^{-1}b$, so $x=A^{-1}b$.  \n",
    "Exactly this works for any *non-singular* matrix $A$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Continuing the grocery example, suppose we want to find the quantities of apples, bananas, and carrots that would cost $\\$10$ at Food Lion and Harris Teeter, and $\\$ 11$ at the local grocery.\n",
    "\n",
    "Experiment with other price totals. (For some totals you may find yourself having to sell produce to the stores.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMk73HyzStFU"
   },
   "outputs": [],
   "source": [
    "totals = np.array([10, 10, 11])\n",
    "\n",
    "quantities = np.linalg.solve(prices, totals)\n",
    "print(\"Quantities of apples, bananas, and carrots:\\n\", quantities)\n",
    "print(\"\\nCheck: prices @ quantities = \\n\", prices @ quantities) # @ is matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_PB056qStFU"
   },
   "source": [
    "\n",
    "### Static Equilibrium (Balanced Forces)\n",
    "\n",
    "\n",
    "<img src=\"https://imgur.com/rcKTVXE.png\" alt=\"a triangle shaped bridge ABC with a load at B\" align=\"right\" height = 220>\n",
    "\n",
    "Here's a more realistic scenario where we want to find quantities that achieve a desired total.  Consider a triangular bridge ABC:\n",
    "\n",
    "* A is fixed.\n",
    "* 10N load is applied at B.\n",
    "* C rolls.\n",
    "\n",
    "We want to find the forces $f_1$, $f_2$, and $f_3$ on the bars (positive for expansion, negative for contraction). The horizontal and vertical forces must balance at each vertex to prevent movement or collapse. Let's find three equations for these three variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFwAR3mSStFV"
   },
   "source": [
    "\n",
    "#### Static Equilibrium Equations\n",
    "<img src=\"https://imgur.com/tWcvMuq.png\" alt=\"forces on a triangle shaped bridge\" align=\"right\" height = 210>\n",
    "\n",
    "* $A$ is fixed (no equations needed).\n",
    "* $B$ may move:\n",
    "    - $B_h$ (horizontal): $f_1\\cos(\\theta) - f_2\\cos(\\gamma) = 0$\n",
    "    - $B_v$ (vertical): $f_1\\sin(\\theta) + f_2\\sin(\\gamma) - \\textit{Load} = 0$\n",
    "* $C$ may move horizontally:\n",
    "    * $C_h$: $f_3 + f_2\\cos(\\gamma) = 0$\n",
    "\n",
    "These equations can be written in matrix form as M @ f = b, where we know $M$ and $b$, once we choose the angles for the bridge.  I'll start with an equilateral triangle, but experiment by setting $\\theta=45$ or even $\\theta=30$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCY_00mPStFV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def bridge_forces(thetaDeg=60, gammaDeg=60, Load = 10):\n",
    "  theta = math.radians(thetaDeg)  # angles to radians\n",
    "  gamma = math.radians(gammaDeg)\n",
    "  M = np.array([\n",
    "      [np.cos(theta), -np.cos(gamma), 0],\n",
    "      [np.sin(theta), np.sin(gamma), 0],\n",
    "      [0, np.cos(gamma), 1]\n",
    "  ])\n",
    "\n",
    "  b = np.array([0, Load, 0])\n",
    "\n",
    "  return np.linalg.solve(M, b) # forces\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Forces:\\n\", bridge_forces())\n",
    "# f_1 & f_2 push up to balance Load, and thus push apart, countered by contraction of f_3.\n"
   ],
   "metadata": {
    "id": "WCbSzddlPv1a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Exercise:\n",
    "\n",
    "plot the values returned from bridge_forces(th) for th in [5 to 110 by 5s].  Use a lineplot with legend f1, f2, f3, putting th on the x axis. Explain what is happening in the plot."
   ],
   "metadata": {
    "id": "YO6aPRx2QjY3"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Cz17mBnGQz7i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkLlMgjjStFV"
   },
   "source": [
    "\n",
    "## 2.2.2 Non-square Systems of Equations\n",
    "\n",
    "* Rectangular matrices represent non-square systems.\n",
    "* Underdetermined systems have fewer equations than variables.\n",
    "* Overdetermined systems have more equations than variables.\n",
    "* `np.linalg.solve` solves Ax = b \"in the least-squares sense\" for overdetermined systems, finding the x that minimizes the squared residual: $||Ax - b||^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJHC9lu8StFV"
   },
   "source": [
    "\n",
    "### Example: Fitting a Line\n",
    "<img src=\"https://imgur.com/9Ab0CPr.png\" alt=\"observed values from a riction experiment\" >\n",
    "\n",
    "**Data:** Friction in response to an applied force.\n",
    "\n",
    "\n",
    "We want to fit a line ($y = mx + c$) to this data and evaluate the error.\n",
    "\n",
    "<img src=\"https://imgur.com/5BUVNUR.png\" alt=\"matrix form of equaations for friction\" align=\"right\" height=180>\n",
    "\n",
    "That is, we want:  \n",
    "$\\hphantom{0}5.84 \\approx m\\cdot 1+c$  \n",
    "$\\hphantom{0}8.87 \\approx m\\cdot 2+c$  \n",
    "$11.76 \\approx m\\cdot 3+c$  \n",
    "&emsp;&emsp;&emsp;&emsp;$\\vdots$  \n",
    "$29.18 \\approx m\\cdot 10+c$\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.column_stack([(1,2,3,4,5,6,7,8,9,10), np.ones(10)])\n",
    "b = np.array([5.84, 8.87, 11.76, 13.71, 14.52, 17.71, 21.80, 24.25, 25.23, 29.18])\n"
   ],
   "metadata": {
    "id": "uPzxHjmDUeJW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#prompt: write plot_fit(x,y, m, c) scatterplot points for input vectors x and, y, then nd draw the line y=mx+b for the inputvalues m,c.  Draw the residuals and sum their squared lengths.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fit(x,y, m, c):\n",
    "  # Scatterplot the points\n",
    "  plt.scatter(x, y, label='Observed Data')\n",
    "\n",
    "  # Draw the fitted line\n",
    "  x_line = np.array([min(x), max(x)])\n",
    "  y_line = m * x_line + c\n",
    "  plt.plot(x_line, y_line, color='red', label='Fitted Line')\n",
    "\n",
    "  # Calculate and draw the residuals\n",
    "  residuals = y - (m * A[:, 0] + c)\n",
    "  plt.vlines(x, y, y-residuals, color='green', linestyle='dashed', label='Residuals')\n",
    "\n",
    "  # Calculate the sum of squared residuals\n",
    "  sum_squared_residuals = np.sum(residuals**2)\n",
    "  plt.text(6, 5, f'Sum of Squared Residuals: {sum_squared_residuals:.2f}', color='purple')\n",
    "\n",
    "  # Add labels and legend\n",
    "  plt.xlabel('Applied Force')\n",
    "  plt.ylabel('Friction')\n",
    "  plt.title('Least Squares Fit')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "59kiNjiXYQeE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiment\n",
    "Change the last two numbers to check different lines $y=mx+b$.\n",
    "Residuals are the distance between the point on the line $(mx_i+c) - y_i$, usually with the absolute value or squared to make each residual positive.  "
   ],
   "metadata": {
    "id": "s-PioAQtzGiq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plot_fit(A[:, 0], b, 2, 5)"
   ],
   "metadata": {
    "id": "QWF90GbiYX2l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Best fit line in least-squares sense\n",
    "\n",
    "Suppose we want to minimize the squared residual: $\\textit{resid}^2 = {(Ax-b)}^2 = (Ax-b)^T(Ax-b)= x^TA^TAx-x^TA^Tb -b^TAx +b^Tb$.  \n",
    "Check the shape: this is a number.  We minimize any function $f(x)$ by taking a derivative $\\frac d{dx} f(x) =0$.  \n",
    "$\\frac d{dx} \\textit{resid}^2 = A^TAx + x^TA^TA-A^Tb -b^TA = 0$ by solving $A^TAx=A^Tb$.  \n",
    "Note that this is a square matrix with the same number of equations as unknowns, so we converted an overconstrained system to optimize to a solveable system."
   ],
   "metadata": {
    "id": "bE1NL6J60iJN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Solve for x in Ax = b using least squares\n",
    "x = np.linalg.lstsq(A, b, rcond=None)[0]\n",
    "m, c = x\n",
    "plot_fit(A[:, 0], b, m, c)\n",
    "print(f\"Best line: h = {m}x + {c}\")\n",
    "\n",
    "# Solve A^T * A * x = A^T * b for x\n",
    "x = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "m2, c2 = x\n",
    "print(f\"linalg.solve: m = {m2}, c = {c2}, error to previous {np.abs(m-m2)+np.abs(c-c2)}\")"
   ],
   "metadata": {
    "id": "eo_6ysByVGLs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM6lH8KxStFV"
   },
   "source": [
    "\n",
    "### Least Squares Reprise\n",
    "\n",
    "* Least squares requires a mathematical model, parameters to be found, and observed data.\n",
    "* Linear least squares problems can be written as Ax = b, where A and b are known, and x is the vector of unknowns.\n",
    "* The error is the squared residual: error = ||Ax - b||^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOGSwtgxStFV"
   },
   "source": [
    "\n",
    "### Fitting Other Models: Paris/Rio Temperatures\n",
    "\n",
    "Let's model the annual temperature cycle for Paris and Rio.\n",
    "\n",
    "The annual cycle looks like a sine wave, so we can try to fit a sine curve to the data.\n",
    "\n",
    "For months $m = 1:12$, consider a model of the form:\n",
    "\n",
    "$temp = A + B\\cos(m\\pi/6) + C\\sin(m\\pi/6)$\n",
    "\n",
    "We want to find the values of A, B, and C that best fit the temperature data.  This gives us a system of equations.  For example, for ParisHi, we have:\n",
    "\n",
    "$55 = A + B\\cos(1\\pi/6) + C\\sin(1\\pi/6)$  \n",
    "$55 = A + B\\cos(2\\pi/6) + C\\sin(2\\pi/6)$  \n",
    "$59 = A + B\\cos(3\\pi/6) + C\\sin(3\\pi/6)$  \n",
    "...\n",
    "\n",
    "And so on.  We can write this as a matrix equation $M @ [[A], [B],[C]] = temp$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1Vhamrb2QAJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "parisHi = np.array([55, 55, 59, 64, 68, 75, 81, 81, 77, 70, 63, 55])\n",
    "parisLo = np.array([39, 41, 45, 46, 55, 61, 64, 64, 61, 54, 49, 41])\n",
    "\n",
    "rioHi = np.array([84, 85, 83, 80, 77, 76, 75, 76, 75, 77, 79, 82])\n",
    "rioLo = np.array([73, 73, 72, 69, 66, 64, 63, 64, 65, 66, 68, 71])\n",
    "\n",
    "temp_rows = np.array([parisHi, parisLo, rioHi, rioLo])\n",
    "temp_legend = ['ParisHi', 'ParisLow', 'RioHi', 'RioLow']\n",
    "temp = temp_rows.transpose() # Swap rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Bpz5TOWStFV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "months = np.arange(1, 13)\n",
    "CON = np.ones(12)\n",
    "COS = np.cos(2*np.pi/12 * months)\n",
    "SIN = np.sin(2*np.pi/12 * months)\n",
    "\n",
    "M = np.column_stack([CON, COS, SIN])\n",
    "fit_params = np.linalg.lstsq(M, temp)[0]\n",
    "\n",
    "plt.plot(months, temp, '+', label=temp_legend)\n",
    "plt.plot(months, M @ fit_params, label=[\"parisHi_fit\", \"parisLo_fit\", \"rioHi_fit\", \"rioLo_fit\"])\n",
    "plt.legend(temp_legend + [\"parisHi_fit\", \"parisLo_fit\", \"rioHi_fit\", \"rioLo_fit\"])\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Temperature')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrjTdXUGStFW"
   },
   "source": [
    "\n",
    "#### Error in Fitting Sine & Cosine\n",
    "\n",
    "We can calculate the residual and error for this fit, too.  Since we fit four curves at once (one for each of \"parisHi\", \"parisLo\", \"rioHi\", and \"rioLo\"), we get four error values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4DWxOa6StFW"
   },
   "outputs": [],
   "source": [
    "resid = M @ fit_params - temp\n",
    "error = np.sum(resid**2, axis=0)\n",
    "\n",
    "print(\"Residuals:\\n\", resid)\n",
    "print(\"Errors:\\n\", error) # Four errors, one for each city\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bql2tbTYOKcG"
   },
   "source": [
    "This notebook draws on  Kyoosik Kim, 2 jan 2019, https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
