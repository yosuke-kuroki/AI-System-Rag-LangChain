{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SQytFC9mcyw"
   },
   "source": "# Predictive Modelling I: Classification & K-Nearest Neighbors (KNN)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGO3IHJFmbE5"
   },
   "source": [
    "# 1. Roadmap from SciKit-Learn\n",
    "![roadmap](https://scikit-learn.org/1.3/_static/ml_map.png)\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html  \n",
    "For an overview on supervised learning methods in the scikit-learn library: https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S9ccQTYmcy3"
   },
   "source": [
    "# 2. Dr. D's Amazin' Grocery Store\n",
    "\n",
    "Dr. D. launched his Amazin’ Grocery Store and immediately started to collect data on his customers (using a loyalty program where they only need to supply their phone number at check-out). He was also able to segment his existing customer base, which enables him to target them much more effectively with promotions that they might be interested in (using the appropriate medium).\n",
    "\n",
    "The segmentation of his customers is a manual process at this time.  Dr. D. has to hire experts from a renown consulting firm (Accidenture) every couple of years for USD 3500 per consultant per day plus travel expenses. Usually, the team of 2 consultants finishes their work within a week. Accidenture discovered 3 customer segment and classifies all existing customers for Dr. D. in his customer database.\n",
    "\n",
    "Clearly, Dr. D. would rather go on a (very!) nice vacation than pay that much money to Accidenture for the segmentation of his customers. He therefore decided that he should use the power of data science and AI to automatically assign **new customers** to the appropriate segment (after some data was collected on them).\n",
    "\n",
    "He put **YOU** in charge of the task and provided you with a dataset of customer records. The dataset contains the following information:\n",
    "\n",
    "- CustomerID\n",
    "- Name of Customer\n",
    "- Nickname of Customer\n",
    "- Average monthly spending in USD (i.e., revenue)\n",
    "- Average number of shopping trips to his grocery store\n",
    "- Average basket size per shopping trip (i.e., number of SKUs purchased on a trip)\n",
    "- The share of private label products bought by the customer\n",
    "- The share of organic products bought by the customer\n",
    "- Whether a customer actively uses the store’s own credit card or not\n",
    "- The segment each customer was assigned to by Accidenture\n",
    "\n",
    "**Let's see if we can save Dr. D. thousands of dollars by automating the assignment of new customers to the three customers segments using Machine Learning techniques!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbMWC3d4mcy4"
   },
   "source": [
    "## 2.1 Data Pre-Processing\n",
    "\n",
    "Before we can start, we need to:\n",
    "- Import Dr. D's Dataset (a csv file)\n",
    "- Inspect it\n",
    "- Make sure the data types are suitable for our purpose\n",
    "- Determine which variables are relevant for our prediction task\n",
    "- Extract the response and feature variables\n",
    "- Visually inspect our response variables to see what is going on in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Os_qeggHzIS"
   },
   "source": [
    "# This notebook uses prompts\n",
    "- In Google Colab, you can use Colab AI to create LLM-generated code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnFpqnavmcy4"
   },
   "source": [
    "### 2.1.1 Import Dr. D's Dataset (a csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYPKaEX19IyU"
   },
   "outputs": [],
   "source": [
    "# 0a Connect our Google Drive and switch to the folder that contains our data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# 0b Change permanently into directory where data files are located\n",
    "%cd /content/gdrive/MyDrive/488/Class17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "av4x09Zo9O2H"
   },
   "outputs": [],
   "source": [
    "# 0c See files that are in the current directory\n",
    "# special shell command to view the files in the home directory of the notebook environment (! command has no lasting effect)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iobr6RxrAESb"
   },
   "source": [
    "**A note on shell commands in python notebooks:** The difference between **!** and **%**\n",
    "\n",
    "- **!** calls out to a shell (in a new process),\n",
    "- **%** affects the process associated with the notebook (or the notebook itself)\n",
    "- many **%** commands have no shell counterpart.\n",
    "\n",
    "***!cd foo***, by itself, has no lasting effect, since the process with the changed directory immediately terminates.\n",
    "\n",
    "***%cd foo*** changes the current directory of the notebook process, which is a lasting effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcQ2My4pmcy5"
   },
   "outputs": [],
   "source": [
    "# 1a Import Pandas so we can load our data into a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# 1b import the data file\n",
    "customers_df = pd.read_csv(\"DrDsAmazinGroceryStore1.csv\") # in the /data subdirectory\n",
    "\n",
    "# 1c Take a look at the first 5 rows\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTh9UaCJmcy7"
   },
   "source": [
    "### 2.1.2 Inspect the data we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VcrILtgJDvN"
   },
   "outputs": [],
   "source": [
    "# prompt: describe customers_df\n",
    "\n",
    "customers_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGnGmf3jmcy9"
   },
   "source": [
    "### 2.1.3 Make sure the data types are suitable for our purpose  \n",
    "\n",
    "- Which columns would be useful to predict which segment a customer belongs to?\n",
    "- Can we use use the columns of interest in a machine learning model that predict's a customer's segment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGeiouW4JJDO"
   },
   "outputs": [],
   "source": [
    "# prompt: Using dataframe customers_df: tell me about the data types in this df\n",
    "\n",
    "customers_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqnhZVWLKcSX"
   },
   "outputs": [],
   "source": [
    "# prompt: Using dataframe customers_df: convert a string column 'Segment' to a categorical type and then map its categories to numeric codes in a DataFrame called customers_df. Show the DataFrame's first few rows\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the 'Segment' column to a categorical type\n",
    "customers_df['Segment'] = customers_df['Segment'].astype('category')\n",
    "\n",
    "# Map the categories to numeric codes\n",
    "customers_df['SegID'] = customers_df['Segment'].cat.codes\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "customers_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt: For SegID in df, make a dictionary that converts to Segment\n",
    "\n",
    "# Create a dictionary to map SegID to Segment\n",
    "segment_mapping = dict(zip(customers_df['SegID'], customers_df['Segment']))\n",
    "\n",
    "segment_mapping\n"
   ],
   "metadata": {
    "id": "aScdSN6IrbDm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qr84o2faKv5w"
   },
   "outputs": [],
   "source": [
    "# prompt: Using dataframe customers_df: check the mean by `segment` column for organic products\n",
    "\n",
    "customers_df.groupby(['Segment']).Organic.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A0I8nl4mcy-"
   },
   "source": [
    "### 2.1.4 Extract the response and feature variables\n",
    "\n",
    "- We only want to use the variables of interest for our prediction of customer segments.\n",
    "- These are the:\n",
    "    - average monthly revenue (Revenue)\n",
    "    - the average number of trips per month (Trips)\n",
    "    - the average basket size (BasketSize)\n",
    "    - the percent share of private label products (Plabel)\n",
    "    - and the percent share of organic products (organic)    \n",
    "\n",
    "*We will ignore the variable \"StoreCC\" for now.*\n",
    "\n",
    "**Let's create two arrays for our prediciton problem:**\n",
    "- The first array holds the segment data (our response variable).\n",
    "- The second array holds the variables of interest (our feature variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRV7kR3ZLFWf"
   },
   "outputs": [],
   "source": [
    "# prompt: Using dataframe customers_df: extract feature variables and a response variable SegID from a pandas DataFrame customers_df for machine learning, excluding specific columns, and display their shapes using numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the columns to exclude\n",
    "exclude_cols = ['CustomerID', 'CustomerName', 'CustomerNick', 'Segment', 'StoreCC', 'SegID']\n",
    "\n",
    "# Extract feature variables\n",
    "feature_cols = [col for col in customers_df.columns if col not in exclude_cols]\n",
    "X = customers_df[feature_cols]\n",
    "\n",
    "# Extract the response variable\n",
    "y = customers_df['SegID']\n",
    "\n",
    "# Print the shapes of X and y\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7L7fbt4mcy_"
   },
   "source": [
    "### 2.1.5 Visual inspection of the data\n",
    "\n",
    "Let's visually inspect how the features in our dataset relate to another!\n",
    "\n",
    "***What can we already learn by just \"eyeballing\" the data?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU-Kga5Rmcy_"
   },
   "outputs": [],
   "source": [
    "# 5a Import seaborn library for visualizing data conveniently\n",
    "import seaborn as sns\n",
    "\n",
    "# 5b Construct a new dataframe that contains our response and feature variables as input to our plot\n",
    "feature_df =  customers_df.drop(['SegID', 'CustomerID', 'CustomerName', 'Segment','StoreCC', 'CustomerNick'], axis=1)\n",
    "response_df = customers_df['Segment']\n",
    "\n",
    "# 5c Join the dataframes\n",
    "joint_df = pd.concat([response_df, feature_df], axis=1)\n",
    "\n",
    "# 5d Create a matrix of scatter plots\n",
    "sns.set(style=\"ticks\")\n",
    "sns.pairplot(joint_df, vars=['Basket','PLabel','Organic', 'Spending','Trips'], hue='Segment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxidfe18L7Y4"
   },
   "outputs": [],
   "source": [
    "# prompt: Using dataframe customers_df: visualize relationships between multiple variables in a pandas DataFrame using seaborn's pairplot, including specific features and a categorical hue\n",
    "# An alternative to the above code block\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the features to include in the pairplot\n",
    "features = ['Spending', 'Basket', 'Trips', 'PLabel', 'Organic']\n",
    "\n",
    "# Create the pairplot with the specified features and hue\n",
    "sns.pairplot(customers_df, vars=features, hue='Segment')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOpRBlNSmcy_"
   },
   "source": [
    "# 3. Predicting Customer Segments using Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHoZHGSEmczJ"
   },
   "source": [
    "## 3.1 We will use Supervised (Machine) Learning to solve Dr. D's problem\n",
    "\n",
    "**Objective** of Supervised (Machine) Learning: Automate time-consuming or expensive manual tasks  \n",
    "\n",
    "**Examples:**\n",
    "- Doctor’s diagnosis\n",
    "- Make predictions about the future\n",
    "- Will a customer click on an ad or not?\n",
    "\n",
    "**Requires:** Labeled data  \n",
    "- Historical data with labels\n",
    "- Experiments to get labeled data\n",
    "- Crowd-sourcing labeled data\n",
    "\n",
    "**Taks/Models:**\n",
    "- Classification: should we target a consumer?\n",
    "- Regression: how much revenue can we expect from a consumer?\n",
    "\n",
    "**Binary vs. Multiclass Prediction**\\\n",
    "Today, firms largely use (are biased towards) classification models.  \n",
    "The reason behind this bias towards classification models is that most analytical problems involve making a decision that requires a simple Yes/No answer:\n",
    " - Will a customer churn or not\n",
    " - Will a customer respond to ad campaign or not\n",
    " - Will the firm default or not  \n",
    "In these cases, we use binary classification.\n",
    "\n",
    "However, **it is also possible to predict multiple classes** at once! Instead of a Yes/No (i.e, positive vs. negative) prediction, a supervised classification model can also be trained to predict multiple classes (e.g., segment 1 vs. segment 2 vs. segment 3). We call this **multiclass prediction**, and we will use it to help Dr. D. segment new customers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_cgjmDjmczJ"
   },
   "source": [
    "## 3.2 The K-Nearest Neighborhod (KNN) Machine Learning Algorithm for Classification\n",
    "\n",
    "\n",
    "### *Show me who your friends are, and I’ll tell you who you are*\n",
    "\n",
    "The concept of KNN can hardly be described more simply. This is an old saying, which can be found in many languages and many cultures.\n",
    "\n",
    "\n",
    "**Basic idea:** Predict the label of a data point by  \n",
    "- Looking at the ‘k’ closest labeled data points\n",
    "- Taking a majority vote  \n",
    "\n",
    "**Underlying Principle**:\n",
    "- Find a predefined number (k) training samples closest in distance to a new sample that has to be classified\n",
    "- The label of the new sample will be defined from these neighbors\n",
    "- KNN has a fixed user defined constant for the number of neighbors which have to be determined\n",
    "\n",
    "\n",
    "![KNN explained from www.python-course.eu/images/k_NN.png](http://www.python-course.eu/images/k_NN.png \"KNN Intuition\")\n",
    "\n",
    "\n",
    "### 3.2.1 *Let's train a model that can predict to which segment a customer belongs using K-Nearest Neigbors (KNN)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3ifCYfcMaeT"
   },
   "outputs": [],
   "source": [
    "# prompt: split the dataset into training and test sets using scikit-learn, then instantiate and fit a KNeighborsClassifier model\n",
    "\n",
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=21)\n",
    "\n",
    "# 2 Check if our sample is split as we expected\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "\n",
    "# Instantiate a KNeighborsClassifier with 3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViuyeEOdmczK"
   },
   "source": [
    "### 3.2.2 So how good is our model at prediction to which segment a customer belongs?\n",
    "\n",
    "***Let's predict the segment of all customers in our test data set and check how often our model was right!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nXwr9W0mczK"
   },
   "outputs": [],
   "source": [
    "# 1 Run prediction on test data\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Test set predictions: \\n {}\".format(y_pred),\"\\n\")\n",
    "\n",
    "# 2 Calculate the accuracy of our prediction using np.mean\n",
    "print(\"Accuracy of Predicition (Manual scoring): {:.2f}\".format(np.mean(y_pred==y_test)))\n",
    "\n",
    "# 3 Alternatively, we can use knn's internal score function\n",
    "print(\"Accuracy of Predicition (KNN internal scoring): {:2f}\".format(knn.score(X_test, y_test)))\n",
    "\n",
    "# 4 Alternatively, we can import a library from sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy of Predicition (sklearn scoring): {round(accuracy_score(y_test, y_pred)*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTqkeW88mczK"
   },
   "source": [
    "### 3.2.3 Predicting the segment of a new customer that we have no segment information on\n",
    "\n",
    "- Basket_Size is 16\n",
    "- Share_Private_Label is 25%\n",
    "- Share_Organic is 25%\n",
    "- Revenue is USD400\n",
    "- Number of Trips are 6"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt: Give me a row from X_train where y_train is not Senior\n",
    "\n",
    "import numpy as np\n",
    "# Find the index where y_train is not equal to 'Senior' (assuming 'Senior' is represented by its numeric code)\n",
    "# Assuming 'Senior' corresponds to SegID 2 in your data\n",
    "non_senior_indices = np.where(y_train != 2)[0]\n",
    "\n",
    "# If non_senior_indices is empty, no such row exists. Otherwise print the first such row\n",
    "if len(non_senior_indices) > 0:\n",
    "  first_non_senior_index = non_senior_indices[0]\n",
    "  print(X_train.iloc[first_non_senior_index])\n",
    "else:\n",
    "  print(\"No rows found where y_train is not 'Senior'\")\n"
   ],
   "metadata": {
    "id": "jIXtOkPKyDoq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt: If I give a new customer X data [50, 25, 25, 200, 6], what are those categories?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the order of features is: Spending, Basket, Trips, PLabel, Organic\n",
    "new_customer_data = np.array([[200, 6, 50, 25, 25]])\n",
    "\n",
    "# Load the pre-trained KNN model (you need to have trained it beforehand)\n",
    "# ... Load your model from the previous code block, e.g., using joblib ...\n",
    "\n",
    "# Predict the segment for the new customer\n",
    "predicted_segment_id = knn.predict(new_customer_data)\n",
    "\n",
    "# Map the predicted segment ID back to the segment name\n",
    "segment_mapping = {0: 'Budget', 1: 'Mainstream', 2: 'Senior'} # Replace with your actual mapping\n",
    "predicted_segment = segment_mapping[predicted_segment_id[0]]\n",
    "\n",
    "print(f\"The predicted segment for the new customer is: {predicted_segment}\")\n"
   ],
   "metadata": {
    "id": "Cv17aGE3yTCd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tq8gL6UlNUjm"
   },
   "outputs": [],
   "source": [
    "# prompt: predict the customer segment for a new record using a pre-trained KNeighborsClassifier model, and translate the numeric prediction back to a segment name using a dictionary\n",
    "\n",
    "# Create a new record\n",
    "x_new = np.array([[50, 25, 25, 200, 6]])\n",
    "\n",
    "# Predict the customer segment using the pre-trained model\n",
    "prediction = knn.predict(x_new)\n",
    "\n",
    "# Create a dictionary to translate numeric prediction to segment names\n",
    "replace_map = {'Segment': {2: 'Yuppies', 1: 'Seniors', 0: 'Families'}}\n",
    "\n",
    "# Print the predicted segment name\n",
    "print(\"Predicted Segment:\", replace_map.get('Segment', {}).get(prediction[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-hkYxbgmczL"
   },
   "source": [
    "# 4. Improving our Prediction\n",
    "\n",
    "**So how can we do better in our prediction?**  \n",
    "There are a few things we can easily change:\n",
    "- The distribtution of segments (labels) within the train and test samples\n",
    "- The size of the testing sample\n",
    "- The value for k\n",
    "- The scales of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxU6PRCgGPd6"
   },
   "source": [
    "## 4.1 Even Distribution of Labels\n",
    "- We want to have the same distribution of labels in our training and testing sets\n",
    "  - use stratify by dependent variable (here, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-ZnwVCeNsry"
   },
   "outputs": [],
   "source": [
    "# prompt: Show how to split a dataset into training and test sets, train a KNeighborsClassifier, make predictions on the test set, and calculate accuracy using scikit-learn\n",
    "\n",
    "# 1. Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=21, stratify=y)\n",
    "\n",
    "# 2. Instantiate the KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# 3. Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 5. Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 6. Print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nif6riQbG0Eb"
   },
   "source": [
    "## 4.2 Size of Test Set\n",
    "- The larger the testing set, the less data the classifier has to train on\n",
    "  - Make test set smaller: How much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuLxRVTjNt3D"
   },
   "outputs": [],
   "source": [
    "# prompt: Explain how to prepare a dataset for machine learning by splitting it into training and test sets, training a KNeighborsClassifier with 4 of neighbors, predicting on the test data, and computing the model's accuracy, all using scikit-learn\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "# Instantiate the KNeighborsClassifier with 3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# Train the model on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ea--11imczN"
   },
   "source": [
    "## 4.3 Overfitting and Underfitting: Finding \"k\"\n",
    "\n",
    "Changing k leads to different results. So what is the *right* value for k?\n",
    "\n",
    "**Let's make the impact of setting k to different values visible:**\n",
    "- Show boundaries of each class (i.e., segment) in a graph\n",
    "- These \"Decision Boundaries\" separate our three segments from another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PbenshGmczN"
   },
   "source": [
    "### 4.3.1 Visualizing Decision Boundaries for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcsQRb7wmczN"
   },
   "outputs": [],
   "source": [
    "# 0 Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors\n",
    "\n",
    "# 1 First split the sample into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)\n",
    "\n",
    "# 2 Select features and set step size\n",
    "X2 = minmax_scale(X_train.iloc[:, [0, 1]].values)  # .values to ensure it's an array\n",
    "y2 = y_train\n",
    "h = .01  # step size in the mesh\n",
    "\n",
    "# 3 Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# 4 Plot for different values of k\n",
    "for k in [2,4,8,16,32,99]:\n",
    "    # 4a We create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X2, y2)\n",
    "\n",
    "    # 4b Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X2[:, 0].min() - .1, X2[:, 0].max() + .1\n",
    "    y_min, y_max = X2[:, 1].min() - .1, X2[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # 4c Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n",
    "\n",
    "    # 4d Plot also the training points\n",
    "    plt.scatter(X2[:, 0], X2[:, 1], c=y_train, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i)\"\n",
    "              % (k), fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D06zVDiymczO"
   },
   "source": [
    "### Study the above graphs carefully. What do you see as k get's larger?\n",
    "\n",
    "- As k gets larger, the boundaries become smoother\n",
    "- As k approaches the number of customers, the whole graph will take on a single color\n",
    "\n",
    "### --> Model Complexity\n",
    "\n",
    "- Larger k = smoother decision boundary = less complex model\n",
    "- Smaller k = more complex model = can lead to overfiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6M476ySmczO"
   },
   "source": [
    "### 4.3.2 Overfitting and Underfitting\n",
    "\n",
    "#### The goal of a good machine learning model is to generalize well from the training data to any data from the problem domain.\n",
    "- This allows us to make predictions in the future on data the model has never seen.\n",
    "\n",
    "A model can be poorly trained by overfitting or underfitting the data:\n",
    "\n",
    "- **Overfitting**\n",
    "    - Happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.   \n",
    "    - An overfit model is one where performance on the train set is good and continues to improve, whereas performance on the validation set improves to a point and then begins to degrade.  \n",
    "    \n",
    "\n",
    "- **Underfitting**\n",
    "    - Refers to a model that can neither model the training data nor generalize to new data.   \n",
    "    - If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks.\n",
    "    - Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and classify data.\n",
    "    - High bias and low variance are good indicators of underfitting.\n",
    "\n",
    "We can test to what extend different values of k overfit or underfit the data.  \n",
    "\n",
    "We proceed as follows:\n",
    "1. Compute and plot the training and testing accuracy scores for a variety of different neighbor values (k).\n",
    "2. Inspect how the accuracy scores differ for the training and testing sets with different values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw8ayF3cTCGj"
   },
   "outputs": [],
   "source": [
    "# prompt: evaluate and plot the training and testing accuracy of a KNeighborsClassifier for varying numbers of neighbors\n",
    "\n",
    "# 1. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=21, stratify=y)\n",
    "\n",
    "# 2. Setup arrays to store train and test accuracies\n",
    "neighbors = np.arange(1, 25)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "# 3. Loop over different values of k\n",
    "for i, k in enumerate(neighbors):\n",
    "    # 3a Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # 3b Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # 3c Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    # 3d Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "# 4. Generate plot\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('Number of Neighbors', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "plt.title('k-NN: Varying Number of Neighbors', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m99UD8zXVxLC"
   },
   "outputs": [],
   "source": [
    "# prompt: instantiate, fit, and evaluate a KNeighborsClassifier with an optimal number of neighbors on a test dataset, including calculating the model's accuracy\n",
    "\n",
    "# Instantiate the KNeighborsClassifier with an optimal number of neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make the prediction for the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy of Predicition: {:2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkHo0M40mczL"
   },
   "source": [
    "## 4.4 It appears that the features that may drive the segment memberships of customers are on different scales.\n",
    "\n",
    "What happens when we re-scale them to the same scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GG9OdFLmczL"
   },
   "outputs": [],
   "source": [
    "# Assuming X is a pandas DataFrame\n",
    "X_scaled = minmax_scale(X)\n",
    "\n",
    "# Adjusted plotting code to use .iloc for integer-location based indexing\n",
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(16,16))\n",
    "sns.histplot(X_scaled[:,0], ax=ax[0,0], color='y')\n",
    "ax[0,0].set_title(\"Scaled Data\", fontsize=20)\n",
    "\n",
    "# Use .iloc for indexing if X is a DataFrame\n",
    "sns.histplot(X.iloc[:,0], ax=ax[0,1])\n",
    "ax[0,1].set_title(\"Original Data\", fontsize=20)\n",
    "sns.histplot(X_scaled[:,1], ax=ax[1,0], color='y')\n",
    "sns.histplot(X.iloc[:,1], ax=ax[1,1])\n",
    "sns.histplot(X_scaled[:,2], ax=ax[2,0], color='y')\n",
    "sns.histplot(X.iloc[:,2], ax=ax[2,1])\n",
    "sns.histplot(X_scaled[:,3], ax=ax[3,0], color='y')\n",
    "sns.histplot(X.iloc[:,3], ax=ax[3,1])\n",
    "sns.histplot(X_scaled[:,4], ax=ax[4,0], color='y')\n",
    "sns.histplot(X.iloc[:,4], ax=ax[4,1])\n",
    "plt.show()\n",
    "\n",
    "# Technical Note: that the y-axis in a density plot is the probability density function for the kernel density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGLaO57zmczM"
   },
   "source": [
    "### 4.4.1 Let's build/train our KNN model again - this time with the re-scaled variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0x1fMD-omczM"
   },
   "outputs": [],
   "source": [
    "# 1 Split sample into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.4 ,random_state=21, stratify=y)\n",
    "\n",
    "# 2 Import the k-nearest neighbors classifier from sci-kit learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 3 Instantiate the KNeighborsClassifier with a n_neighbors value of 4\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# 4 Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 5 Make the prediction for the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 6 And calculate the accuracy\n",
    "print(\"Accuracy of Predicition: {:2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqQy6RunW5AZ"
   },
   "outputs": [],
   "source": [
    "# prompt: Show how to prepare scaled data for machine learning by splitting into training and test sets, train a KNeighborsClassifier with a specified number of neighbors, make predictions on the test data, and compute the model's accuracy\n",
    "\n",
    "# Split the scaled data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.4, random_state=21, stratify=y)\n",
    "\n",
    "# Instantiate the KNeighborsClassifier with 4 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# Train the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Compute the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
