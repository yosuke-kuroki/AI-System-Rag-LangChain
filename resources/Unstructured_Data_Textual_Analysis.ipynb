{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkUYF0gHmDP_"
      },
      "source": [
        "# **BUSI/COMP 488 Data Science in the Business World**\n",
        "## *Spring 2025*  \n",
        "Daniel M. Ringel  \n",
        "Kenan-Flagler Business School  \n",
        "*The University of North Carolina at Chapel Hill*  \n",
        "dmr@unc.edu  www.ringel.ai\n",
        "\n",
        "![Text Analysis I](https://www.mapxp.app/BUSI488/textanalysisI.jpg \"Text Analysis I\")\n",
        "## Class 04 - **Unstructured Data**: Introduction to Text Analysis\n",
        "*January 21, 2025*  \n",
        "Version 1.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC9eVEQhmCfI"
      },
      "source": [
        "# Today's Agenda\n",
        "\n",
        "1. **Text Analysis**\n",
        "2. **Regular Expressions**\n",
        "3. **Tokenization**\n",
        "4. **Tokenizing Social Media**\n",
        "5. **Bag-of-Words**\n",
        "6. **NaÃ¯ve Topic Discovery**\n",
        "7. **Sentiment Analysis**\n",
        "8. **Sentiment Analysis of Social Media**\n",
        "\n",
        "\n",
        "## Prep-Check:\n",
        "- Sign-up to DataCamp\n",
        "- Completed Class Takeaway Surveys for classes 1 - 3\n",
        "- Started HW1: DataCamp Python Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6SeiGmJd5ag"
      },
      "source": [
        "# 1 Text Analysis\n",
        "\n",
        "#### What is Text Analysis?\n",
        "\n",
        "- Text Analysis (also called text mining or text extraction) is about parsing texts in order to extract machine-readable facts from them\n",
        "- The purpose of Text Analysis is to create structured data out of free text content\n",
        "- The process can be thought of as slicing and dicing heaps of unstructured, heterogeneous documents into easy-to-manage and interpret data pieces\n",
        "\n",
        "**Text Analysis** is the term that describes the very process of computational analysis of texts.  \n",
        "\n",
        "vs.   \n",
        "\n",
        "**Text Analytics** which are a set of techniques and approaches that create *insights* by identifying *trends* and/or *patterns* in the prepared data.  \n",
        "\n",
        "> ### ***Key Take-away:***  Text Analysis helps translate a text into the language of data. Once Text Analysis â€œpreparedâ€ the content, Text Analytics kicks in to help make sense of these data.\n",
        "\n",
        "### Text Analysis for Business\n",
        "\n",
        "- Firms use Text Analysis to set the stage for a data-driven approach towards managing content.\n",
        "- Once textual sources are sliced into easy-to-automate data pieces, a whole new set of opportunities opens for processes like:\n",
        "    - decision making\n",
        "    - product development\n",
        "    - marketing optimization\n",
        "    - business intelligence\n",
        "    - process automation\n",
        "    - and more\n",
        "    \n",
        "\n",
        "### The Big Picture: Natural Language Processing (NLP)\n",
        "\n",
        "- NLP is a subfield of linguistics, computer science, information engineering, and artificial intelligence\n",
        "- Concerned with the interactions between computers and human (natural) languages\n",
        "- Central question: How to program computers to process and analyze large amounts of natural language data\n",
        "\n",
        "**Challenges in NLP:**\n",
        "- Speech recognition\n",
        "- Natural language understanding\n",
        "- Natural language translation\n",
        "- Natural language generation\n",
        "\n",
        "\n",
        "### **Today, we examine NLP foundations for Text Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XhCKs9xd5ai"
      },
      "source": [
        "## 1.1 Text: here, there, and everywhere!\n",
        "\n",
        "Text is a very common type of unstructured data.\n",
        "\n",
        "![Unstructured Data - EVERYWHERE!](https://mapxp.app/BUSI488/GoogleKFBS \"Unstructured Data - EVERYWHERE!\")\n",
        "\n",
        "\n",
        "### Text is EVERYWHERE!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XREHasyWd5ak"
      },
      "source": [
        "## 1.2 Text Analysis Tools and Tasks\n",
        "\n",
        "\n",
        "![Text Analysis Tools and Tasks](https://mapxp.app/BUSI488/TextAnalysis-Tools+Tasks.jpg \"Text Analysis Tools and Tasks\")\n",
        "\n",
        "### *Let's roll-up our sleeves and build the foundations of text analysis together!*  \n",
        "\n",
        "   \n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt7w8EZ0d5al"
      },
      "source": [
        "# 2 Regular Expressions\n",
        "\n",
        "At the most basic level, Text Analysis helps analysts to:\n",
        "\n",
        "1. Find information in Text\n",
        "2. Extract information from Text\n",
        "3. Count word frequencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZOnacGHOaKj"
      },
      "source": [
        "## 2.1 Introduction to Regular Expression\n",
        "\n",
        "#### *What are regular expressions (RegEx)?*\n",
        "\n",
        "- A RegEx is a pattern string with syntax to allow alternatives, wildcards, and repeats\n",
        "- A RegEx either matches another string or does not.\n",
        "- Why are they defined?\n",
        "   - Each RegEx converts to a simple programs with no variables; fast matching!\n",
        "   - There is no RegEx for the pattern \"every ( has a closing )\"; RegEx can't count...\n",
        "\n",
        "\n",
        "#### Applications of regular expressions:\n",
        "- Find all web links or phone numbers in a document\n",
        "- Parse email addresses, remove/replace unwanted characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPLDaL1dd5am"
      },
      "source": [
        "## 2.2 Basic RegEx\n",
        "\n",
        "Python syntax: (Other languages will differ. E.g. SQL will use _ for . and % for .*)\n",
        "\n",
        "![Basic RegEx](https://mapxp.app/BUSI488/RegEx.jpg \"BasicRegEx\")\n",
        "\n",
        "### 2.2.1 Pythons re (RegEx) module\n",
        "\n",
        "- split: split a string on regex\n",
        "- findall: find all patterns in a string\n",
        "- search: search for a pattern\n",
        "- match: match an entire string or substring based on a pattern\n",
        "\n",
        "**Example:**  re.split('PATTERN', 'Your string.')\n",
        "\n",
        "- Pattern first, and the string second\n",
        "- May return an iterator, string, or match object\n",
        "- GeminiAI can write these for you once you know they exist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpyrUlGbh6w-"
      },
      "outputs": [],
      "source": [
        "# Splitting a Sentence into words\n",
        "\n",
        "# 1. Import necessary modules\n",
        "import re\n",
        "\n",
        "# 2. Define text\n",
        "my_string = \"The ceiling is the roof!\"\n",
        "\n",
        "# 3. Split by Spaces\n",
        "pattern = r\"\\s+\"\n",
        "display(re.split(pattern, my_string ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Turn mystring into a list of words by splitting at spaces\n",
        "\n",
        "import re\n",
        "\n",
        "# 3. Split by Spaces\n",
        "pattern = r\"\\s+\"\n",
        "word_list = re.split(pattern, my_string)\n",
        "word_list"
      ],
      "metadata": {
        "id": "OIGJUol8CD7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1686yJ9jare"
      },
      "source": [
        "**Note**: the *r* before the patterns indicates that this is a raw string where all escape codes are to be ignored.\n",
        "\n",
        "*For example:*\n",
        "\n",
        "*'\\n'* will be treated as a newline character, while **r**'\\n' will be treated as the characters \\ followed by n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4apwWSIakHXO"
      },
      "source": [
        "## 2.3 Splitting-up Text into Words: Excerpt of OpenAI/Microsoft Press Release 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTWxAaBsQN9E"
      },
      "outputs": [],
      "source": [
        "# 1. Press Release Text\n",
        "openai='Today, we are announcing the third phase of our long-term partnership with OpenAI through a multiyear, multibillion dollar investment to accelerate AI breakthroughs to ensure these benefits are broadly shared with the world.\\\n",
        "This agreement follows our previous investments in 2019 and 2021. It extends our ongoing collaboration across AI supercomputing and research and enables each of us to independently commercialize the resulting advanced AI technologies.\\\n",
        "Supercomputing at scale â€“ Microsoft will increase our investments in the development and deployment of specialized supercomputing systems to accelerate OpenAI\\â€™s groundbreaking independent AI research. We will also continue to build out Azure\\â€™s leading AI infrastructure to help customers build and deploy their AI applications on a global scale.\\\n",
        "New AI-powered experiences â€“ Microsoft will deploy OpenAIâ€™s models across our consumer and enterprise products and introduce new categories of digital experiences built on OpenAIâ€™s technology. This includes Microsoftâ€™s Azure OpenAI Service, which empowers developers to build cutting-edge AI applications through direct access to OpenAI models backed by Azureâ€™s trusted, enterprise-grade capabilities and AI-optimized infrastructure and tools.\\\n",
        "Exclusive cloud provider â€“ As OpenAIâ€™s exclusive cloud provider, Azure will power all OpenAI workloads across research, products and API services.\\\n",
        "â€œWe formed our partnership with OpenAI around a shared ambition to responsibly advance cutting-edge AI research and democratize AI as a new technology platform,â€ said Satya Nadella, Chairman and CEO, Microsoft. â€œIn this next phase of our partnership, developers and organizations across industries will have access to the best AI infrastructure, models, and toolchain with Azure to build and run their applications.â€\\\n",
        "â€œThe past three years of our partnership have been great,â€ said Sam Altman, CEO of OpenAI. â€œMicrosoft shares our values and we are excited to continue our independent research and work toward creating advanced AI that benefits everyone.â€\\\n",
        "Since 2016, Microsoft has committed to building Azure into an AI supercomputer for the world, serving as the foundation of our vision to democratize AI as a platform. Through our initial investment and collaboration, Microsoft and OpenAI pushed the frontier of cloud supercomputing technology, announcing our first top-5 supercomputer in 2020, and subsequently constructing multiple AI supercomputing systems at massive scale. OpenAI has used this infrastructure to train its breakthrough models, which are now deployed in Azure to power category-defining AI products like GitHub Copilot, DALLÂ·E 2 and ChatGPT.\\\n",
        "These innovations have captured imaginations and introduced large-scale AI as a powerful, general-purpose technology platform that we believe will create transformative impact at the magnitude of the personal computer, the internet, mobile devices and the cloud.\\\n",
        "Underpinning all of our efforts is Microsoft and OpenAIâ€™s shared commitment to building AI systems and products that are trustworthy and safe. OpenAIâ€™s leading research on AI Alignment and Microsoftâ€™s Responsible AI Standard not only establish a leading and advancing framework for the safe deployment of our own AI technologies, but will also help guide the industry toward more responsible outcomes.\"'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: How many times does \"the\" appear as a word in text openai?\n",
        "\n",
        "import re\n",
        "words = re.findall(r'\\Wthe\\W', openai.lower())\n",
        "print(len(words))"
      ],
      "metadata": {
        "id": "otD9eCcR0Vjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGKpRkDid5am"
      },
      "outputs": [],
      "source": [
        "# 2. Define Pattern\n",
        "PATTERN =  r\"\\w+\"\n",
        "\n",
        "# 3. Use findall to get all words from a recent press release by OpenAi and Microsoft\n",
        "set(re.findall(PATTERN, openai))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crsAQ-X2d5ao"
      },
      "outputs": [],
      "source": [
        "# 4. Let's use RegEX to extract some data from a sentence in the above excerpt:\n",
        "my_text = 'Exclusive cloud provider â€“ As OpenAIâ€™s exclusive cloud provider, Azure will power all OpenAI workloads 24/7 across research, products and API services in 2023 and beyond.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYeixA6Bd5ao"
      },
      "outputs": [],
      "source": [
        "# 5. Split my_text on spaces and display the result\n",
        "spaces = r\"\\s+\"\n",
        "display(set(re.split(spaces, my_text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m0JgL6xd5ap"
      },
      "outputs": [],
      "source": [
        "# 6. Find all digits in my_text and display the result\n",
        "digits = r\"\\d+\"\n",
        "display(re.findall(digits, my_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNTP4DURd5ap"
      },
      "outputs": [],
      "source": [
        "# 7. Find all unique words in my_text and display the result\n",
        "words = r\"\\w+\"\n",
        "display(set(re.findall(words, my_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMNQLWcSd5ap"
      },
      "source": [
        "***What is the difference between the output of 5. (split \\s+)  and 7. (findall \\w+)?***\n",
        "\n",
        "1. splits by space and includes special characters like -\n",
        "3. findall extracts only alphanumeric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Count the occurrences of words that appear more than once in openai.  (Split by spaces and punctuation)\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Preprocess the text\n",
        "text = re.sub(r'[^\\w\\s]', '', openai).lower() # Remove punctuation and lowercase\n",
        "words = text.split()\n",
        "\n",
        "# Count word occurrences\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Find and print words that appear more than once\n",
        "for word, count in word_counts.items():\n",
        "    if count > 1:\n",
        "        print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "CPe_Krn5Cd_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd7GJ2Hhd5aq"
      },
      "source": [
        "## 2.4 Finding Information in Text using RegEx\n",
        "\n",
        "You have several options to find words or parts of words in a text. Three common functions are:\n",
        "1. re.match()     - tries to match from the beginning\n",
        "2. re.search()    - searches through entire string\n",
        "3. re.findall()   - finds *all* the matches and returns them as a list of s strings, with each string representing one match.\n",
        "\n",
        "*What is the difference?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBC5WSl7hLsa"
      },
      "source": [
        "## 2.5 Extracting Information from Product Descriptions\n",
        "\n",
        "![Swagtron Text Analysis](https://mapxp.app/BUSI488/SwagtronEB7.jpg \"Swagtron Text Analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0tBmrUwgFGh"
      },
      "outputs": [],
      "source": [
        "# 1. Import required packages\n",
        "from google.colab import drive\n",
        "\n",
        "# 2. Mount google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 3. Change into the directory our data are in\n",
        "%cd /content/gdrive/MyDrive/488/Class04\n",
        "\n",
        "# 4. List files in current directory\n",
        "!ls # special shell command to view the files in the home directory of the notebook environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPuextDNhSQa"
      },
      "outputs": [],
      "source": [
        "# 5. Load the file and display text\n",
        "general_description = open(\"simple_product_text.txt\").read()\n",
        "display(general_description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ0-vTxld5au"
      },
      "source": [
        "### 2.5.1 Brand Mentions in Text\n",
        "\n",
        "How prevalent is a brand in a product description?\n",
        "\n",
        "We can easily find out how many times your brand is mentioned in a text\n",
        "- or in many descriptions\n",
        "- or in reviews\n",
        "- or in news\n",
        "- or in any other text source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gIxvabaUyWf"
      },
      "outputs": [],
      "source": [
        "# 1. Define pattern you are looking for: Here a Brand Name\n",
        "pattern = r'Shimano'\n",
        "\n",
        "# 2. Use re.findall to find our pattern (the brand name) in entire text. We set a flag to ignore the case (i.e., allow any capitalization)\n",
        "match = re.findall(pattern, general_description, flags=re.IGNORECASE)\n",
        "print(match)\n",
        "\n",
        "# 3. Because we get list back, we import counter to count how many times our pattern appeared in the text. Returns a dict!\n",
        "from collections import Counter\n",
        "print (Counter(match))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_P011DymLm4"
      },
      "source": [
        "### 2.5.2 Extracting key performance indicators (KPIs) from text\n",
        "\n",
        "Perhaps you are in search of specific types of numbers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtVjESbId5au"
      },
      "outputs": [],
      "source": [
        "# 1. Define a pattern for anything with a percent\n",
        "pattern = r'\\w*.%'\n",
        "\n",
        "# 2. Use re.search to find the first word that is followed by a % symbol\n",
        "display(re.findall(pattern, general_description))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initially we used +, which is a mistake.  Do you see why?\n",
        "display(re.findall(r'\\w+.%', \" 1%. then 2 % and 3%.  Then 45%. Finally 100%, which is 100 %.\"))"
      ],
      "metadata": {
        "id": "0T5OHRS-2I7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvGFoFYhd5av"
      },
      "source": [
        "## 2.6 More Complex Regular Expressions\n",
        "\n",
        "RegEx is extremely flexible and powerful. You can use them to find:\n",
        "- groups of words using ()\n",
        "- character ranges using []\n",
        "- and either one *or* the other using \"|\"\n",
        "\n",
        "\n",
        "![More Complex RegEx](https://mapxp.app/BUSI488/ComplexRegEx.jpg \"More Complex RegEx\")\n",
        "\n",
        "There is a great regular expression editor that you might want to try out: https://regex101.com/\n",
        "\n",
        "***HINT:*** Use genAI to build your regular expression. Then test them in the regular expression editor and check for \"corner solutions\" (i.e., create and test rare and difficult cases).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ALNR-WmlOa"
      },
      "source": [
        "### 2.6.1 Extracting Phone Numbers from Text\n",
        "\n",
        "Using regular expressions it is easy to extract even more complex numbers like phone numbers or social security numbers.\n",
        "\n",
        "***Why might this be useful?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj7IXkrqd5av"
      },
      "outputs": [],
      "source": [
        "# 1. Extracting phone numbers from a text\n",
        "display(re.findall(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', general_description))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZegQFTsd5av"
      },
      "outputs": [],
      "source": [
        "# 3. The RegEx will find phone numbers of different formats!\n",
        "alternative_text = '+79082343434 keeps going  8(912)2342554,  +7 982 342 some random words 911 77 more stuff 8-923-132-34-23\\\n",
        "                    +7 982 342 34 34! who knows! I call 919-962-8746 if I have questions.'\n",
        "\n",
        "# 4. Display our Text\n",
        "print(alternative_text)\n",
        "print()\n",
        "\n",
        "# 5. Collect all phone numbers\n",
        "display(re.findall(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', alternative_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCqKTyBonEZi"
      },
      "source": [
        "### 2.6.2 Checking a Word's *Neighborhood* in Text\n",
        "\n",
        "It might be more informative to also examine the words around a word of interest, that is, the context.\n",
        "\n",
        "***Why?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZnBUL3kWlqY"
      },
      "outputs": [],
      "source": [
        "# Find the word(s) preceding and following a BRAND in a product description\n",
        "\n",
        "# 1. Define the pattern (include forward and backward looking elements)\n",
        "pattern= r'((?:\\S+\\s+){0,3}\\bShimano\\b\\s*(?:\\S+\\b\\s*){0,3})'\n",
        "\n",
        "# 2. Find your brand with the words preceding and following it\n",
        "match = re.findall(pattern, general_description, flags=re.IGNORECASE)\n",
        "display(match)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLc9rYP_d5aq"
      },
      "source": [
        "# 3 Tokenization: An higher-level way to split Text into Words\n",
        "\n",
        "- Turning a string or document into **tokens** (smaller chunks)\n",
        "- An early step in preparing a text for Text Analytics and NLP\n",
        "- Many different theories and rules what these text chunks should look like\n",
        "\n",
        "**Great News! Today, you already learned how to create your own rules using regular expressions!**\n",
        "\n",
        "*Some examples of what tokenization normally does:*\n",
        "\n",
        "- Breaking out words or sentences\n",
        "- Separating punctuation\n",
        "- Separating all hashtags in a tweet\n",
        "\n",
        "--> Many analysts use the Python library called **nltk** (natural language toolkit) for tokenization.\n",
        "\n",
        "***Let's use nltk to tokenize some text!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQz9LXI6enV3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qEuLS0ud5aq"
      },
      "outputs": [],
      "source": [
        "# 1. Import the Tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 2. And start tokenizing right out of the box!\n",
        "print(word_tokenize(\"What is RedNote, the Chinese social media app that US TikTokers are flocking to?\")) # https://edition.cnn.com/2025/01/14/tech/rednote-china-popularity-us-tiktok-ban-intl-hnk/index.html\n",
        "print(word_tokenize(\"How should we test AI for human-level intelligence? OpenAI's o3 electrifies quest\")) # https://www.nature.com/articles/d41586-025-00110-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPrfrfbEd5ar"
      },
      "source": [
        "## 3.1 Why Tokenize?\n",
        "\n",
        "Helps us with simple text processing tasks:\n",
        "\n",
        "- Easier to map part of speech\n",
        "- Matching common words\n",
        "- Removing unwanted tokens (e.g., common words, repeated words, etc.)\n",
        "\n",
        "Example text: \"I don't like Dr. D's bowtie!\"  \n",
        "Tokenized: ['I', 'do', \"n't\", 'like', 'Dr.', 'D', \"'s\", 'bowtie', '!']  \n",
        "\n",
        "***What can we learn?***\n",
        "- negation from \"n't\"\n",
        "- possession from \"'s\"\n",
        "\n",
        "***Tokenization can give us a first hint at the meaning of the text***\n",
        "\n",
        "***Good News!*** You have several powerful tokenizers installed on your system with nltk\n",
        "\n",
        "- **sent_tokenize**: tokenize a document into sentences\n",
        "- **regexp_tokenize**: tokenize a string or document based on a regular expression pattern\n",
        "- **tweetTokenizer**: special class just for tweet tokenization, allowing you to separate hashtags, mentions, and lots of exclamation points!!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vSmVcDbd5ar"
      },
      "outputs": [],
      "source": [
        "# 1. Import necessary modules\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 2. load the file and display text\n",
        "general_description = open(\"simple_product_text.txt\").read()\n",
        "display(general_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg-3R_IcbQH0"
      },
      "outputs": [],
      "source": [
        "# 3. Split general_descriptiongeneral_description into sentences:\n",
        "sentences = sent_tokenize(general_description)\n",
        "display(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp8Tr47Bd5as"
      },
      "outputs": [],
      "source": [
        "# 4. Let's use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[3])\n",
        "display(tokenized_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrXfKWRYd5au"
      },
      "outputs": [],
      "source": [
        "# 6. Finally, let's generate a set of unique tokens in the entire general_description (set is practical because mathematically, every element of a set is unique!)\n",
        "unique_tokens = set(word_tokenize(general_description))\n",
        "display(unique_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5j0E7hgd5az"
      },
      "source": [
        "# 4 Tokenizing Social Media: UGC from X(Twitter)\n",
        "\n",
        "\n",
        "![Tokenizing Social Media ](https://www.mapxp.app/BUSI488/tokenizesocialmedia.jpg \"Tokenizing Social Media \")\n",
        "\n",
        "Social Media is a frequently used source consumer and business insights.  \n",
        "However, social media posts present some challenges:\n",
        "- Many acronyms (lol, IMHO, BTW)\n",
        "- Very short text\n",
        "- Little punctuation\n",
        "- Emojis ðŸ¤“\n",
        "- Hashtags (#)\n",
        "- Mentions (@)\n",
        "\n",
        "Let's build a more complex tokenizer for posts with hashtags and mentions using nltk and regex.  \n",
        "\n",
        "The nltk.tokenize.TweetTokenizer class provides us with some extra methods and attributes for parsing tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRE0-R4Nd5az"
      },
      "outputs": [],
      "source": [
        "# Below are 4 tweets that we will analyze\n",
        "\n",
        "tweets = ['This is the best course @KFBS ever! #AI #datascience ðŸ¤“',\n",
        "         'FBâ€™s stock finally crashed, drive by three things: competition (#TikTok), loss of access to data (#Apple ATT), and huge spending on virtual reality. Hard to see any of these issues going away soon. I went on @CNBC @SquawkStreet to talk about it.',\n",
        "         '#NLP is SUPER cool <3 bc it helps us understand the World better :) #learning',\n",
        "         'Thanks @KFBS for updating your values to #integrity, #inclusion, #impact and #innovation']\n",
        "\n",
        "# What do you notice about these tweets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDPXhmXkqV2l"
      },
      "source": [
        "## 4.1 Extracting Hashtags from Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crSkn7Nyd5az"
      },
      "outputs": [],
      "source": [
        "# 1. Start by importing the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "\n",
        "# 2. Define a regex pattern to find hashtags: pattern1\n",
        "pattern = r'#\\w+'\n",
        "\n",
        "# 3. Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern)\n",
        "display(hashtags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKvMCi__qczU"
      },
      "source": [
        "## 4.2 Extracting Mentions from Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXMwziRwd5az"
      },
      "outputs": [],
      "source": [
        "# 1. Write a pattern that matches mentions (@)\n",
        "pattern = r'([@]\\w+)'\n",
        "\n",
        "# 2. Use the pattern on the second tweet in the tweets list\n",
        "mentions = regexp_tokenize(tweets[1], pattern)\n",
        "display(mentions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trDfZliWqsBs"
      },
      "source": [
        "## 4.3 Extracting Hashtags and Mentions from Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YazgoVuqs0m"
      },
      "outputs": [],
      "source": [
        "# 1. Write a pattern that matches mentions (@)\n",
        "pattern = r'([@#]\\w+)'\n",
        "\n",
        "# 2. Use the pattern on the second tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[1], pattern)\n",
        "display(mentions_hashtags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FCdB5HBd5a0"
      },
      "source": [
        "## 4.4 The TweetTokenizer\n",
        "\n",
        "Doing everything manually is only fun for some time ...\n",
        "\n",
        "There are easier options for Tweets!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxmMCBJ3d5a0"
      },
      "outputs": [],
      "source": [
        "# Use the TweetTokenizer to tokenize all tweets into one list\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# 1. Instantiate the model (i.e., the tweet tokenizer)\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "# 2. Feed all tweets into it, tweet by tweet to get the tokens for each tweet in a list of lists\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "\n",
        "# 3. Show list of lists that contain the tokens of each tweet\n",
        "display(all_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MscacZ68d5a3"
      },
      "source": [
        "# 5 Bag-of-Words (BoW)\n",
        "\n",
        "![ Bag-of-Words](https://www.mapxp.app/BUSI488/bagofwords.jpg \" Bag-of-Words\")\n",
        "\n",
        "\n",
        "***Basic method for finding topics in a text***\n",
        "## - Need to first create tokens using tokenization\n",
        "- Then count up all the tokens\n",
        "- The more frequent a word, the more important it might be\n",
        "- Can be a great way to determine the significant words in a text\n",
        "\n",
        "**Example Text:**  \n",
        "\n",
        "\n",
        "> *The ceiling is the roof! Beat DUKE! Carolina beats Duke University in terms of NCAA titles. CAROLINA and DUKE are Rivals forever! And we'll continue beating them. Carolina rules the UNIVERSE!*\n",
        "\n",
        "\n",
        "***Let's create a Bag of Words (BoW)!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWflpHVJd5a4"
      },
      "outputs": [],
      "source": [
        "# 1. Start by importing the necessary modules\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# 2. Define your text and tokenize it\n",
        "my_text = \"The ceiling is the roof! Beat DUKE! Carolina beats Duke University in terms of NCAA titles. CAROLINA and DUKE are Rivals forever! And we'll continue beating them. Carolina rules the UNIVERSE!\"\n",
        "BoW = word_tokenize(my_text)\n",
        "\n",
        "# 3. User counter to find the word frequency - creates a dict\n",
        "token_counts = Counter(BoW)\n",
        "display(token_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrtjfK6wd5a4"
      },
      "source": [
        "## 5.1 Get Ready to Preprocess Text\n",
        "\n",
        "**Challenges in Text Analysis**\n",
        "- There is a lot of variation in the way people write text\n",
        "- Not every character and/or string in a text is relevant to us\n",
        "- Sometimes several words essentially mean the same thing\n",
        "- Sometimes several words look like they mean the same thing, but actually don't\n",
        "\n",
        "*By preprocessing text, we can start to overcome these challenges!*\n",
        "\n",
        "**Preprocessing of text can include many steps such as:**\n",
        "\n",
        "- Tokenization to create a bag of words\n",
        "- Lowercasing words\n",
        "- Shorten words to their root stems\n",
        "- Lemmatization\n",
        "- Removing stop words, punctuation, and/or unwanted tokens\n",
        "\n",
        "***Good to experiment with different approaches!***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUy01sHbd5af"
      },
      "outputs": [],
      "source": [
        "# 0. Please run pip install on your computer to install the nltk library (natural language toolkit)\n",
        "# It is already installed on CoLab!\n",
        "# !pip install nltk\n",
        "\n",
        "# 1. Now import it and download:\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 2. Alternatively, download everything! But that takes a moment and we won't need it.\n",
        "#nltk.download() # type when asked what to do: \"d all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NunOZnOm07xU"
      },
      "source": [
        "## 5.2 Lower-case for easier processing\n",
        "\n",
        "Upper and lower-case words are tokenized as different words, even when they are the exact same word.\n",
        "- Sometimes desirable (e.g., sentiment analysis)\n",
        "- Often not desirable\n",
        "\n",
        "***Let's fix it!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36KYNnE5d5a4"
      },
      "outputs": [],
      "source": [
        "# 1. Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in BoW]\n",
        "\n",
        "# 2. Show frequencies of lower case tokens\n",
        "token_counts = Counter(lower_tokens)\n",
        "display(token_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7frOWZgdd5a4"
      },
      "outputs": [],
      "source": [
        "# 3. We can output the three most common tokes to make things easier to read:\n",
        "token_counts.most_common(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvrzE7fa9ugL"
      },
      "source": [
        "## 5.3 Stopwords\n",
        "\n",
        "Text usually contains a lot of stopwords that are not necessarily meaningful to our analysis:\n",
        "- A stop word is a commonly used word (such as â€œtheâ€, â€œaâ€, â€œanâ€, â€œinâ€)\n",
        "- What exactly these words are depends on the analyst (or the NLP software they use)\n",
        "- We can easily remove stopwords from our text using the nltk library\n",
        "\n",
        "***Let's remove some stopwords!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWNghk-Td5a5"
      },
      "outputs": [],
      "source": [
        "# Let's remove some stopwords from our text\n",
        "\n",
        "# 1. Import required packages\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 2. Remove stopwords\n",
        "no_stops = [t for t in lower_tokens\n",
        "            if t not in stopwords.words('english')]\n",
        "\n",
        "# 3. Let's take a look at our text without stopwords\n",
        "display(no_stops)\n",
        "\n",
        "# 4. Let's count the three most common tokens again\n",
        "display(Counter(no_stops).most_common(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBGdML4g9_pK"
      },
      "source": [
        "## 5.4 Removing Punctuation\n",
        "\n",
        "- We can make our analysis even simpler by removing punctuation form our text.   \n",
        "- A straight-forward approach is to remove all words that are not alphabetic\n",
        "\n",
        "***Let's drop the punctuation!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbqi9CoP-Di6"
      },
      "outputs": [],
      "source": [
        "# 1. Create tokens\n",
        "no_punct = [w for w in no_stops\n",
        "        if w.isalpha()]  #returns \"true\" if string only includes alphabetical strings\n",
        "\n",
        "# 2. Let's take a look at our text without punctuation\n",
        "display(no_punct)\n",
        "\n",
        "# 3. Let's count the three most common tokens again\n",
        "display(Counter(no_punct).most_common(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKE3xV7R-NK0"
      },
      "source": [
        "## 5.5 Stemming\n",
        "\n",
        "Notice how the words \"beat\", \"beats\" and \"beating\" are each counted only once although they essentially mean the same thing.   \n",
        "\n",
        "\n",
        "- Languages we speak and write are made up of several words often derived from one another.\n",
        "- When a language contains words that are derived from another word as their use in the speech changes is called **Inflected Language**.  \n",
        "\n",
        "\n",
        "- A **stem** (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as:  \n",
        "  - -ed, -ize, -s   \n",
        "  - de-, mis-\n",
        "- Stems are created by removing the suffixes or prefixes used with a word.\n",
        "- So stemming a word or sentence may result in words that are not actual words.\n",
        "\n",
        "\n",
        "#### NLTK helps us stem words with so-called ***Stemmers***\n",
        "- **PorterStemmer**: oldest (1979), uses *Suffix Stripping*\n",
        "- **LancasterStemmer**: newer (1990), more aggressive, iterative, can \"over-stem\" more easily  \n",
        "\n",
        "***Beware!*** Over-stemming can give you word stems that have no interpretable meaning!\n",
        "\n",
        "![Stemming](https://www.mapxp.app/MBA742/stemming.jpg \"Stemming\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8kWMZTS-Q8i"
      },
      "outputs": [],
      "source": [
        "# Stemming Example\n",
        "\n",
        "# 1. Import stemming module\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# 2. Instantiate the stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# 3. Let's do some stemming!\n",
        "for w in ['Consultant', 'Consulting', 'Consultation', 'Consultants', 'Consult']:\n",
        "    print(w, \" : \", ps.stem(w))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDy22H4X-WKy"
      },
      "source": [
        "***Alright, let's clean-up our text about the Carolina and Duke Rivalry!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPT9lBbZ-WS0"
      },
      "outputs": [],
      "source": [
        "# 1. Import stemming module\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# 2. Instantiate the stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# 3. Let's do some stemming!\n",
        "for w in no_punct:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "\n",
        "# 4. Let's count the two most common tokens again\n",
        "stemmed = [ps.stem(w) for w in no_punct]\n",
        "display(Counter(stemmed).most_common(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8V9EtLh-k6c"
      },
      "source": [
        "![Carolina Beats Duke](https://www.mapxp.app/MBA742/carolinabeatsduke.jpg \"Carolina Beats Duke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRj3ldZt-rSr"
      },
      "source": [
        "## 5.6 Lemmatization\n",
        "\n",
        "***University = Universe?***  \n",
        "\n",
        "> *Universal Studio's movie about a University's study in which students studied the universe is universally acclaimed*\n",
        "\n",
        "\n",
        "- Lemmatization is the algorithmic process of finding the lemma of a word ***depending on its meaning***.\n",
        "- It usually refers to the morphological analysis of words, which aims to remove inflectional endings.\n",
        "- It helps in returning the ***base or dictionary form of a word***, which is known as the _lemma_.\n",
        "\n",
        "*A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.*\n",
        "\n",
        "**Why is Lemmatization better than Stemming?**\n",
        "\n",
        "- Stemming algorithms essentially cut the suffix or prefix from a word.\n",
        "- Lemmatization takes into consideration morphological analysis of words.\n",
        "    - Returns the lemma which is the base form of all its inflectional forms.\n",
        "    - In-depth linguistic knowledge is required to create dictionaries and look for the proper form of the word.\n",
        "- Stemming can be thought of a more general operation, while lemmatization is an intelligent operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8WvOxoh-4JC"
      },
      "outputs": [],
      "source": [
        "# Lemmatization Example\n",
        "\n",
        "# 1. Import lemmatization module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 2. Instantiate the lemmatizer\n",
        "le = WordNetLemmatizer()\n",
        "\n",
        "# 3. Let's do some Lemmatization!\n",
        "print(\"Lemma:\")\n",
        "for w in ['University', 'Universe', 'Universal']:\n",
        "    print(w, \" : \", le.lemmatize(w))\n",
        "\n",
        "# 4. vs. Stemming\n",
        "print(\"\\nStem:\")\n",
        "for w in ['University', 'Universe', 'Universal']:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxABO7Yl_mXk"
      },
      "source": [
        "## 6 NaÃ¯ve Topic Discovery\n",
        "\n",
        "By studying the frequency of relevant words in a text, we can potentially learn what the text is about.\n",
        "\n",
        "--> We might identify the central ***Topic(s)*** of a text without having to read the text itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgdYwgio_3Ek"
      },
      "source": [
        "## 6.1 Topic Discovery in Tesla's 2020 10-K\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "The COVID-19 pandemic impacted our business and financial results in 2020. The temporary suspension of production at our factories during the first half of 2020 caused production limitations that, together with reduced or closed government and third party partner operations in the year, negatively impacted our deliveries and deployments in 2020. While we resumed operations at all of our factories worldwide, our temporary suspension at our factories resulted in idle capacity charges as we still incurred fixed costs such as depreciation, certain payroll related expenses and property taxes. As part of our response strategy to the business disruptions and uncertainty around macroeconomic conditions caused by the COVID-19 pandemic, we instituted cost reduction initiatives across our business globally to be commensurate to the scope of our operations while they were scaled back in the first half of 2020. This included temporary labor cost reduction measures such as employee furloughs and compensation reductions. Additionally, we suspended non-critical operating spend and opportunistically renegotiated supplier and vendor arrangements. As part of various governmental responses to the pandemic granted to companies globally, we received certain payroll related benefits which helped to reduce the impact of the COVID-19 pandemic on our financial results. Such payroll related benefits related to our direct headcount have been primarily netted against our disclosed idle capacity charges and they marginally reduced our operating expenses. The impact of the idle capacity charges incurred during the first half of 2020 were almost entirely offset by our cost savings initiatives and payroll related benefits.\n",
        "\n",
        "---\n",
        "***Your new Text Analysis skills might help!***\n",
        "\n",
        "\n",
        "*Source* https://www.sec.gov/Archives/edgar/data/1318605/000156459021004599/tsla-10k_20201231.htm#ITEM_7A_QUANTITATIVE_QUALITATIVE_DISCLOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zTGZ8rf1KD1"
      },
      "outputs": [],
      "source": [
        "# 0. Define our text\n",
        "tesla10k2020 = \"The COVID-19 pandemic impacted our business and financial results in 2020. The temporary suspension of production at our factories during the first half of 2020 caused production limitations that, together with reduced or closed government and third party partner operations in the year, negatively impacted our deliveries and deployments in 2020. While we resumed operations at all of our factories worldwide, our temporary suspension at our factories resulted in idle capacity charges as we still incurred fixed costs such as depreciation, certain payroll related expenses and property taxes. As part of our response strategy to the business disruptions and uncertainty around macroeconomic conditions caused by the COVID-19 pandemic, we instituted cost reduction initiatives across our business globally to be commensurate to the scope of our operations while they were scaled back in the first half of 2020. This included temporary labor cost reduction measures such as employee furloughs and compensation reductions. Additionally, we suspended non-critical operating spend and opportunistically renegotiated supplier and vendor arrangements. As part of various governmental responses to the pandemic granted to companies globally, we received certain payroll related benefits which helped to reduce the impact of the COVID-19 pandemic on our financial results. Such payroll related benefits related to our direct headcount have been primarily netted against our disclosed idle capacity charges and they marginally reduced our operating expenses. The impact of the idle capacity charges incurred during the first half of 2020 were almost entirely offset by our cost savings initiatives and payroll related benefits.\"\n",
        "\n",
        "# 1. Start by importing the necessary modules\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 2. Tokenize\n",
        "tokens = word_tokenize(tesla10k2020)\n",
        "\n",
        "# 3. Lower-case\n",
        "tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# 4. Remove Stopwords\n",
        "tokens = [t for t in tokens\n",
        "            if t not in stopwords.words('english')]\n",
        "# 5. Remove Punctuation\n",
        "tokens = [w for w in tokens\n",
        "        if w.isalpha()]\n",
        "# 6. Lemmatize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "le = WordNetLemmatizer()\n",
        "lemmatized = [le.lemmatize(w) for w in tokens]\n",
        "\n",
        "# 7. Show word frequencies\n",
        "display(Counter(lemmatized).most_common(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVO4duZoHAs-"
      },
      "source": [
        "![TeslaCovid](https://www.cnet.com/a/img/HfoRBtrthWpuoQhxIaFaMudCl-c=/1200x675/2020/11/13/c526147f-b605-49a1-93a9-3e7aedd36b2b/ogi-musk.jpg \"Tlsa Covid Response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI2q7oCSm7rG"
      },
      "source": [
        "# 6.2 Word Clouds\n",
        "\n",
        "- Also known as **Tag Clouds**\n",
        "- Visual representation of text data\n",
        "- Typically used to visualize:\n",
        "    - keyword metadata (tags) on websites\n",
        "    - free form text.\n",
        "- Tags are usually single words\n",
        "    - importance of each tag is shown with:\n",
        "        - font size\n",
        "        - or color\n",
        "- Useful for quickly perceiving the most prominent terms to determine its relative prominence.\n",
        "\n",
        "*Source: Wikipedia*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHraZt4em7rH"
      },
      "source": [
        "### 6.2.1 Word Clouds from Text\n",
        "\n",
        "> Some folks love Word Clouds.\n",
        "\n",
        "> Personally, I don't.\n",
        "\n",
        "> Nontheless, I'll show how to easily generate them.   \n",
        "\n",
        "**Conveniently, the python module we will use does the preprocessing for us!**   \n",
        "All we have to to is pass a text (as a string) to it.\n",
        "\n",
        "\n",
        "for more details see: https://www.datacamp.com/community/tutorials/wordcloud-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVkP02mNm7rH"
      },
      "outputs": [],
      "source": [
        "# Install WordCloud on your local computer (already installed on CoLab)\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbFrfUW2m7rH"
      },
      "outputs": [],
      "source": [
        "# 1. Import modules\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2. Generate Word Cloud\n",
        "wordcloud = WordCloud(collocations=True, width=800, height=500, random_state=5, max_font_size=110).generate(tesla10k2020)\n",
        "\n",
        "# 3. Visualize Cloud\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3HeD2IfDqTJ"
      },
      "source": [
        "# 7 Sentiment Analysis\n",
        "\n",
        "![Sentiment Analysis](https://blog.reputationx.com/hubfs/what-is-sentiment-analysis-cover.jpg?auto=webp&quality=95&crop=16:9&width=675 \"https://blog.reputationx.com/whats-sentiment-analysis\")\n",
        "\n",
        "**Sentiment Analysis**, or **Opinion Mining**, is a sub-field of Natural Language Processing (NLP)\n",
        "- Tries to identify and extract opinions within a given text.\n",
        "\n",
        "**Definition of Sentiment**\n",
        "1. A view of or attitude toward a situation or event; an opinion.  \n",
        "\"I agree with your sentiments regarding the road bridge\"  \n",
        "\n",
        "\n",
        "2. Exaggerated and self-indulgent feelings of tenderness, sadness, or nostalgia.  \n",
        "\"many of the appeals rely on treacly sentiment\"\n",
        "\n",
        "*Source: Oxford Dictionary*\n",
        "\n",
        "**In this course, we will measure the polarity of text.**\n",
        "\n",
        "**Polarity** in sentiment analysis refers to identifying sentiment orientation:\n",
        "- positive\n",
        "- neutral\n",
        "- negative   \n",
        "\n",
        "in written or spoken language\n",
        "\n",
        "\n",
        "**Objetive of Sentiment Analysis**\n",
        "Gauge the attitude, sentiments, evaluations, attitudes and emotions of a speaker/writer based on the computational treatment of subjectivity in a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4p_OCAtFJAs"
      },
      "source": [
        "##7.1 Importance of Sentiment Analysis\n",
        "\n",
        "- Enables companies to make sense out of unstructured data such as UGC (user generated content), news, or financial reports.\n",
        "- Automated way to extract insights about perceptions, experiences, or positions toward something.\n",
        "\n",
        "### **Sentiment Analysis for Businesses and Organizations**\n",
        "\n",
        ">**Financial Market Forecasting:** Analyzing news articles and social media posts related to financial markets. Positive sentiment may suggest bullish trends, while negative sentiment may indicate bearish trends.\n",
        "\n",
        ">**Brand Reputation and Crisis Management:** Monitoring social media and news articles. Real-time sentiment analysis allows businesses to respond quickly, address issues, and manage their brand reputation effectively.\n",
        "\n",
        ">**Political Campaigns and Elections:** Analyzing social media discussions and news articles during political campaigns. Fine-tune campaign strategies and messaging to resonate with voters.\n",
        "\n",
        ">**Employee Engagement and HR Management:** Analyzing employee feedback and surveys. Improve workplace culture and employee retention.\n",
        "\n",
        ">**Strategic Planning**:Analyzing industry news and competitor mentions.Positive sentiment for competitors may suggest they are gaining market share, while negative sentiment can help identify vulnerabilities in the competition, leading to informed strategic decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p1-5oLdFw5F"
      },
      "source": [
        "## 7.2 Advantage of Sentiment Analysis\n",
        "- Sifting through huge volumes of text is **difficult** and **time-consuming**\n",
        "- Requires **expertise** and **resources**\n",
        "\n",
        "Sentiment Analysis:\n",
        "- Enables firms to make sense out large amounts of textual: ***In an automated way***\n",
        "- Allows firms to elicit vital insights from a vast unstructured dataset without having to manually process it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2smeopmF1CU"
      },
      "source": [
        "## 7.3 Limitations of Sentiment Analysis\n",
        "\n",
        "\n",
        "1. Understanding emotions through text are not always easy:\n",
        "    - 100% accuracy from a computer is not rational\n",
        "    - A text may contain ***multiple*** sentiments (polarities) all at once\n",
        "        - *â€œThe coffee was great, but the service could have been betterâ€.*\n",
        "    - ***Figurative Speech*** is difficult for machines to understand\n",
        "        - *â€œThat coffee tastes very interesting\"*  \n",
        "        \n",
        "\n",
        "2. Micro-blogging content from social media platforms such as X(Twitter) and Facebook poses serious challenges:  \n",
        "    - large amount of data\n",
        "    - language and expressions used to express sentiment\n",
        "        - short forms\n",
        "        - memes\n",
        "        - emoticons\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYXjr1ZmY53L"
      },
      "source": [
        "## 7.4 Sentiment Scoring - Polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsLN6EMnm7q-"
      },
      "outputs": [],
      "source": [
        "# 0. Run once to install the Vader Sentiment Classification Package (if it is not already installed on your computer)\n",
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIKBOE7pm7q-"
      },
      "outputs": [],
      "source": [
        "# 1. Import the module you need\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# 2. Instantiate the sentiment analyzer\n",
        "analyser = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkRJ2qntm7q_"
      },
      "outputs": [],
      "source": [
        "# 3. Define a function that returns the polarity score of a sentence\n",
        "def sentiment_analyzer_scores(sentence):\n",
        "    score = analyser.polarity_scores(sentence)\n",
        "    print(\"{:-<55} {}\".format(sentence, str(score)), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXrIq5Rrm7q_"
      },
      "outputs": [],
      "source": [
        "# 4. Test how well Vader does\n",
        "my_tweet = \"UNC is not the best place to study data science for business!\"\n",
        "sentiment_analyzer_scores(my_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iejtlpafx9L_"
      },
      "outputs": [],
      "source": [
        "# 5. Print the individual polarity scores\n",
        "sentiment_dict = analyser.polarity_scores(my_tweet)\n",
        "print(\"Compound sentiment is\", sentiment_dict['compound'], \"\\n\")\n",
        "print(\"Sentence was rated as\", sentiment_dict['neg']*100, \"% Negative\")\n",
        "print(\"Sentence was rated as\", sentiment_dict['neu']*100, \"% Neutral\")\n",
        "print(\"Sentence was rated as\", sentiment_dict['pos']*100, \"% Positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cl4fpmjm7q_"
      },
      "source": [
        "### 7.4.1 Interpreting Polarity\n",
        "\n",
        "- The ***Positive***, ***Negative*** and ***Neutral scores*** represent the proportion of text that falls in these categories.   \n",
        "\n",
        "\n",
        "- This means our sentence was rated as 46.0% Positive, 54.0% Neutral and 0% Negative.\n",
        "    - should add up to 1   \n",
        "    \n",
        "\n",
        "- The ***Compound score*** is a metric that calculates the sum of all the lexicon ratings\n",
        "    - which have been normalized between:\n",
        "        - -1 (most extreme negative) and\n",
        "        - +1 (most extreme positive)\n",
        "    - the ranges of the compound scrore are:\n",
        "        - positive sentiment: compound score >= 0.05\n",
        "        - neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
        "        - negative sentiment: compound score <= -0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPwjiVgQm7rA"
      },
      "source": [
        "***DIY*** Try editing my_text:\n",
        "- writing \"good\" in all caps\n",
        "- add exclamation marks\n",
        "- add an emoji (e.g., ðŸ˜Š)\n",
        "- add adjectives or adverbs (e.g., degree modifiers)\n",
        "- use synonyms for words\n",
        "- add a conjunction (e.g., \"but\") to signal a shift in sentiment\n",
        "- negate the sentence with a tri-gram (e.g., \"my coffee isn't really all that great\")\n",
        "- turn it into a negative statement about the Corona virus\n",
        "- use some slang (e.g., \"that virus really SUX!\")  \n",
        "\n",
        "***What do you observe?*** *Paste your example text and the compound score into the Zoom chat window*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk7JZbf6m7rA"
      },
      "source": [
        "### 7.4.2 Where do Vader's scores come from?\n",
        "\n",
        "***The authors of Vader built a rule-based sentiment analysis engine that uses a dictionary to classify sentiment***\n",
        "\n",
        "**To build their dictionary, they did the following:**\n",
        "\n",
        "- Sentiment ratings from 10 independent human raters (all pre-screened, trained, and quality checked for optimal inter-rater reliability).\n",
        "- Over 9,000 token features were rated on a scale from \"[â€“4] Extremely Negative\" to \"[4] Extremely Positive\", with allowance for \"[0] Neutral (or Neither, N/A)\".\n",
        "- We kept every lexical feature that had a non-zero mean rating, and whose standard deviation was less than 2.5 as determined by the aggregate of those ten independent raters.\n",
        "- This left us with just over 7,500 lexical features with validated valence scores that indicated both the sentiment polarity (positive/negative), and the sentiment intensity on a scale from â€“4 to +4.\n",
        "- For example:\n",
        "    - the word \"okay\" has a positive valence of 0.9, \"good\" is 1.9, and \"great\" is 3.1\n",
        "    - whereas \"horrible\" is â€“2.5, the frowning emoticon :( is â€“2.2, and \"sucks\" and it's slang derivative \"sux\" are both â€“1.5.\n",
        "\n",
        "-----------------\n",
        "\n",
        "***Excerpt from Vader's sentiment lexicon (dictionary) that can be found in the file \"vader_lexicon.txt\"***\n",
        "\n",
        "| Word          | Polarity | Intensity | Ratings of 10 humans                     |\n",
        "|---------------|----------|-----------|------------------------------------------|\n",
        "| brightly      | 1.5      | 0.67082   | [2, 3, 1, 2, 1, 1, 2, 1, 1, 1]           |\n",
        "| brightness    | 1.6      | 0.91652   | [2, 2, 1, 1, 1, 3, 3, 0, 2, 1]           |\n",
        "| brightnesses  | 1.4      | 0.91652   | [2, 3, 1, 2, 1, 1, 0, 0, 2, 2]           |\n",
        "| brights       | 0.4      | 0.66332   | [0, 0, 2, 0, 0, 1, 0, 0, 1, 0]           |\n",
        "| brightwork    | 1.1      | 0.83066   | [1, 0, 1, 2, 1, 0, 3, 1, 1, 1]           |\n",
        "| brilliance    | 2.9      | 0.83066   | [4, 3, 2, 4, 4, 3, 2, 3, 2, 2]           |\n",
        "| brilliances   | 2.9      | 0.83066   | [3, 4, 3, 4, 4, 2, 3, 2, 2, 2]           |\n",
        "| brilliancies  | 2.3      | 1.18743   | [1, 4, 1, 3, 3, 2, 1, 3, 4, 1]           |\n",
        "| brilliancy    | 2.6      | 1.0198    | [4, 3, 2, 4, 2, 3, 1, 3, 1, 3]           |\n",
        "| brilliant     | 2.8      | 0.6       | [2, 3, 3, 2, 3, 3, 4, 2, 3, 3]           |\n",
        "| brilliantine  | 0.8      | 1.16619   | [-1, 3, 1, 0, 1, 0, 2, 0, 2, 0]          |\n",
        "| brilliantines | 2        | 1.34164   | [0, 1, 4, 2, 3, 1, 3, 0, 3, 3]           |\n",
        "| brilliantly   | 3        | 0.44721   | [3, 2, 3, 3, 3, 3, 3, 3, 4, 3]           |\n",
        "| brilliants    | 1.9      | 0.83066   | [3, 1, 2, 1, 2, 1, 3, 2, 1, 3]           |\n",
        "| brisk         | 0.6      | 0.8       | [0, 0, 0, 0, 1, 1, 0, 2, 0, 2]           |\n",
        "| broke         | -1.8     | 0.4       | [-2, -2, -2, -2, -1, -2, -2, -1, -2, -2] |\n",
        "| broken        | -2.1     | 0.53852   | [-2, -2, -2, -2, -3, -2, -1, -3, -2, -2] |\n",
        "| brooding      | 0.1      | 1.3       | [3, 0, -1, -1, -1, 1, 1, -1, 1, -1]      |\n",
        "| brutal        | -3.1     | 0.7       | [-3, -3, -4, -2, -3, -4, -3, -4, -3, -2] |\n",
        "| brutalise     | -2.7     | 1.1       | [-4, -3, -3, -4, -3, -2, -2, -3, 0, -3]  |\n",
        "| brutalised    | -2.9     | 0.83066   | [-3, -3, -2, -3, -3, -4, -4, -1, -3, -3] |\n",
        "| brutalises    | -3.2     | 0.4       | [-3, -3, -3, -3, -3, -4, -4, -3, -3, -3] |\n",
        "| brutalising   | -2.8     | 0.74833   | [-3, -3, -4, -3, -2, -3, -3, -3, -1, -3] |\n",
        "\n",
        "-----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D9dToJ5m7rA"
      },
      "source": [
        "# 8 Sentiment Analysis of Social Media\n",
        "\n",
        "**Today, we will analyze tweets that I collected during the CoVid Pandemic in 2022**\n",
        "\n",
        "Before we can start analyzing the sentiment of the scraped tweets, we need to do some data cleaning and preprocessing\n",
        "- remove URLs from tweets\n",
        "- remove # and @ from tweets\n",
        "- remove reserved words (e.g., RT and FAV) from tweets\n",
        "- remove links to images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZviJ2BwZLJf"
      },
      "source": [
        "- We will now look at some real-world data from Twitter that was collected from Twitter's website.\n",
        "- These data (i.e., tweets) were written by real people (whom I have no influence over!)\n",
        "- **Tweets can contain explicit and offensive content:**\n",
        "    - Sexuality\n",
        "    - Inappropriate language\n",
        "    - Racism\n",
        "    - Slurs\n",
        "\n",
        "![Warning Explicit Content](https://mapxp.app/MBA742/pngaaa.com-1860621.png \"Warning\")\n",
        "\n",
        "\n",
        "**DISCLAIMER**\n",
        "\n",
        "- The opinions and statements expressed by the tweets you might scrape and that we use\n",
        "    - **Do not** represent the opinions and choice of words of your instructors, KFBS, UNC or the State of North Carolina\n",
        "    - Are being analyzed solely for educational purposes, i.e., students' development of Data Science skills\n",
        "    \n",
        "**YOUR CHOICE**\n",
        "If you would not like to be exposed to possibly inappropriate, explicit and/or offensive content  \n",
        "***YOU MAY LEAVE THIS SESSION NOW***\n",
        "\n",
        "- Please contact me after today's session so that we can develop an alternative way of conveying the content of today's class to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS4kuWxP25Dk"
      },
      "outputs": [],
      "source": [
        "# 1. Load Tweets and inspect\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', 20)\n",
        "tweets = pd.read_json('coronaUSA2022.json', lines=True)\n",
        "tweets.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-nzWkEcm7rA"
      },
      "source": [
        "## 8.1 Remove Undesirable Characters and Strings from Tweets\n",
        "\n",
        "Tweets can contain a range of characters, words, and strings that do not add value to our analysis. These include:\n",
        "\n",
        "- Reserved Words like RT and FAV\n",
        "- URLs\n",
        "- Pictures\n",
        "- Hashtags #\n",
        "- Mentions @\n",
        "- HTML entities, e.g.: \\&amp;\n",
        "\n",
        "*Let's remove them before we proceed with our analysis*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncoJbNS6m7rB",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# 1. Import regular expressions\n",
        "import re\n",
        "\n",
        "# 2. Set-up patterns to be removed fro the tweets\n",
        "pat1 = r\"http\\S+\"   # web links\n",
        "pat2 = r\"#\"         # hashtags\n",
        "pat3 = r\"@\"         # mentions\n",
        "pat4 = r\"FAV\"       # twitter reserved abbreviation\n",
        "pat5 = r\"RE\"        # twitter reserved abbreviation\n",
        "pat6 = r\"pic.\\S+\"   # twitter links to images\n",
        "pat7 = r\"\\n\"        # line breaks\n",
        "pat8 = '\\r\\n'       # line breaks\n",
        "pat9 = r'|'.join((r'&amp;',r'&copy;',r'&reg;',r'&quot;',r'&gt;',r'&lt;',r'&nbsp;',r'&apos;',r'&cent;',r'&euro;',r'&pound;'))  # HTML tags\n",
        "\n",
        "# 3. Combine all patterns\n",
        "combined_pat = r'|'.join((pat1, pat2, pat3, pat4, pat5, pat6, pat7, pat8, pat9))\n",
        "\n",
        "# 4. Replace the patterns with an empty string\n",
        "tweets['stripped'] =  [re.sub(combined_pat, '', w) for w in tweets.content]\n",
        "\n",
        "# 5. might have double spaces now (because of empty string replacements above) - remove double empty spaces\n",
        "tweets['stripped'] = tweets.stripped.replace({' +':' '},regex=True)\n",
        "\n",
        "# 6. Print some tweets to check if it worked\n",
        "for i in range(0,10):\n",
        "    print(tweets.stripped[i])\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD3EsvHLm7rB"
      },
      "source": [
        "## 8.2 Compound Sentiment Scores for all Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXpFXqnEm7rC"
      },
      "outputs": [],
      "source": [
        "# 1. Import the sentiment module (in case you haven't already done so)\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# 2. Import numpy (in case you have not already done so)\n",
        "import numpy as np\n",
        "\n",
        "# 3. Instantiate the sentiment analyzer (in case you haven't already done so)\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "# 4. Now get the compound sentiment score for each tweet\n",
        "tweets['C_Score'] = np.nan # initialize empty comlumn in our tweets dataframe (empty = missing values)\n",
        "for index, row in tweets.iterrows():  # loop through all tweets (i.e., rows)\n",
        "    tweets.loc[index, 'C_Score'] = analyser.polarity_scores(row['stripped'])['compound']\n",
        "\n",
        "# 5. Let's take a look!\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "tweets[['stripped','C_Score']][5300:5310]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrSyJg4sm7rC"
      },
      "source": [
        "## 8.2.1 Some Basic Sentiment Descriptives\n",
        "Let's get a first impression of the Sentiment across the tweets we scraped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIVtEjyvm7rC"
      },
      "outputs": [],
      "source": [
        "# 1. import necessary modules (in case not already imported)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Count positive tweets: {sum(tweets['C_Score'] > 0.05)}\")\n",
        "print(f\"Count netural tweets: {tweets['C_Score'].between(-0.05, 0.05).sum()}\")\n",
        "print(f\"Count negative tweets: {sum(tweets['C_Score'] < -0.05)}\")\n",
        "print(f\"Total number of tweets: {tweets['C_Score'].count()}\")\n",
        "print()\n",
        "display(tweets.C_Score.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B1IJTLem7rC"
      },
      "source": [
        "## 8.3 X(Twitter) Sentiment EDA\n",
        "\n",
        "Let's explore consumer sentiment some more using data visualization.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN_aC0Ncm7rC"
      },
      "source": [
        "### 8.3.1 Get a Visual Impression of the Sentiment Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJibwWWem7rC"
      },
      "outputs": [],
      "source": [
        "# 1. import necessary modules (in case not already imported)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 2. Settings for seaborn plotting style\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "# 3. Settings for seaborn plot sizes\n",
        "sns.set(rc={'figure.figsize':(5,5)})\n",
        "\n",
        "# 4. Create Histogram\n",
        "ax = sns.histplot(tweets['C_Score'],\n",
        "                  bins=10,\n",
        "                  kde=False,\n",
        "                  color='skyblue')\n",
        "ax.set(xlabel='Sentiment Distribution', ylabel='Frequency')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J8SB6Whm7rC"
      },
      "source": [
        "**Let's simplify our analysis** *by creating a new variable called* ***Sentiment*** that assumes the strings:\n",
        "- **Postive** if the compound sentiment score (C_Score) is greater than 0.05\n",
        "- **Negative** if the compound sentiment score (C_Score) is less than - 0.05\n",
        "- **Neutral** if the compound sentiment score (C_Score) is between -0.05 and 0.05 (including both values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create an empty column with 'object' dtype\n",
        "tweets['Sentiment'] = np.nan\n",
        "tweets['Sentiment'] = tweets['Sentiment'].astype(object) # setting data type\n",
        "\n",
        "# 2. Loop through rows of dataframe and determine strings for new column \"Sentiment\"\n",
        "for index, row in tweets.iterrows():\n",
        "    if tweets.loc[index, 'C_Score'] > 0.05 :\n",
        "            tweets.loc[index, 'Sentiment'] = \"Positive\"\n",
        "    elif tweets.loc[index, 'C_Score'] < -0.05 :\n",
        "            tweets.loc[index, 'Sentiment'] = \"Negative\"\n",
        "    else :\n",
        "        tweets.loc[index, 'Sentiment'] = \"Neutral\"\n",
        "\n",
        "# 3. Typecast as categorical variable (computationally more efficient)\n",
        "tweets['Sentiment'] = tweets['Sentiment'].astype(\"category\")"
      ],
      "metadata": {
        "id": "VTBcgWmw11e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blu9pC-zm7rD"
      },
      "outputs": [],
      "source": [
        "# 4. Check that it worked\n",
        "tweets[['stripped','C_Score', 'Sentiment']][5300:5310]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzxtTuWqm7rD"
      },
      "source": [
        "### 8.3.2 Visualize the Sentiment Category Shares in a Donut Chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGMnBDjfm7rD"
      },
      "outputs": [],
      "source": [
        "# 1. Import necessary modules (in case not already imported)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 2. Set font size\n",
        "plt.rcParams['font.size']=24\n",
        "\n",
        "# 3. Define figure\n",
        "fig, ax = plt.subplots(figsize=(9, 6), subplot_kw=dict(aspect=\"equal\"))\n",
        "\n",
        "# 4. Get count by sentiment category from tweets_df\n",
        "sentiment_counts = tweets.Sentiment.value_counts()\n",
        "labels = sentiment_counts.index\n",
        "\n",
        "# 5. Define colors\n",
        "color_palette_list = ['lightgreen', 'lightblue', 'red','orange']\n",
        "\n",
        "# 6. Generate graph components\n",
        "wedges, texts, autotexts = ax.pie(sentiment_counts, wedgeprops=dict(width=0.5), startangle=-40,\n",
        "       colors=color_palette_list[0:3], autopct='%1.0f%%', pctdistance=.75, textprops={'color':\"w\", 'weight':'bold'})\n",
        "\n",
        "# 7. Plot wedges\n",
        "for i, p in enumerate(wedges):\n",
        "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
        "    y = np.sin(np.deg2rad(ang))\n",
        "    x = np.cos(np.deg2rad(ang))\n",
        "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
        "    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
        "    ax.annotate(labels[i], xy=(x, y), xytext=(1.2*x, 1.2*y),\n",
        "                horizontalalignment=horizontalalignment)\n",
        "# 8. Set title\n",
        "ax.set_title(\"Sentiment Distribution\", y=.95, fontsize = 24)\n",
        "\n",
        "# 9. Show Doughnut Chart\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMpOr8BI-qwr"
      },
      "source": [
        "### 8.3.3 Sentiment Distribution over Time\n",
        "\n",
        "Did the sentiment towards corona change over the course of a week?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRL4xEh5m7rE"
      },
      "outputs": [],
      "source": [
        "# 1. Import required package\n",
        "import math\n",
        "\n",
        "# 2. New column that holds days (sorted)\n",
        "tweets['day'] = [one.date() for one in tweets['date']]\n",
        "tweets = tweets.sort_values(by=['day'])\n",
        "\n",
        "# 3. Create props (stacked bars) for sentiment grouped by day (as % shares)\n",
        "sentiments = [\"Positive\", \"Neutral\", \"Negative\"]\n",
        "positiveProps = (tweets[tweets.Sentiment == 'Positive'].groupby(['day']).count()[['Sentiment']]/ tweets.groupby(['day']).count()[['Sentiment']])*100\n",
        "neutralProps = (tweets[tweets.Sentiment == 'Neutral'].groupby(['day']).count()[['Sentiment']]/ tweets.groupby(['day']).count()[['Sentiment']])*100\n",
        "negativeProps = (tweets[tweets.Sentiment == 'Negative'].groupby(['day']).count()[['Sentiment']]/ tweets.groupby(['day']).count()[['Sentiment']])*100\n",
        "\n",
        "# 4.Turn props into lists\n",
        "positiveProps = positiveProps['Sentiment'].tolist()\n",
        "neutralProps = neutralProps['Sentiment'].tolist()\n",
        "negativeProps = negativeProps['Sentiment'].tolist()\n",
        "\n",
        "# 5. Set-up plot\n",
        "plt.figure(figsize=[24, 8])\n",
        "barWidth = 0.5\n",
        "labels = tweets.day.unique()\n",
        "r = np.arange(len(labels))\n",
        "\n",
        "# 6. Set values to zero if missing\n",
        "positiveProps = [0 if math.isnan(x) else x for x in positiveProps]\n",
        "neutralProps = [0 if math.isnan(x) else x for x in neutralProps]\n",
        "negativeProps = [0 if math.isnan(x) else x for x in negativeProps]\n",
        "\n",
        "# 7. Define appearance of bar plot\n",
        "plt.bar(r,positiveProps, color='lightgreen', edgecolor='white', width=barWidth)\n",
        "plt.bar(r, neutralProps, bottom=positiveProps, color='skyblue', edgecolor='white', width=barWidth)\n",
        "plt.bar(r, negativeProps, bottom=[i+j for i,j in zip(positiveProps, neutralProps)], color='red', edgecolor='white', width=barWidth)\n",
        "\n",
        "# 8. Additional plot settings and style\n",
        "plt.xticks(r, labels, rotation = 45, fontsize=12)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.suptitle('Sentiment Distribution over Time')\n",
        "plt.xlabel(\"Date\", fontsize=18)\n",
        "plt.ylabel(\"Share\", fontsize=20)\n",
        "plt.legend(sentiments)\n",
        "plt.show()\n",
        "\n",
        "# 9. Sort by Index again to restore orignal order of tweets (since we had grouped and sorted them differently for this part)\n",
        "tweets.sort_index(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlSLeDwKm7rE"
      },
      "source": [
        "## 8.4 Topic Search in Social Media Posts\n",
        "\n",
        "It can be important to understand what topics are discussed on social media. However, discovering topics from thousands of tweets is not a trivial task!\n",
        "\n",
        "- Human Approach: Have people read tweets and determine what the main topics are\n",
        "- Automated Approach: Use Data Science to discover topics\n",
        "    - Search for tweets that contain certain text / words / strings: **Topic Tagging**\n",
        "    - Visualize Word Frequencies: **World Clouds**\n",
        "    - Use **Deep Learning** (coming attractions - stay tuned to this course!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzvWhICTm7rE"
      },
      "source": [
        "### 8.4.1 Topic Tagging\n",
        "\n",
        "1. Define a list of words that are commonly associated with a topic of interest\n",
        "2. Seach for those words in all tweets\n",
        "3. Identify those tweets that contain one or more of the \"topic words\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_pLcnOmm7rE"
      },
      "outputs": [],
      "source": [
        "# 1. Import required modules (in case not already imported)\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# 2. Let's try to identify tweets that are about beer\n",
        "#    You need think for words that would be indicative of beer, that is, that make it likely that the tweet is about corona the beer.\n",
        "tweets['Beer'] = tweets.stripped.str.contains('(?:^|\\W*)(?:beer|beers|drink|party|beach|lime|\"corona extra\")(?:$|\\W*)',\n",
        "    flags = re.IGNORECASE).astype(int)\n",
        "\n",
        "# 3. Let's try to identify tweets that are about the virus\n",
        "#    You need think for words that would be indicative of the corona virus, that is, that make it likely that the tweet is about corona the virus.\n",
        "tweets['Virus'] = tweets.stripped.str.contains('(?:^|\\W*)(?:virus|\"covid-19\"|death|pandemic|mask|hoax|vaccine)(?:$|\\W*)',\n",
        "    flags = re.IGNORECASE).astype(int)\n",
        "\n",
        "# 4. How many tweets of each topic?\n",
        "print(f\"Total {tweets['Beer'].count()}\")\n",
        "print(f\"Beer {tweets['Beer'].sum()}\")\n",
        "print(f\"Virus {tweets['Virus'].sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RSfNN53m7rF"
      },
      "outputs": [],
      "source": [
        "# 5. Make Pandas Columns wider so we can see all the tweet texts easily\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# 6. Show tweets with their repsective topic labels\n",
        "tweets[['stripped','Beer','Virus']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pU4pZXHm7rF"
      },
      "outputs": [],
      "source": [
        "# 7. Show the tweets that are about beer / virus\n",
        "select_tweets = tweets.loc[tweets['Beer'] == 1, 'stripped'].values[:]\n",
        "for w in select_tweets[0:10]:\n",
        "    print(w)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0yNMDQdG8-K"
      },
      "source": [
        "### 8.4.2 Word Clouds from Tweets\n",
        "\n",
        "for more details see: https://www.datacamp.com/community/tutorials/wordcloud-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXgVJ_XIG8-M"
      },
      "outputs": [],
      "source": [
        "# Install WordCloud on your local comouter (already installed on CoLab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdlXagPXHFlI"
      },
      "outputs": [],
      "source": [
        "# 1. Import module\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# 2. Define what we are looking for:\n",
        "#sent = 'Positive'\n",
        "#sent = 'Neutral'\n",
        "sent = 'Negative'\n",
        "\n",
        "clmn = 'Beer'\n",
        "#clmn = 'Virus'\n",
        "\n",
        "# 3. How many Tweets will contribute to Cloud?\n",
        "print(f\"Contributing Tweets {tweets[tweets['Sentiment'] == sent][clmn].sum()}\\n\")\n",
        "\n",
        "\n",
        "# 4. Create bag of words for tweets of certain sentiment\n",
        "all_words = ' '.join([text for text in tweets[(tweets['Sentiment'] == sent) & (tweets[clmn] == 1)]['stripped']])\n",
        "\n",
        "# 5. Generate Word Cloud\n",
        "wordcloud = WordCloud(collocations=True, width=800, height=500, random_state=5, max_font_size=110).generate(all_words)\n",
        "\n",
        "# 6. Visulaize Cloud\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmkmspzVqXbu"
      },
      "source": [
        "# **Looking Ahead:**  \n",
        "\n",
        "#### **Next Class:** Monday, January 26, 2025  \n",
        "\n",
        "#### ***Representation Learning for Recommender Systems***\n",
        "\n",
        "> ### Don't forget the Class Takeaways Survey\n",
        "\n",
        "***HW1***: DataCamp Introduction to Python is ***due Friday, January 24 before midnight***. Must report XP earned on Canvas (no need to upload screenshots)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}