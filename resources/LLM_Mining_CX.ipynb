{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kD1KfLPNXYmw"
   },
   "source": "# Large Language Modelling: Mining CX from Social Media"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNobejvRnTqt"
   },
   "source": [
    "# 1. BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "#### **Let's work with a MLM called BERT!**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Quick Recap\n",
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT1.png \"BERT Explained 1\")"
   ],
   "metadata": {
    "id": "RgxtkjvLYvTJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT2.png \"BERT Explained 2\")"
   ],
   "metadata": {
    "id": "B9AkQTXbYvbd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT4.png \"BERT Explained 4\")"
   ],
   "metadata": {
    "id": "zVSYEnxhYviP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT5.png \"BERT Explained 5\")"
   ],
   "metadata": {
    "id": "zdN1Gh2-cVgz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT6.png \"BERT Explained 6\")"
   ],
   "metadata": {
    "id": "6wqszds8cZUq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT7.png \"BERT Explained 7\")"
   ],
   "metadata": {
    "id": "YHFuHduvYvmt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT8.png \"BERT Explained 8\")"
   ],
   "metadata": {
    "id": "5crjuoApceMv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![From NLP to NLU](https://mapXP.app/MBA742/BERT9.png \"BERT Explained 9\")"
   ],
   "metadata": {
    "id": "_PyKAe2Rcg8W"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DORYrlYdOdAj"
   },
   "source": [
    "## 1.2 Sentence Embedding with SentenceTransformers\n",
    "\n",
    "> SBERT Special version of BERT that was trained for Sentence Similarity\n",
    "\n",
    "![SBERT](https://www.sbert.net/_static/logo.png \"Sentence BERT\")\n",
    "\n",
    "- SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings.\n",
    "  * Check out the website with code examples: https://www.sbert.net/index.html\n",
    "  * Here is the GitHib repository: https://github.com/UKPLab/sentence-transformers  \n",
    "  * Read the research article for details: https://arxiv.org/abs/1908.10084\n",
    "\n",
    "\n",
    "- You can use this framework to compute sentence / text embeddings for more than 100 languages.\n",
    "- Sentence embeddings can be compared with cosine-similarity to find sentences with a similar meaning.\n",
    "  * Semantic search\n",
    "  * Paraphrase mining\n",
    "  * Clustering\n",
    "  * Visualization in maps\n",
    "\n",
    "The framework is based on PyTorch and Transformers\n",
    "   * Large collection of pre-trained models tuned for various tasks.\n",
    "   * Easy to fine-tune your own models.\n",
    "\n",
    "***That's exactly what we need to analyze entire tweets so that we can discover topics on X(Twitter)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5Xs5pc6OdAj"
   },
   "source": [
    "## 1.3 Install Sentence Transformers (SBERT)\n",
    "\n",
    "- You can easily download and install it on CoLab or your own computer using pip install\n",
    "  *\n",
    "```\n",
    "!pip install -U sentence-transformers\n",
    "```\n",
    "\n",
    "- To install SBERT on your Apple Computer with Apple Silicone (M1 and M2 chips), I recommend that you use Conda:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge sentence-transformers\n",
    "```\n",
    "\n",
    "- We will also use **PyTorch**\n",
    " * Open source machine learning framework that accelerates the path from research prototyping to production deployment https://pytorch.org/\n",
    " * **PyTorch is already installed on CoLab (Torch)**\n",
    " * To install PyTorch on your computer, visit https://pytorch.org/get-started/locally/\n",
    "![PyTorch](https://www.mapXP.app/MBA742/Pytorch_logo.jpg \"PyTorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ***GPU Support on CoLab***\n",
    "- The code of this notebook will run on CPUs\n",
    "- To make things faster, we can leverage GPUs\n",
    "- CoLab grants us free access to GPUs\n",
    "  - Click on **\"Runtime\"** in the menubar\n",
    "  - Click on **\"Change Runtime type\"** in the dropdown\n",
    "  - Select **\"GPU\"** as Hardware accelerator\n",
    "  - Click **\"Save\"** button\n"
   ],
   "metadata": {
    "id": "C0Pc05l2rn2s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Install SentenceTransformers (SBERT) if it is not already installed\n",
    "#!pip install -U sentence-transformers\n",
    "#!pip install --upgrade tensorflow"
   ],
   "metadata": {
    "id": "jh9o47CAc6v4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aTFIpTcOdAq"
   },
   "source": [
    "## 1.4 Download a Pre-Trained SBERT Model\n",
    "\n",
    "Now you need to\n",
    "1. import SBERT, and\n",
    "2. download a pre-trained model.\n",
    "  * **Models can be very large, i.e., over 1GB of data!**  \n",
    "  * There are over 4,000 pre-trained models available:\n",
    "    * https://www.sbert.net/docs/pretrained_models.html\n",
    "    * https://huggingface.co/models?library=sentence-transformers&p=3&sort=downloads\n",
    "  * Pre-trained models are for different languages or topics such as\n",
    "    * Patents (PatentSBERT)\n",
    "    * Medical Claims and Fake News (BioBERT)\n",
    "    * English-German Cross-Language (Cross En-De RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IUnD6i1VOdAt"
   },
   "source": [
    "# 1. Import required libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# 2. Load a pre-trained SBERT model (this one is rather small and has \"only\" 384 dimensions)\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# OPTIONAL: If you are runniung a macbook with an M chip that incoproates a GPU, then you can leverage that GPU when you load the pre-trained SBERT model as follows:\n",
    "# embedder = SentenceTransformer('all-MiniLM-L6-v2', device='mps')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hazDshrwOdAw"
   },
   "source": [
    "## 1.5 Embed Sentences with SBERT"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "99XZi9CjOdAx"
   },
   "source": [
    "# 1. Let's create a word, sentence and even paragraph:\n",
    "words = \"Artificial Intelligence\"\n",
    "\n",
    "sentence = \"AI is transforming buisness in remarkable ways!\"\n",
    "\n",
    "paragraph = \"Artificial intelligence is a rapidly advancing field that integrates concepts from computer science, \" \\\n",
    "               \"mathematics, and cognitive psychology to create systems capable of performing tasks traditionally \" \\\n",
    "               \"requiring human intelligence. These systems, powered by machine learning and deep learning techniques, \" \\\n",
    "               \"are now driving innovations in areas such as healthcare, finance, transportation, and entertainment. \" \\\n",
    "               \"By enabling machines to learn from data, recognize patterns, and make decisions, AI has become a \" \\\n",
    "               \"cornerstone of modern technology, offering transformative potential while also raising important \" \\\n",
    "               \"ethical and societal questions about its implications for the future.\"\n",
    "\n",
    "corpus = [words, sentence, paragraph]\n",
    "\n",
    "# 2. Now we can easily embed them with SBERT\n",
    "if torch.cuda.is_available()==True:\n",
    "  print(\"Embedding on GPU\\n\")\n",
    "corpus_embeddings = embedder.encode(corpus, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "# 3. Move embeddings from GPU to CPU IF you are using a GPU!\n",
    "if torch.cuda.is_available()==True:\n",
    "  print(\"Moving Embeddings from GPU to CPU\\n\")\n",
    "  corpus_embeddings=corpus_embeddings.cpu()\n",
    "\n",
    "# 4. Let's look at text_embeddings and see that they are vectors, that is, latent feature vectors\n",
    "for text, vector in zip(corpus, corpus_embeddings):\n",
    "  print(f\"Text: {text}\")\n",
    "  print(f\"Embedding size: {len(vector)}\")\n",
    "  print(f\"Embedding: [{', '.join(map(str, vector[:5].numpy())) +',...'}]\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdRIqQu8OdA0"
   },
   "source": [
    "***Question: What do you notice?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLdAxij3OdA1"
   },
   "source": [
    "## 1.6 Sentence Similarity\n",
    "We can use latent feature vectors to determine how similar sentences are!\n",
    "\n",
    "- The output of SBERT is a matrix of dimension N*384 (for the model we used!)\n",
    "- Each sentence of N sentences is a feature vector of size 384\n",
    "- When the vectors are normalized (which is the case for the pre-trained model), the inner product of encodings can be treated as a similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qe8coU_-OdA1"
   },
   "source": [
    "# 1. Write several sentences of different topics for restaurants\n",
    "\n",
    "corpus = [\n",
    "    # Good Service\n",
    "    \"The waiter at the restaurant was very nice\",\n",
    "    \"The restaurant had great service\",\n",
    "    \"The service is great because of the nice waiters\",\n",
    "\n",
    "    # Good Food\n",
    "    \"Very flavorful chicken!\",\n",
    "    \"I love the taste of the food.\",\n",
    "    \"They make yummy food!\",\n",
    "\n",
    "    # Good Ambience\n",
    "    \"The interior is amazing.\",\n",
    "    \"I like the way it looks inside.\",\n",
    "    \"The ambience of the place is wonderful.\",\n",
    "\n",
    "    # Food Delivery\n",
    "    \"They deliver all orders to your door.\",\n",
    "    \"You can order all items for delivery.\",\n",
    "    \"They delivered the wrong items!\"\n",
    "]\n",
    "for item in corpus:\n",
    "    print(item)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Embed sentences with SBERT\n",
    "corpus_embeddings = embedder.encode(corpus, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "# 3. Move embeddings from GPU to CPU\n",
    "if torch.cuda.is_available()==True:\n",
    "  print(\"Moving Embeddings from GPU to CPU\\n\")\n",
    "  corpus_embeddings=corpus_embeddings.cpu()"
   ],
   "metadata": {
    "id": "9ryZmHvfmh9W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Look at an embedded sentence\n",
    "corpus_embeddings[0]"
   ],
   "metadata": {
    "id": "zmYJMHyF1FoC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Optional: Normalize embeddings to 1 if not already done by pre-trained model\n",
    "# corpus_embeddings = util.normalize_embeddings(corpus_embeddings)"
   ],
   "metadata": {
    "id": "c7qbi3YooKaC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 7. Generate a Similarity Matrix of Embeddings\n",
    "import numpy as np\n",
    "sim_matrix = np.inner(corpus_embeddings, corpus_embeddings)\n",
    "print(sim_matrix[0:5,0:5])"
   ],
   "metadata": {
    "id": "WHoapc8TmiBK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcUr8jX6OdA6"
   },
   "source": [
    "***What do you notice about the matrix above?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1HMzrwQi5Aj"
   },
   "source": [
    "## 1.7 Visualizing Sentence Similarity\n",
    "Let's generate a heatmap to see to what extent the vectors of sentences that refer to similar topics are also similar"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cl-4GMW7OdA7"
   },
   "source": [
    "# 1. Truncate sentences to create labels\n",
    "corpuslabels = [elem[:30] for elem in corpus]\n",
    "for item in corpuslabels:\n",
    "    print(item)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zJG8HoHAOdA_"
   },
   "source": [
    "# 2. Import needed packages\n",
    "import seaborn as sns\n",
    "\n",
    "# 3. Let's visualize the similarities in a heatmap to test whether we can discover topics\n",
    "# Define a function that creates a heatmap for sentence similarity\n",
    "def plot_similarity(labels, sim_, rotation=90):\n",
    "  sns.set(rc = {'figure.figsize':(10,8)}, font_scale=1.5)\n",
    "  g = sns.heatmap(sim_,\n",
    "      xticklabels=labels, yticklabels=labels,\n",
    "      vmin=0, vmax=1, cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "# 4. Call the function to show the heatmap\n",
    "plot_similarity(corpuslabels, sim_matrix, 90)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USVvQizdOdBC"
   },
   "source": [
    "## 1.8 Topic Discovery with Cluster Analysis\n",
    "\n",
    "### Cluster embedded vectors using k-Means"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2hWKI4hWOdBC"
   },
   "source": [
    "# 1. Import package\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 2. Initializing KMeans\n",
    "kmeans = KMeans(n_clusters=4, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 42)\n",
    "\n",
    "# 3. Fitting with inputs\n",
    "kmeans = kmeans.fit(corpus_embeddings)\n",
    "\n",
    "# 4. Predicting the clusters\n",
    "labels = kmeans.predict(corpus_embeddings)\n",
    "print(labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhX07wBEjUqs"
   },
   "source": [
    "## 1.9 Explore Sentence Similarity in an Interactive Map\n",
    "\n",
    "We can visualize the relationships between our 12 sentences in a map using t-SNE:\n",
    "\n",
    "1. Reduce the dimensionality of the vectors from 384 to 2 with t-SNE\n",
    "2. Visualize the similarity of sentences in a scatterplot.\n",
    "\n",
    "**Note:** To save time in class, we will not run t-SNE multiple times with different seeds to find a better local optimum. For practice, you should however run t-SNE more than once with different seeds (i.e., random states) and pick the solution with the lowest cost!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F7cg2a8vjUXk"
   },
   "source": [
    "# 1. Import packages\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 2. Instantitate and fit t-SNE, giving array of x,y coordinates\n",
    "X_tsne = TSNE(n_components=2, verbose=1, perplexity=5, max_iter=1000, learning_rate=50, init='random', random_state=42\n",
    "              ).fit_transform(corpus_embeddings)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7MAOrszkJiL"
   },
   "source": [
    "**We will use Altair to create an interactive Map**\n",
    "\n",
    "Altair is a powerful tool for interactive visualization in Python https://altair-viz.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "m3BAhljUGZkL"
   },
   "source": [
    "# 1. Import Altair\n",
    "import altair as alt\n",
    "\n",
    "# 2. Create a new DataFrame that holds all the information we need for our map\n",
    "import pandas as pd\n",
    "source = pd.DataFrame(\n",
    "    {'x': X_tsne[:, 0],\n",
    "     'y': X_tsne[:, 1],\n",
    "     'txt': corpus,\n",
    "     'Topic' : labels\n",
    "     #'size'  : 10\n",
    "    })\n",
    "\n",
    "# 3. Define Bubbles on Map\n",
    "bubbles = alt.Chart(source).mark_circle(size=400).encode(\n",
    "    x=alt.X('x:Q', axis=alt.Axis(title=\"not directly interpretable\", grid=False, labels=False),scale=alt.Scale(domain=[min(source.x)-1, max(source.x)+1])),\n",
    "    y=alt.Y('y:Q', axis=alt.Axis(title=\"not directly interpretable\", grid=False, labels=False),scale=alt.Scale(domain=[min(source.y)-1, max(source.y)+1])),\n",
    "    #size='size',\n",
    "    color = 'Topic:N',\n",
    "    tooltip=[alt.Tooltip('txt', title='Tweet'),                            # We can include a lot of information in the tooltips (mouseover pop-up)\n",
    "             alt.Tooltip('Topic', title='Topic')\n",
    "            ]\n",
    ")\n",
    "\n",
    "# 4. Define Labels next to Bubbles on Map\n",
    "text = alt.Chart(source).mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=10 # offset label in x coordinate\n",
    ").encode(\n",
    "    x='x:Q',\n",
    "    y='y:Q',\n",
    "    text='txt',\n",
    "    #color = 'Topic:N'\n",
    ")\n",
    "\n",
    "# 5. Visualizes Bubbles and Labels in an interactive Map\n",
    "bubbles.encode().interactive().properties(height=700,width=700,\n",
    "                                          title=\"Restaurant Experiences\") + text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym-vVd6_OdBF"
   },
   "source": [
    "## 1.10 Search for Similar Sentences\n",
    "- Sometimes, we want to explore similar sentences to learn more about a text corpus.\n",
    "- Finding similar sentences is easy with SBERT: ***A search utility comes with SBERT!***\n",
    "- Can be helpful when you investigate a particular topic, person, brand, firm, etc."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Define query sentences:\n",
    "queries = ['I hate their aweful pasta.',\n",
    "           'The floor is stained and dirty.',\n",
    "           'The waiter was so cute.']\n",
    "\n",
    "# 2. Embed query sentences with SBERT\n",
    "query_embedding = embedder.encode(queries, batch_size=64, show_progress_bar=True, convert_to_tensor=True)"
   ],
   "metadata": {
    "id": "vWbmJXjQxdzt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Use semantic search function to find top_3 similar sentences to each query\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\n",
    "hits"
   ],
   "metadata": {
    "id": "2GzLzRsFcxoa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Pick a query and print top_k sentences including their original index and the similarity score to the query\n",
    "_hits = hits[0]      #Get the hits for the first query at index 1 in hits\n",
    "for hit in _hits:\n",
    "    print(hit['corpus_id'], corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))"
   ],
   "metadata": {
    "id": "MNx6RsxKxsUu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Topic Discovery on X(Twitter) with Sentence Embedding using LLMs"
   ],
   "metadata": {
    "id": "9Ty4HN7LQnk_"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M10Ct_ztXq78"
   },
   "source": [
    "## 2.1 Hospitality during the CoVID-19 Pandemic\n",
    "\n",
    "- Now that we know how to embed entire sentences, let's use sentence embedding to ***discover what people are twittering about***\n",
    "- In today's class, we will examine what people have to say about ***Graduate Hotels***\n",
    "  - Privately owned collection of boutique hotels\n",
    "  - College themed\n",
    "  - Over 30 locations worldwide\n",
    "  - We have one right on Franklin Street\n",
    "- I collected data on variations of the term \"Graduate Hotels\" from X(Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0GwRi5hn4R2"
   },
   "source": [
    "## 2.2 Load Tweets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pjQBYcs4A3UO"
   },
   "source": [
    "# 1. Connect your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Navigate to the folder where the files for Class 07 are:\n",
    "%cd /content/drive/MyDrive/488/Class07\n",
    "\n",
    "# 3. See what is in the folder: Special shell command to view the files in the current directory of the notebook environment\n",
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JCaab19lii5I"
   },
   "source": [
    "# 1. Load file into DataFrame (Data is in `data` subfolder -- load to your Google Drive if necessary\n",
    "tweets = pd.read_json('Graduate2022-2k.json', lines=True)\n",
    "\n",
    "# 2. Keep only certain columns\n",
    "tweets = tweets.filter(['id','content','date'], axis=1)\n",
    "tweets.rename(columns={'content':'Tweet'}, inplace=True)\n",
    "tweets"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wYLFMepn7Zg"
   },
   "source": [
    "## 2.3 Pre-process Tweets: **The Tweet Preprocessor**\n",
    "\n",
    "Let's leverage the work of someone else (https://github.com/s) to preprocess our tweets. They created a [tweet preprocessor](https://github.com/s/preprocessor) as part of their bachelor thesis on sentiment analysis.\n",
    "\n",
    "It gives several options for elements that you may want to remove (i.e., clean).  Or we can apply the manual approach that I showed in class 17:\n",
    "\n",
    "| Option   Name  | Option Short Code |\n",
    "|----------------|-------------------|\n",
    "| URL            | p.OPT.URL         |\n",
    "| Mention        | p.OPT.MENTION     |\n",
    "| Hashtag        | p.OPT.HASHTAG     |\n",
    "| Reserved Words | p.OPT.RESERVED    |\n",
    "| Emoji          | p.OPT.EMOJI       |\n",
    "| Smiley         | p.OPT.SMILEY      |\n",
    "| Number         | p.OPT.NUMBER      |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gz4kSkm1misf"
   },
   "source": [
    "# 1. You need to install it first:\n",
    "!pip install tweet-preprocessor"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i5SeNemZmivn"
   },
   "source": [
    "# 2. Preprocess the tweets\n",
    "\n",
    "# a. Import the preprocessor\n",
    "import preprocessor as prepro\n",
    "\n",
    "# b. Set options to remove URL, Reserved word\n",
    "prepro.set_options(prepro.OPT.URL, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.HASHTAG)\n",
    "\n",
    "# c. Let's do it for all tweets\n",
    "tweets['text']  = tweets['Tweet'].apply(prepro.clean)\n",
    "\n",
    "# d. Check our work\n",
    "tweets['text'].head(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6JjrpHHsMch"
   },
   "source": [
    "- Because we scraped the tweets from the internet, the tweet preprocessor may not have dealt with special HTML entities such as the â‚¬ symbol.\n",
    "- We also want to remove line breaks, tabs and the @ and #."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OJ6uIfztmoXF"
   },
   "source": [
    "# 3. Fix some things the preprocessor missed\n",
    "htmlents = r'|'.join((r'&copy;',r'&reg;',r'&quot;',r'&gt;',r'&lt;',r'&nbsp;',r'&apos;',r'&cent;',r'&euro;',r'&pound;'))\n",
    "tweets.text = tweets.text.replace(\n",
    "    {htmlents:'',       # remove html punctuation codes\n",
    "     '#|@':'',          # remove hashtag # and reference @, leaving tags (unless preprocessor removed already)\n",
    "     '&amp;':' and ',   # &amp; to and\n",
    "     '\\n|\\t':' '}, regex=True) # strip HTMLentries, hash tag markers, reference @, newlines\n",
    "tweets.text = tweets.text.str.strip().replace({' +':' '},regex=True) # collapse extra spaces\n",
    "# Check our Work\n",
    "tweets.text.tail(10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0nh8p6xslU3"
   },
   "source": [
    "- Our data may include the same tweet multiple times.\n",
    "- We will remove identical tweets before our analysis as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BpF-Bltqwnk0"
   },
   "source": [
    "# 4. Remove duplicate tweets and reindex\n",
    "\n",
    "print(tweets.shape)\n",
    "tweets.drop_duplicates(subset='text', keep=\"first\", inplace=True)\n",
    "tweets.drop_duplicates(subset='id', keep=\"first\", inplace=True)\n",
    "tweets.reset_index(drop=True, inplace=True)\n",
    "print(tweets.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 BERTopic - A convenient tool for Topic Discovery\n",
    "\n",
    "![BERTopic](https://maartengr.github.io/BERTopic/logo.png \"BERTopic\")\n",
    "\n",
    "- Topic modeling technique that leverages sentence transformers and c-TF-IDF to\n",
    "  * Create dense clusters of text\n",
    "  * That allow for easily interpretable topics\n",
    "  * Whilst keeping important words in the topic descriptions\n",
    "\n",
    "- BERTopic is essentially a sequence of steps to create its topic representations. There are five steps to this process:\n",
    "\n",
    "![BERTopic](https://maartengr.github.io/BERTopic/algorithm/default.svg \"BERTopic\")"
   ],
   "metadata": {
    "id": "qcpf0LlcFmW5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4.1. Set-up BERTopic\n",
    "\n",
    "- Install it\n",
    "- Load it\n",
    "- Fit it to our pre-processed Tweets"
   ],
   "metadata": {
    "id": "JcCTs6Z_L7Zw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Let's install BERTopic\n",
    "!pip install bertopic"
   ],
   "metadata": {
    "id": "DPyLAxieFlu5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Import libraries\n",
    "#import numpy as np #(aleady done)\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# 3. Set-up BERTopic model\n",
    "topic_model = BERTopic(verbose=True)\n",
    "\n",
    "# 4. Convert tweets to list\n",
    "docs = tweets.text.to_list()\n",
    "\n",
    "# 5. Find topics using BERTopic\n",
    "topics, probabilities = topic_model.fit_transform(docs)"
   ],
   "metadata": {
    "id": "qY3QVTMELVKX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4.2. Explore discovered Topics\n",
    "- Frequencies\n",
    "- Words\n",
    "- Visualize"
   ],
   "metadata": {
    "id": "6jg9_WjyMFay"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Let's see how many topics we found (Topic -1 means that these tweets are not associated with any topic!)\n",
    "topic_model.get_topic_info().head(11)"
   ],
   "metadata": {
    "id": "CmcAYoJaL4AB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Let's look at the words and their topic probabilities that are sssociated with an indivual topic: Topic 1\n",
    "topic_model.get_topic(1)"
   ],
   "metadata": {
    "id": "e_vTiO_0Mpz9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Let's visually explore topics\n",
    "topic_model.visualize_topics()"
   ],
   "metadata": {
    "id": "g6-XbPI1M9-d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. We can also get a Barchart for the topics with the most relevant words\n",
    "topic_model.visualize_barchart(top_n_topics=10)"
   ],
   "metadata": {
    "id": "rqspoQmrNRnt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 5. Which topic would a tweet (or text) best fit into?\n",
    "new_doc = \"Had a hard time to get hall pass - hate it!\"\n",
    "topic, score = topic_model.transform([new_doc])\n",
    "print(f'Best match is topic {topic[0]} with probability {score[0]}')"
   ],
   "metadata": {
    "id": "p1yMezDwOXqt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. Find topics that a word is most likely associated with\n",
    "pd.DataFrame(topic_model.find_topics(\"love\")) # most relevant is with highest score (row 1, column 0), where topic number is in row 0, column 0."
   ],
   "metadata": {
    "id": "CpyyBycRPN5V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. Save a fitted BERTopic model\n",
    "topic_model.save(\"graduatetweets\")\n",
    "\n",
    "# 6a. Load a fitted BERTopic model\n",
    "graduate_model = BERTopic.load(\"graduatetweets\")\n",
    "\n",
    "# 6b. Test loaded model for same results\n",
    "pd.DataFrame(graduate_model.find_topics(\"love\")) # most relevant is with highest score (row 1, column 0), where topic number is in row 0, column 0."
   ],
   "metadata": {
    "id": "NgOj954gPDKO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4.3. Visualize the Tweet Landscape\n",
    "\n",
    "Let's explore how all the Tweets we collected are related to another in a 2D Map\n",
    "\n"
   ],
   "metadata": {
    "id": "GUaK6ADPrEA2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. Import libraries\n",
    "#import numpy as np #(aleady done)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from bertopic import BERTopic #already done\n",
    "\n",
    "# set a seed for reproducible results\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# 2. Set-up BERTopic model with more control over it's individual components\n",
    "langmodel =\"all-MiniLM-L6-v2\"\n",
    "clustering=HDBSCAN(min_cluster_size=10)\n",
    "dimreduct=UMAP(n_components=2, random_state=0)\n",
    "vects=CountVectorizer(ngram_range=(1, 3), stop_words=\"english\")\n",
    "topic_model = BERTopic(embedding_model=langmodel, umap_model=dimreduct, hdbscan_model=clustering, vectorizer_model=vects, verbose=True)\n",
    "\n",
    "# 3. Convert tweets to list\n",
    "docs = tweets.text.to_list()\n",
    "\n",
    "# 5. Find topics using BERTopic\n",
    "topics, probabilities = topic_model.fit_transform(docs)"
   ],
   "metadata": {
    "id": "-VpwplVmpqGk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. Let's get the dimensionality reduced embeddings from BERTopic:\n",
    "umap_embeddings = topic_model.umap_model.embedding_\n",
    "print(umap_embeddings.shape)"
   ],
   "metadata": {
    "id": "RCHkmP7WfqNl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 7. Import Altair\n",
    "import altair as alt\n",
    "\n",
    "# 8. Create a new DataFrame that holds all the information we need for our map\n",
    "import pandas as pd\n",
    "source = pd.DataFrame(\n",
    "    {'x': umap_embeddings[:, 0],\n",
    "     'y': umap_embeddings[:, 1],\n",
    "     'txt': docs,\n",
    "     'Topic' : topics\n",
    "     #'size'  : 100\n",
    "    })\n",
    "\n",
    "# 9. Define Bubbles on Map\n",
    "bubbles = alt.Chart(source).mark_circle(size=100).encode(\n",
    "    x=alt.X('x:Q', axis=alt.Axis(title=\"not directly interpretable\", grid=False, labels=False),scale=alt.Scale(domain=[min(source.x)-1, max(source.x)+1])),\n",
    "    y=alt.Y('y:Q', axis=alt.Axis(title=\"not directly interpretable\", grid=False, labels=False),scale=alt.Scale(domain=[min(source.y)-1, max(source.y)+1])),\n",
    "    #size='size',\n",
    "    color = 'Topic:N',\n",
    "    tooltip=[alt.Tooltip('txt', title='Tweet'),    # We can include a lot of information in the tooltips (mouseover pop-up)\n",
    "             alt.Tooltip('Topic', title='Topic')\n",
    "            ]\n",
    ")\n",
    "\n",
    "# # 4. Define Labels next to Bubbles on Map\n",
    "# text = alt.Chart(source).mark_text(\n",
    "#     align='left',\n",
    "#     baseline='middle',\n",
    "#     dx=10 # offset label in x coordinate\n",
    "# ).encode(\n",
    "#     x='x:Q',\n",
    "#     y='y:Q',\n",
    "#     text='txt',\n",
    "#     #color = 'Topic:N'\n",
    "#)\n",
    "\n",
    "# 10. Visualizes Bubbles and Labels in an interactive Map\n",
    "bubbles.encode().interactive().properties(height=700,width=700,\n",
    "                                          title=\"Graduate Hotel Tweets\")# + text"
   ],
   "metadata": {
    "id": "RIYcVfxwgUvV"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
